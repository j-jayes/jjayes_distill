[
  {
    "path": "posts/2022-12-20-gpt-chat-blog-post-author/",
    "title": "ChatGPT fills in for me as a blog post author",
    "description": "I ask ChatGPT to write a blog post in the same manner as one I wrote last week. The results will amaze you!",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {
          "interludeone.com": {}
        }
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPurpose\r\nPrompt\r\nOutput\r\nWhat would I change?\r\n\r\n\r\nPurpose\r\nChatGPT is all the rage at the moment. On Friday last week I spent an hour writing a short blog post to explain how to draw coefficient plots for a colleague.\r\nToday I asked ChatGPT to to the same.\r\nIt took 30 seconds.\r\nHere are the results!\r\n\r\n\r\n\r\n\r\nPenguins created with DALL·E with the prompt “A diagram of three penguins in the style of Paul Cézanne, hyper realistic, beautiful art”\r\nPrompt\r\n‘Prompt engineering’ describes the art of illiciting useful responses from Large Language Models (LLMs). Yet, the latest iteration of ChatGPT often gives me what I ask for on the first try.\r\nHere, I asked ChatGPT to:\r\n\r\nWrite a blog post for a website using R tidyverse syntax that shows how to draw a coefficient plot with error bars in ggplot. Use the Palmer Penguins dataset and show the relationship between bill_length_mm and body_mass_g. Use the lm() command to run a regression and the broom::tidy(conf.int = TRUE) command on the regression output to draw error bars with the geom_errorbarh() geometry.\r\n\r\nOutput\r\nI recieved text and code that ran almost perfectly:\r\nIt began\r\n\r\nAre you trying to visualize the relationship between two variables in your dataset using ggplot and the tidyverse syntax in R? One way to do this is by creating a coefficient plot, which is a type of scatterplot that includes error bars for the data points. In this blog post, we’ll walk through the steps for creating a coefficient plot with error bars using the Palmer Penguins dataset, which includes measurements of bill length and body mass for several species of penguins.\r\nTo start, let’s load the necessary packages :\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(broom)\r\n\r\n\r\n\r\nNext, we’ll load the Palmer Penguins dataset and take a look at the variables we have to work with:\r\n\r\n\r\n\r\nlibrary(palmerpenguins)\r\ndata(\"penguins\")\r\n\r\nhead(penguins)\r\n\r\n# A tibble: 6 × 8\r\n  species island    bill_length_mm bill_…¹ flipp…² body_…³ sex    year\r\n  <fct>   <fct>              <dbl>   <dbl>   <int>   <int> <fct> <int>\r\n1 Adelie  Torgersen           39.1    18.7     181    3750 male   2007\r\n2 Adelie  Torgersen           39.5    17.4     186    3800 fema…  2007\r\n3 Adelie  Torgersen           40.3    18       195    3250 fema…  2007\r\n4 Adelie  Torgersen           NA      NA        NA      NA <NA>   2007\r\n5 Adelie  Torgersen           36.7    19.3     193    3450 fema…  2007\r\n6 Adelie  Torgersen           39.3    20.6     190    3650 male   2007\r\n# … with abbreviated variable names ¹​bill_depth_mm,\r\n#   ²​flipper_length_mm, ³​body_mass_g\r\n\r\n\r\nWe can see that the dataset includes variables such as bill_length_mm, body_mass_g, and species. For our coefficient plot, we’ll focus on the relationship between bill_length_mm and body_mass_g.\r\nTo understand the coefficient plot, we’ll start by using the ggplot function to specify the data and the aesthetics for a scatter plot. We’ll set the x variable to be bill_length_mm and the y variable to be body_mass_g.\r\n\r\n\r\n\r\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g))\r\n\r\n\r\n\r\n\r\nNow we’ll add the geom_point layer to plot the data points and the geom_smooth layer to add a smooth line of best fit.\r\n\r\n\r\n\r\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) +\r\n  geom_point() +\r\n  geom_smooth()\r\n\r\n\r\n\r\n\r\nThis gives us a scatter plot with a line of best fit. We can see that body mass increases as does bill length.\r\nNext, we can use the lm() command to run a regression analysis on the data, using bill length as the predictor variable and body mass as the outcome variable. We can then use the broom::tidy() function with the conf.int = TRUE argument to extract the regression results, including the confidence intervals for the coefficients:\r\n\r\n\r\n\r\nreg_results <- lm(body_mass_g ~ bill_length_mm, data = penguins)\r\nreg_tidy <- broom::tidy(reg_results, conf.int = TRUE)\r\n\r\n\r\n\r\nNow that we have the regression results, we can use ggplot to create our coefficient plot.\r\nTo add the error bars, we’ll use the geom_errorbarh() geometry and specify the xmin and xmax aesthetics as the lower and upper bounds of the confidence intervals, respectively:\r\n\r\n\r\n\r\nggplot(reg_tidy, aes(x = estimate, y = term)) +\r\n  geom_point() +\r\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))\r\n\r\n\r\n\r\n\r\nAnd that’s it! We now have a coefficient plot with error bars showing the relationship between bill length and body mass in the Palmer Penguins dataset. This plot can help us understand how the two variables are related, and how certain we can be about the strength of that relationship based on the size of the error bars.\r\nI hope this tutorial was helpful in showing you how to create a coefficient plot with error bars using the tidyverse syntax in R. If you have any questions or comments, please feel free to leave them below. Happy plotting!\r\n\r\nWhat would I change?\r\nThis was a fantastic tutorial and written up in a way that is relatively easy to understand too. I enjoyed the use of succinct variable names and how to the point the write up was.\r\nThe only things I added in my own post on Friday were a dotted line at zero, a title, and some axis labels. This is shown below.\r\n\r\n\r\nggplot(reg_tidy, aes(x = estimate, y = term)) +\r\n  geom_point() +\r\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\r\n  geom_vline(xintercept = 0, lty = 2) +\r\n  labs(\r\n    x = \"Estimate of effect of variable on body mass (in grams)\",\r\n    y = NULL,\r\n    title = \"Coefficient plot with error bars\"\r\n  )\r\n\r\n\r\n\r\nWow! Amazing. I learned a bunch about ggplot and R from reading David Robinson’s\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/j-jayes/jjayes_distill/raw/fba82ad50bbd00f3768079fb983f8c8d2706cde2/_posts/2022-12-20-gpt-chat-blog-post-author/images/DALL%C2%B7E%202022-12-20%2016.38.40%20-%20A%20diagram%20of%20three%20penguins%20in%20the%20style%20of%20Paul%20C%C3%A9zanne%2C%20hyper%20realistic%2C%20beautiful%20art.png",
    "last_modified": "2022-12-20T17:20:32+02:00",
    "input_file": "gpt-chat-blog-post-author.knit.md"
  },
  {
    "path": "posts/2022-12-15-coef-plots/",
    "title": "Coefficient plots in `ggplot`",
    "description": "Drawing coefficient plots in R and `ggplot`.",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {
          "interludeone.com": {}
        }
      }
    ],
    "date": "2022-12-15",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nData\r\nBasic regression\r\nTidy with broom package\r\nCoefficient plot\r\nError bars\r\nCategories of predictor\r\nConclusion\r\n\r\nRecently a colleague asked how they could quickly draw a coefficient plot in R. In Stata, this is relatively simple - just run your regression and use the command coefplot afterwards. This produces a graphic that is perfectly acceptable for exploratory data analysis, but leaves something to be desired if you want to use it in a publication.\r\nThis post shows you how to draw coefficient plots in R and ggplot, and is extensible for use with regressions beyond the basic lm command.\r\nData\r\nFor this example we will use data from the lovely Palmer Penguins package from Allison Horst. The penguins dataset is a great toy dataset for exploration and visualization, based on genuine data collected by Dr. Kristen Gorman at the Palmer Station in Antarctica.\r\n\r\n\r\n\r\nBasic regression\r\nWe begin with a basic regression where our dependent variable is penguin body weight in grams, and independent variables are the dimensions of the penguins’ bills, flipper length, as well as species and sex.\r\n\r\n\r\nlibrary(tidyverse)\r\ntheme_set(theme_light())\r\n\r\n# read in data\r\ndf <- palmerpenguins::penguins\r\n\r\n# basic regression\r\nbasic_reg <- df %>% \r\n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .)\r\n\r\nbasic_reg\r\n\r\n\r\nCall:\r\nlm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \r\n    species + sex, data = .)\r\n\r\nCoefficients:\r\n      (Intercept)     bill_length_mm      bill_depth_mm  \r\n         -1460.99              18.20              67.22  \r\nflipper_length_mm   speciesChinstrap      speciesGentoo  \r\n            15.95            -251.48            1014.63  \r\n          sexmale  \r\n           389.89  \r\n\r\nThis produces a rather messy output. It includes both the regression specification and the coefficients.\r\nTidy with broom package\r\nWe can use the broom package to return a tibble, a neat data object that is easy to work with.\r\n\r\n\r\nlibrary(broom)\r\n\r\ntidy_reg <- df %>% \r\n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .) %>% \r\n    tidy()\r\n\r\ntidy_reg\r\n\r\n# A tibble: 7 × 5\r\n  term              estimate std.error statistic  p.value\r\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)        -1461.     571.       -2.56 1.10e- 2\r\n2 bill_length_mm        18.2      7.11      2.56 1.09e- 2\r\n3 bill_depth_mm         67.2     19.7       3.40 7.45e- 4\r\n4 flipper_length_mm     16.0      2.91      5.48 8.44e- 8\r\n5 speciesChinstrap    -251.      81.1      -3.10 2.09e- 3\r\n6 speciesGentoo       1015.     130.        7.83 6.85e-14\r\n7 sexmale              390.      47.8       8.15 7.97e-15\r\n\r\nGreat! This output is much easier to deal with.\r\nCoefficient plot\r\nLet’s try and make a coefficient plot.\r\n\r\n\r\ntidy_reg %>%\r\n  filter(term != \"(Intercept)\") %>%\r\n  # reorder the coefficients so that the largest is at the top of the plot\r\n  mutate(term = fct_reorder(term, estimate)) %>%\r\n  ggplot(aes(estimate, term)) +\r\n  geom_point() +\r\n  # add in a dotted line at zero\r\n  geom_vline(xintercept = 0, lty = 2) +\r\n  labs(\r\n    x = \"Estimate of effect of variable on body mass (in grams)\",\r\n    y = NULL,\r\n    title = \"Coefficient plot\"\r\n  )\r\n\r\n\r\n\r\nWe can see that relative to Adelie penguins (the base category), Gentoo penguins weigh more and Chinstrap penguins weigh less.\r\nFurther, male penguins weigh more than females.\r\nError bars\r\nTo get the error bars, we specify that we want a confidence interval when we use the tidy command from the broom package, like so: tidy(conf.int = TRUE)\r\n\r\n\r\ntidy_reg_conf_int <- df %>% \r\n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .) %>% \r\n    tidy(conf.int = TRUE)\r\n\r\ntidy_reg_conf_int\r\n\r\n# A tibble: 7 × 7\r\n  term              estimate std.er…¹ stati…²  p.value conf.…³ conf.…⁴\r\n  <chr>                <dbl>    <dbl>   <dbl>    <dbl>   <dbl>   <dbl>\r\n1 (Intercept)        -1461.    571.     -2.56 1.10e- 2 -2.58e3  -337. \r\n2 bill_length_mm        18.2     7.11    2.56 1.09e- 2  4.22e0    32.2\r\n3 bill_depth_mm         67.2    19.7     3.40 7.45e- 4  2.84e1   106. \r\n4 flipper_length_mm     16.0     2.91    5.48 8.44e- 8  1.02e1    21.7\r\n5 speciesChinstrap    -251.     81.1    -3.10 2.09e- 3 -4.11e2   -92.0\r\n6 speciesGentoo       1015.    130.      7.83 6.85e-14  7.60e2  1270. \r\n7 sexmale              390.     47.8     8.15 7.97e-15  2.96e2   484. \r\n# … with abbreviated variable names ¹​std.error, ²​statistic,\r\n#   ³​conf.low, ⁴​conf.high\r\n\r\nNow in our tibble we get columns called conf.low and conf.high.\r\nTo plot these, we use an additional geometry in our ggplot, called geom_errorbarh. Here the h at the end specifies we want it in the horizontal direction. We map the conf.low and conf.high variables to xmin and xmax respectively.\r\n\r\n\r\ntidy_reg_conf_int %>%\r\n  filter(term != \"(Intercept)\") %>%\r\n  # reorder the coefficients so that the largest is at the top of the plot\r\n  mutate(term = fct_reorder(term, estimate)) %>%\r\n  ggplot(aes(estimate, term)) +\r\n  geom_point() +\r\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\r\n  # add in a dotted line at zero\r\n  geom_vline(xintercept = 0, lty = 2) +\r\n  labs(\r\n    x = \"Estimate of effect of variable on body mass (in grams)\",\r\n    y = NULL,\r\n    title = \"Coefficient plot with error bars\"\r\n  )\r\n\r\n\r\n\r\nBecause none of the error bars cross the dotted line at zero, we conclude that the point estimates of the effects are significantly different from zero at the five percent level.\r\nCategories of predictor\r\nFinally, let’s facet the variables by type.\r\nWe use the case_when command from dplyr to call any variable containing mm a numeric variable, and the others categorical. We colour our bars and points by variable type, and facet_wrap to make a small multiple chart. This makes it easy to differentiate between the different types of variables.\r\n\r\n\r\ntidy_reg_conf_int %>%\r\n  filter(term != \"(Intercept)\") %>%\r\n  # create a type variable with case_when\r\n  mutate(type = case_when(\r\n    str_detect(term, \"mm\") ~ \"Numeric variable\",\r\n    TRUE ~ \"Categorical variable\"\r\n  )) %>%\r\n  # reorder the coefficients so that the largest is at the top of the plot\r\n  mutate(term = fct_reorder(term, estimate)) %>%\r\n  ggplot(aes(estimate, term, colour = type)) +\r\n  geom_point() +\r\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\r\n  # add in a dotted line at zero\r\n  geom_vline(xintercept = 0, lty = 2) +\r\n  # remove the legend as the facet show that information\r\n  theme(legend.position = \"none\") +\r\n  # facet by type\r\n  facet_wrap(~type, scales = \"free_y\") +\r\n  labs(\r\n    x = \"Estimate of effect of variable on body mass (in grams)\",\r\n    y = NULL,\r\n    title = \"Coefficient plot with error bars\",\r\n    subtitle = \"By variable type\"\r\n  )\r\n\r\n\r\n\r\nIn this way we can see that the scaling of the variables can have an impact on how we perceive the results. Because the bill and flipper dimensions are measured in mm, and the body mass in grams, we are seeing the effect of an additional mm of bill depth, for example, on body mass in grams, which appears quite small.\r\nConclusion\r\nGreat - I hope that this was useful! It’s possible to easily customise your coefficient plots in ggplot, adding labels and colours to help your reader understand your regression results in a glance.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-15-coef-plots/lter_penguins.png",
    "last_modified": "2022-12-15T10:54:14+02:00",
    "input_file": {},
    "preview_width": 1800,
    "preview_height": 1074
  },
  {
    "path": "posts/2022-06-01-linkedin-style-about-info/",
    "title": "LinkedIn style tables for personal websites",
    "description": "A post on how to us the {gt} table package to make tables for the about page of a pesonal website or blog.",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {
          "interludeone.com": {}
        }
      }
    ],
    "date": "2022-06-01",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPurpose\r\nWhere to begin?\r\nBuilding the table\r\nFinalizing the table\r\n\r\nPurpose\r\nKatherine asked me if she could include the university logo in her\r\neducation section on the about page on her website. The about page is\r\nmade with the postcards\r\npackage. I didn’t know how to do it but thought that a table might\r\nmake the most sense.\r\nI really liked the style of the LinkedIn education sections:\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"images/linkedin_education.PNG\")\r\n\r\n\r\n\r\n\r\nFigure 1: LinkedIn screenshot\r\n\r\n\r\n\r\nIn this post I walk through recreating this table with the gt package in R.\r\nWhere to begin?\r\nThe education section in the LinkedIn table has six components for\r\neach entry. They are:\r\nA logo photo\r\nA hyperlink to the institution though the image\r\nName of the institution (in bold)\r\nTitle of the degree\r\nDate (start and end year)\r\nGrade or comment\r\nLet’s create a tibble with these components.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\neducation <- tribble(\r\n  ~logo, ~link, ~institution, ~degree, ~date, ~grade_comment,\r\n  # LU\r\n  \"images/lu.jfif\", \"https://ehl.lu.se/ekonomisk-historia\", \"Lund University\", \"Master's degree, Economics\", \"2021\", \"Exchange semester\",\r\n  # SU\r\n  \"images/su.jfif\", \"https://www.ekon.sun.ac.za/\", \"Stellenbosch University\", \"Master's degree, Economics\", \"2020\", \"Grade: <em>cum laude<\/em>\",\r\n  # uct hnours\r\n  \"images/uct.jfif\", \"http://www.economics.uct.ac.za/\", \"University of Cape Town\", \"Honours degree, Economics\", \"2018\", \"Grade: 2:1\",\r\n  # uct undergrad\r\n  \"images/uct.jfif\", \"http://www.economics.uct.ac.za/\", \"University of Cape Town\", \"Bachelor's degree, Economics and Economic History\", \"2015-2017\", \"Grade: 2:1\",\r\n)\r\n\r\n\r\n\r\nBuilding the table\r\nNow lets start on the table structure. What we’re gonna do is take\r\nthe individual pieces and format them to html, then stick the text\r\ntogether in one cell and the logos/links in another.\r\n\r\n\r\neducation <- education %>%\r\n  mutate(\r\n    logo = glue::glue(\"<a href = {link}>\r\n                        <img src='{logo}' width='50' height='50'>\r\n                      <\/a>\"),\r\n    institution = glue::glue(\"<b>{institution}<\/b>\"),\r\n    date = glue::glue(\"<span style='opacity: .7;'> {date} <\/span>\"),\r\n    grade_comment = glue::glue(\"<span style='opacity: .7;'> {grade_comment} <\/span>\"),\r\n  )\r\n\r\n\r\n\r\nIn the cell above we create an image tag for the logo, putting a\r\nhyperlink tag for the link to the university website inside it. Here we\r\nuse the glue\r\npackage which is useful for working with strings constructed from\r\nother columns. You can think of it as gluing together the html\r\ntag parts and the content from our dataframe. Next we put the\r\ninstitution in bold, and grey out the dates and grades with the html\r\nstyle tag, 'opacity: .7;'.1\r\nNow we can stick these text components together into one cell (just\r\ncalled text) and keep only the logo and text columns.2\r\nFinally we use the map function from the purrr package\r\nto transform the character strings into html. Printing the tibble shows\r\nthat it now contains html elements inside each cell.\r\n\r\n\r\neducation <- education %>%\r\n  mutate(text = str_c(\r\n    institution, \"<br>\",\r\n    degree, \"<br>\",\r\n    date, \"<br>\",\r\n    grade_comment\r\n  )) %>%\r\n  select(logo, text) %>%\r\n  mutate(\r\n    logo = purrr::map(logo, gt::html),\r\n    text = purrr::map(text, gt::html)\r\n  )\r\n\r\neducation\r\n\r\n\r\n# A tibble: 4 x 2\r\n  logo       text      \r\n  <list>     <list>    \r\n1 <html [1]> <html [1]>\r\n2 <html [1]> <html [1]>\r\n3 <html [1]> <html [1]>\r\n4 <html [1]> <html [1]>\r\n\r\nNow we create the table by calling the gt() command.\r\n\r\n\r\nShow code\r\n\r\nlibrary(gt)\r\n\r\neducation %>% \r\n    gt()\r\n\r\n\r\n\r\nlogo\r\n      text\r\n    \r\n  \r\nLund UniversityMaster's degree, Economics 2021  Exchange semester \r\n  \r\nStellenbosch UniversityMaster's degree, Economics 2020  Grade: cum laude \r\n  \r\nUniversity of Cape TownHonours degree, Economics 2018  Grade: 2:1 \r\n  \r\nUniversity of Cape TownBachelor's degree, Economics and Economic History 2015-2017  Grade: 2:1 \r\n\r\nGreat, there it is!\r\nFinalizing the table\r\nWe still want to make some changes to tidy it up a bit:\r\nAdding a table header\r\nAligning the text to the left\r\nRemoving the column labels\r\n\r\n\r\nShow code\r\n\r\neducation %>%\r\n  gt() %>%\r\n  tab_header(md(\"**Education**\")) %>%\r\n  cols_align(align = c(\"left\"), columns = text) %>%\r\n  cols_label(\r\n    logo = \"\",\r\n    text = \"\",\r\n  )\r\n\r\n\r\n\r\nEducation\r\n    \r\n      \r\n    \r\n  \r\nLund UniversityMaster's degree, Economics 2021  Exchange semester \r\n  \r\nStellenbosch UniversityMaster's degree, Economics 2020  Grade: cum laude \r\n  \r\nUniversity of Cape TownHonours degree, Economics 2018  Grade: 2:1 \r\n  \r\nUniversity of Cape TownBachelor's degree, Economics and Economic History 2015-2017  Grade: 2:1 \r\n\r\nLovely! Now we have the table we can use in the about page of a blog\r\nor personal website, with nice clickable logos that take you to the\r\nwebsite of the institution.\r\n\r\nWe opt to use the\r\n<span> tag rather than the <p> tag\r\nbecause <span> does not introduce any extra space.↩︎\r\nWe use str_c from the\r\nstringr package for its nice syntax, we could have used\r\nglue here again↩︎\r\n",
    "preview": "https://raw.githubusercontent.com/j-jayes/jjayes_distill/main/_posts/2022-06-01-linkedin-style-about-info/images/gt_educ.PNG",
    "last_modified": "2022-06-12T16:30:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-17-our-world-in-data-choropleth/",
    "title": "Our World In Data choropleth",
    "description": "A post on how recreate the Our World In Data characteristic choropleth in R.",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {
          "interludeone.com": {}
        }
      }
    ],
    "date": "2022-06-01",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPurpose\r\nContext\r\nComponents\r\nThe world map\r\nData\r\nCreate a colour palette\r\nHow to plot the line graph?\r\nA function for plotting the graph\r\nNow making the table\r\nCreating the plots for each country\r\nCreating the interactive figure\r\nShow off the interactive figure\r\n\r\n\r\nPurpose\r\nI really look up to Max Roser and the team at Our World in Data. They have some of the best short form articles condensing a wealth of academic literature to, in their words, “make progress against the world’s largest problems”.\r\nThe mission is summed up well in a lecture given at Stellenbosch University by Max Roser this year, included below.\r\n\r\n\r\nIn this tutorial I want to walk through recreating one of their classic chart types in R, the world map choropleth with an overlayed line graph for each coutnry. A typical example shown below.\r\n\r\nContext\r\nThere is a lot of information about the OWID grapher tool. You can have a look at their github repo and an older reddit AMA if you are interested. It’s a custom system that allows for systematic changes across their website, drawing on data from a central database.\r\nComponents\r\nWhat are the parts I want to recreate? The map has:\r\na base map, where the colour fill of each country indicates it’s position in a specific measure in a particular year.\r\na simple line chart that appears when you hover over a country, showing how the measure has changed within a country over time.\r\na clear legend\r\na note specifying the source of the data\r\nI walk through creating each of these below.\r\nThe world map\r\nThe base map is sourced from the maps package. I add a three letter country code from the english name of the country using the countrycode package and filter out Antarctica, Greenland and the French Southern and Antarctic Lands.\r\n\r\n\r\nShow code\r\n\r\n# preamble\r\nlibrary(tidyverse)\r\nlibrary(sf)\r\ntheme_set(theme_light())\r\n\r\n# load map\r\nmap <- st_as_sf(maps::map(database=\"world\", plot = FALSE, fill = TRUE))\r\n\r\n# create code to match coutnry to data with\r\nlibrary(countrycode)\r\nmap <- map %>% \r\n  mutate(code = countrycode(ID, \"country.name\", \"iso3c\"))\r\n\r\n# remove clutter from map\r\ncountry_to_remove <- c(\r\n  'Antarctica','Greenland', 'French Southern and Antarctic Lands'\r\n)\r\n\r\nmap <- map %>% \r\n  filter(!ID %in% country_to_remove)\r\n\r\n\r\nThe base map is projected with the Web Mercator or WGS 84 projection, the same one Google Maps uses.\r\n\r\n\r\nShow code\r\n\r\nmap %>% \r\n  ggplot() +\r\n  geom_sf()\r\n\r\n\r\n\r\nData\r\nWe read in the data as a CSV file, and tidy up the column names so that they are in snake case with the clean_names() command from the very helpful janitor package.\r\n\r\n\r\nShow code\r\n\r\ndf <- read.csv(\"data/share-of-adults-who-smoke.csv\")\r\n\r\ndf <- df %>% \r\n  as_tibble() %>% \r\n  dplyr::rename(value = Prevalence.of.current.tobacco.use....of.adults.) %>% \r\n  janitor::clean_names()\r\n\r\n\r\nNext we remove the summary groups which we cannot show on the map, including the World Bank country income groupings.\r\n\r\n\r\nShow code\r\n\r\ndf %>% \r\n  filter(!code %in% map$code) %>% \r\n  distinct(entity)\r\n\r\n# A tibble: 16 × 1\r\n   entity                      \r\n   <chr>                       \r\n 1 East Asia and Pacific       \r\n 2 Europe and Central Asia     \r\n 3 European Union              \r\n 4 High income                 \r\n 5 Latin America and Caribbean \r\n 6 Low and middle income       \r\n 7 Low income                  \r\n 8 Lower middle income         \r\n 9 Middle East and North Africa\r\n10 Middle income               \r\n11 North America               \r\n12 South Asia                  \r\n13 Sub-Saharan Africa          \r\n14 Tuvalu                      \r\n15 Upper middle income         \r\n16 World                       \r\n\r\nShow code\r\n\r\ndf <- df %>% \r\n  filter(code %in% map$code)\r\n\r\n\r\nCreate a colour palette\r\nSo what we want to do is use the scale_color_viridis_c() palette. We have to map it to the min and max of the values in our dataset.\r\n\r\n\r\nShow code\r\n\r\ndf %>%\r\n  summarise(\r\n    min = min(value),\r\n    max = max(value)\r\n  )\r\n\r\n# A tibble: 1 × 2\r\n    min   max\r\n  <dbl> <dbl>\r\n1   3.5  68.5\r\n\r\nShow code\r\n\r\nlibrary(viridisLite) \r\n\r\nvir_10 <- viridis(n = 10)\r\n\r\nsmoking_hex <- scales::gradient_n_pal(\r\n  colours = vir_10,\r\n  values = seq(0, 50, by = 5)\r\n)\r\n\r\nscale_smoking <- function() {\r\n  scale_color_gradientn(\r\n    colours = vir_10,\r\n    values = seq(0, 50, by = 5) / 50,\r\n    limits = c(0, 68.5),\r\n    name = \"value\"\r\n  )\r\n}\r\n\r\n\r\nHow to plot the line graph?\r\nThe line graph that appears when you hover over OWID map is very simple. It has just the starting value on the y-axis, and the first and last years on the x-axis, and a line that changes colour in accordance with the scale of the choropleth. The hover window which contains the graph also shows the country name, and the value of the measure in the most recent year.\r\nTo recreate it, we need store these four values, and draw the coloured line.\r\n\r\nA function for plotting the graph\r\n\r\n\r\nShow code\r\n\r\n# Here the function to plot the line takes only one argument, `cd` the country code \r\nplot_line <- function(cd) {\r\n  # get axis marks\r\n  label_y <- df %>%\r\n    filter(code == cd) %>%\r\n    mutate(\r\n      min_year = min(year),\r\n      max_year = max(year)\r\n    ) %>%\r\n    filter(year == min(year))\r\n  \r\n  # plot the line\r\n  df %>%\r\n    filter(code == cd) %>%\r\n    ggplot(aes(year, value)) +\r\n    geom_point(cex = 3) +\r\n    # mapping the colour of the line segment to the value is done here\r\n    geom_line(aes(colour = value), cex = 2, alpha = .7) +\r\n    # this scale is created above, with bounds appropriate to this data\r\n    scale_smoking() +\r\n    scale_y_continuous(\r\n      # specifying the break on the y-axis creates the axis text\r\n      breaks = c(label_y$value),\r\n      labels = scales::percent_format(scale = 1, accuracy = .1),\r\n      # the limits argument here ensures the y-axis starts at zero\r\n      limits = c(0, NA)\r\n    ) +\r\n    scale_x_continuous(\r\n      # x-axis needs only two years, the first and last\r\n      breaks = c(label_y$min_year, label_y$max_year)) +\r\n    theme(\r\n      # removing the axis ticks and lines clears the graph of clutter\r\n      axis.ticks.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor.x = element_blank(),\r\n      legend.position = \"none\",\r\n      text = element_text(size = 20)\r\n    ) +\r\n    labs(\r\n      x = NULL,\r\n      y = NULL\r\n    )\r\n}\r\n\r\n# Test the function with India.\r\nplot_line(\"IND\")\r\n\r\n\r\n\r\nNow making the table\r\n\r\n\r\nShow code\r\n\r\nlibrary(gt)\r\nlibrary(gtExtras)\r\n\r\nmake_table <- function(cd) {\r\n  message(\"Making table for \", cd)\r\n  vars <- df %>%\r\n    filter(code == cd) %>%\r\n    filter(year == max(year))\r\n\r\n  plot <- plot_line(cd)\r\n\r\n  # the `ggplot_image` command outputs an image that can easily be put into a gt table when formatted as markdown\r\n  plot <- gt::ggplot_image(plot, height = px(250), aspect_ratio = 1.6)\r\n\r\n  tbl <- tibble(plot = plot, value = vars$value, context = glue::glue(\"in {vars$year}\"))\r\n\r\n  gt(tbl) %>%\r\n    fmt_markdown(columns = c(plot)) %>%\r\n    fmt_percent(value, scale_values = F, decimals = 1) %>% \r\n    # the `merge_stack` command joins the value and the year in one cell\r\n    # the `smoking_hex` function we created above makes the text coloured appropriately\r\n    gt_merge_stack(col1 = value, col2 = context, palette = c(smoking_hex(tbl$value), \"grey\")) %>%\r\n    tab_style(\r\n      style = cell_text(size = \"xx-large\"),\r\n      locations = cells_body(\r\n        columns = c(value)\r\n      )\r\n    ) %>%\r\n    tab_header(\r\n      # title table with coutnry name\r\n      title = md(glue::glue(\"**{vars$entity}**\"))) %>%\r\n    tab_options(column_labels.hidden = TRUE) %>% \r\n    as_raw_html(inline_css = F)\r\n}\r\n\r\n# Test on South Africa\r\nmake_table(\"ZAF\")\r\n\r\n\r\n  South Africa\r\n    \r\n20.3%\r\nin 2020\r\n\r\nCreating the plots for each country\r\nHere we use the purrr::map command to make the table in raw HTML for each country and save it inside a tibble. The output shows an HTML list in the column called gt.\r\n\r\n\r\nShow code\r\n\r\ngt_tables <- df %>% \r\n  distinct(code) %>%\r\n  mutate(gt = purrr::map(code, make_table))\r\n\r\ngt_tables\r\n\r\n# A tibble: 162 × 2\r\n   code  gt        \r\n   <chr> <list>    \r\n 1 AFG   <html [1]>\r\n 2 ALB   <html [1]>\r\n 3 DZA   <html [1]>\r\n 4 AND   <html [1]>\r\n 5 ARG   <html [1]>\r\n 6 ARM   <html [1]>\r\n 7 AUS   <html [1]>\r\n 8 AUT   <html [1]>\r\n 9 AZE   <html [1]>\r\n10 BHS   <html [1]>\r\n# … with 152 more rows\r\n\r\nWe thencreate a tibble called df_map that selects the most recent year for each country from the dataset and joins it to the map by the country code variable we created above. Finally we join this to the tibble of tables called gt_tables.\r\n\r\n\r\nShow code\r\n\r\ndf_map <- df %>% \r\n  group_by(entity) %>% \r\n  filter(year == max(year)) %>% \r\n  ungroup() %>% \r\n  left_join(map, by = c(\"code\"))\r\n\r\ndf_map <- df_map %>% \r\n  inner_join(gt_tables)\r\n\r\n\r\nCreating the interactive figure\r\nNow we are ready to create the interactive figure!\r\nWe begin by drawing a static map in grey, with data from the original map. Next we overlay the interactive choropleth. The grey static map will show through all the countries we don’t have data on in the dataset.\r\n\r\n\r\nShow code\r\n\r\nlibrary(ggtext)\r\nlibrary(ggiraph)\r\n\r\ng <- df_map %>%\r\n  ggplot(aes(geometry = geom)) +\r\n  geom_sf(data = map, fill = \"grey80\") +\r\n  geom_sf_interactive(aes(fill = value, tooltip = gt)) +\r\n  scale_fill_binned(type = \"viridis\", labels = scales::percent_format(scale = 1)) +\r\n  cowplot::theme_minimal_grid() +\r\n  theme(legend.position = \"bottom\") +\r\n  guides(fill = guide_colourbar(barwidth = 20, barheight = .5, title.position = \"top\", label = TRUE)) +\r\n  labs(\r\n    fill = \"Share of adults who smoke, 2020\",\r\n    caption = \"Source: World Health Organization (via World Bank)\"\r\n  ) +\r\n  theme(\r\n    plot.background = element_rect(fill = \"white\", color = NA),\r\n    plot.title = element_text(hjust = 0.5, family = \"marker\", size = 50),\r\n    plot.subtitle = element_markdown(size = 20, family = \"open\", lineheight = 0.5),\r\n    plot.caption = element_markdown(size = 12, family = \"open\"),\r\n    axis.text = element_blank(),\r\n    axis.title = element_blank(),\r\n    panel.grid.major = element_line(color = \"grey80\", size = 0.1),\r\n    legend.title.align = .5\r\n  )\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ng\r\n\r\n\r\n\r\nShow off the interactive figure\r\n\r\n\r\nShow code\r\n\r\n# customizing the CSS makes the hover box easier to read.\r\ntooltip_css <- \"background-color:gray;color:white;padding:10px;border-radius:5px;text-align:center;\"\r\n\r\nggiraph(\r\n  ggobj = g,\r\n  options = list(\r\n    opts_tooltip(css = tooltip_css),\r\n    opts_sizing(width = 1)\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/j-jayes/img-repo/raw/1f154af0b081a6423c73c6fe453c93ea16c891c2/images/owid_smokers.PNG",
    "last_modified": "2022-10-31T12:29:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/",
    "title": "Programmatically generate tabs in Rmarkdown",
    "description": "A walk-though for writing code that writes it's own code.",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {
          "interludeone.com": {}
        }
      }
    ],
    "date": "2022-03-04",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPurpose\r\nProblem\r\nBad solution\r\nBetter solution\r\nTools\r\n\r\nHow does it work?\r\nGood to remember\r\n\r\nDoes it work?\r\nPostscript\r\n\r\nThank you to Adam A. Kemberling for providing some useful\r\nfeedback on how to explain the panelset syntax and suggesting the use of\r\ndplyr::walk() instead of a for loop. His suggestion is here.\r\nPurpose\r\nI want to create a parametrized report in Rmarkdown. The report is\r\npopulated after querying a database. In short, the Rmarkdown document\r\nmust have a tab for each category of a categorical\r\nvariable, and draw a ggplot in the tab. The number of\r\ncategories depends on the result of the query.\r\nWe can simulate the problem by removing one of the islands from the\r\npalmerpenguins dataset:\r\nThere are three islands:\r\n\r\nIsland name\r\nNumber of penguins\r\nBiscoe\r\n168\r\nDream\r\n124\r\nTorgersen\r\n52\r\n\r\nWe will subset the data to choose only two islands:\r\n\r\n\r\nShow code\r\n\r\ndf <- palmerpenguins::penguins %>% \r\n    filter(island != \"Biscoe\")\r\n\r\ndf %>%\r\n  distinct(island) %>%\r\n  knitr::kable(col.names = c(\"Remaining islands\"))\r\n\r\n\r\nRemaining islands\r\nTorgersen\r\nDream\r\n\r\nProblem\r\nIf I hardcode the number of tabs one of the plots is drawn but has no\r\ndata!\r\n\r\nBiscoe\r\n\r\n\r\nplot_penguins(\"Biscoe\")\r\n\r\n\r\n\r\n\r\nDream\r\n\r\n\r\nplot_penguins(\"Dream\")\r\n\r\n\r\n\r\n\r\nTorgersen\r\n\r\n\r\nplot_penguins(\"Torgersen\")\r\n\r\n\r\n\r\n\r\n\r\nBad solution\r\nI could amend my plot_penguins function to display\r\nnothing if there is no data to plot:\r\n\r\n\r\nShow code\r\n\r\nplot_penguins <- function(isl) {\r\n  if (dim(df %>% filter(island == isl))[1] != 0) { #<<\r\n    df %>%\r\n      filter(island == isl) %>%\r\n      ggplot(aes(bill_length_mm, bill_depth_mm, colour = body_mass_g,\r\n        shape = species)) +\r\n      geom_point(cex = 2) +\r\n      geom_smooth(method = \"lm\", se = F) +\r\n      scale_color_viridis_c(labels = scales::number_format()) +\r\n      labs(\r\n        x = \"Bill length (mm)\",\r\n        y = \"Bill depth (mm)\",\r\n        colour = \"Body mass (g)\",\r\n        shape = \"Penguin species\"\r\n      )\r\n  } #<<\r\n}\r\n\r\n\r\n\r\n\r\nBiscoe\r\n\r\n\r\nplot_penguins(\"Biscoe\")\r\n\r\n\r\n\r\nDream\r\n\r\n\r\nplot_penguins(\"Dream\")\r\n\r\n\r\n\r\n\r\nTorgersen\r\n\r\n\r\nplot_penguins(\"Torgersen\")\r\n\r\n\r\n\r\n\r\n\r\nThis is a poor solution. In the case where there are only a small\r\nnumber of possible categories, it works. However, if there are many\r\npotential tab names and many are empty, having to click through and find\r\ncontent is not pleasing at all!\r\nBetter solution\r\nA better solution is to generate the tabs programmatically, with code\r\nthat writes itself depending on the contents of the returned\r\ndataset.\r\nI am creating the tabs with Garrick Aden-Buie’s\r\nxaringanExtra package. Have a look at the specifics in the\r\npresentation below.\r\n\r\n\r\nknitr::include_url(\"https://pkg.garrickadenbuie.com/xaringanExtra/panelset/?panelset=r-code#2\")\r\n\r\n\r\n\r\n\r\n\r\nOther useful resources incude this\r\nScience Loft post and this\r\nStack Overflow question.\r\nTools\r\nIn vanilla Rmarkdown you can create the tabs with a simple heading\r\nstructure.\r\n\r\n\r\n## Here are the penguins {.panelset} \r\n\r\n### Biscoe\r\nplot_penguins(\"Biscoe\")\r\n### Dream\r\nplot_penguins(\"Dream\")\r\n\r\n\r\n\r\nI opt to use Pandoc’s\r\nfenced div structure instead, to avoid inadvertently creating\r\nadditional tabs with a third level heading. It looks like this:\r\n\r\n::::: {.panelset}\r\n\r\n::: {.panel}\r\n\r\n[Biscoe]{.panel-name}\r\n\r\nplot_penguins(\"Biscoe\")\r\n:::\r\n    \r\n::: {.panel}\r\n\r\n[Dream]{.panel-name}\r\n\r\nplot_penguins(\"Dream\")\r\n\r\n:::\r\n\r\n:::::\r\n\r\nIt is worth highlighting that the tabset\r\nbegins with ::::: {.panelset} and,\r\nends with five colons :::::\r\nEach individual tab needs to\r\nopen with ::: {.panel},\r\ninclude a [Panel name specified here]{.panel-name},\r\nand\r\nclose with three colons :::\r\nHow does it work?\r\nNow I can programmatically create my tabs and plots within the\r\nfences. I start with a function that makes the tabs. Then I open the\r\ntabset, get a list of the islands present, and walk across each one\r\napplying my make_tab function before closing the\r\ntabset.\r\n\r\nmake_tab <- function(isl) {         # function to make the tabs\r\n  cat(\"::: {.panel}\\n\")             # Open tab\r\n  cat(\"##\", isl, \"{.panel-name}\\n\") # Label tab\r\n  p <- plot_penguins(isl)           # Create plot\r\n  print(p)                          # Display plot\r\n  cat(\"\\n\")                         # Space\r\n  cat(\":::\\n\")                      # Close tab\r\n}\r\n\r\n::::: {.panelset}\r\n\r\nisl_list <- unique(df$island) %>%   # Get a list of islands present in the dataset\r\n    as.character()\r\n\r\ndplyr::walk(isl_list, make_tab)     # make the tabs for each island that is there\r\n\r\n:::::\r\n\r\nGood to remember\r\nWrap code you want executed in cat(), and\r\nprint() your ggplot objects.\r\nSet code options to results = 'asis' for the code\r\nchunk that makes your tabs. According to the Rmarkdown\r\nCookbook:\r\n\r\nThis option tells knitr not to wrap your text output in verbatim code\r\nblocks, but treat it “as is.” This can be particularly useful when you\r\nwant to generate content dynamically from R code.\r\n\r\nPut in lots of line breaks to separate Rmarkdown content,\r\ne.g. frequent uses of cat(\"\\n\") between elements.\r\nDoes it work?\r\nYes! Look at that, just two tabs and no empty plots!\r\n\r\n\r\n\r\n\r\n\r\n\r\nisl_list <- unique(df$island) %>% as.character()\r\n\r\nwalk(isl_list, make_tab)\r\n\r\n\r\nTorgersen\r\n\r\nDream\r\n\r\n\r\n\r\nI hope that this was useful. Please create an issue on Github if you\r\nhave any suggestions.\r\nPostscript\r\nThis is the function that plots the figures:\r\n\r\n\r\nplot_penguins <- function(isl) {\r\n  df %>%\r\n    filter(island == isl) %>%\r\n    ggplot(aes(bill_length_mm, \r\n               bill_depth_mm, \r\n               colour = body_mass_g, \r\n               shape = species)) +\r\n    geom_point(cex = 2) +\r\n    geom_smooth(method = \"lm\", se = F) +\r\n    scale_color_viridis_c(labels = scales::number_format()) +\r\n    labs(\r\n      x = \"Bill length (mm)\",\r\n      y = \"Bill depth (mm)\",\r\n      colour = \"Body mass (g)\",\r\n      shape = \"Penguin species\"\r\n    )\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/j-jayes/Monday-Munch/main/images/cover.jpg",
    "last_modified": "2022-05-20T15:08:45+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-17-flavours-of-cocoa/",
    "title": "Flavours of cocoa",
    "description": "An exploration of chocolate bar reviews",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {
          "interludeone.com": {}
        }
      }
    ],
    "date": "2022-02-15",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nFlavours of Cocoa\r\nBeans?\r\nIngest the data\r\nData cleaning and\r\nfeature engineering\r\nAnalysing the data\r\nBasic descriptives\r\nHave the\r\nratings been increasing over time?\r\nWhat are\r\nthe frequencies of ingredients and percentages?\r\nWhat about the\r\ndifferent countries??\r\nCountry map\r\n\r\nWord model\r\n\r\n\r\nFlavours of Cocoa\r\nWelcome to the first in a series of data screencasts where I attempt\r\nto show you how great the R language is.\r\nThis post follows along with the data screencast and includes the\r\ncode methodically.\r\nIt begins by reading in the data, then tidying it up, analysing it,\r\nmaking some visualizations and then performing some predictive\r\nmodelling.\r\nBeans?\r\nI’m signed up to a fantastic newsletter called “Data is Plural” curated by\r\njournalist Jeremy Singer-Vine. Truly, it is a treasure trove of\r\ninteresting public datasets from all over the internet. You can sign up\r\nto the newsletter here if this sounds\r\nup your alley.\r\nThis week it included a link to a fun selection of chocolate bar\r\nreviews, which Jeremy described as:\r\n\r\nChocolate bar reviews. The Manhattan Chocolate\r\nSociety’s Brady\r\nBrelinski has reviewed 2,500+ bars of craft chocolate since 2006,\r\nand compiles his findings into a copy-paste-able\r\ntable that lists each bar’s manufacturer, bean origin, percent cocoa,\r\ningredients, review notes, and numerical rating.\r\n\r\nA live link to the database is shown below.\r\n\r\n\r\nknitr::include_url(\"http://flavorsofcacao.com/chocolate_database.html\")\r\n\r\n\r\n\r\n\r\n\r\nIngest the data\r\nI’ve copied and pasted the data into an Excel spreadsheet, accessible\r\non my Github\r\nif you want to download it an analyse it yourself.\r\nThe underlying data is served in a Javascript container on the\r\nwebsite rather than vanilla HTML. This makes it a little bit more\r\ndifficult to scrape with a package like rvest, for example.\r\nHence the Excel spreadsheet.\r\nTo begin the analysis, we’ll read in the Excel file using the\r\nreadxl package, and the here package that\r\nhelps us with file paths.1\r\nWe don’t need to load the packages via the\r\nlibrary(readxl) command because we’re only going to use\r\nthem once or twice. Instead we can call the name of the package followed\r\nby two colons and the command, as shown below.\r\n\r\n\r\nlibrary(tidyverse)\r\ntheme_set(theme_light())\r\n# read in the data\r\ndf <- readxl::read_excel(here::here(\"_posts\", \r\n                                    \"2022-01-17-flavours-of-cocoa\", \r\n                                    \"data\", \r\n                                    \"chocolate-bar-ratings.xlsx\"))\r\n\r\n# display the first six rows of tibble\r\nhead(df)\r\n\r\n\r\n# A tibble: 6 x 10\r\n    REF `Company (Manufacturer)` `Company Location` `Review Date`\r\n  <dbl> <chr>                    <chr>                      <dbl>\r\n1  2454 5150                     U.S.A.                      2019\r\n2  2458 5150                     U.S.A.                      2019\r\n3  2454 5150                     U.S.A.                      2019\r\n4  2542 5150                     U.S.A.                      2021\r\n5  2546 5150                     U.S.A.                      2021\r\n6  2546 5150                     U.S.A.                      2021\r\n# ... with 6 more variables: `Country of Bean Origin` <chr>,\r\n#   `Specific Bean Origin or Bar Name` <chr>, `Cocoa Percent` <dbl>,\r\n#   Ingredients <chr>, `Most Memorable Characteristics` <chr>,\r\n#   Rating <dbl>\r\n\r\nThis gives us a tibble (similar to a dataframe) with 10\r\ncolumns (4 numeric and 6 character) and 2,530 individual reviews.\r\nThe column names are a big ugly though:\r\n\r\n\r\n# show column names\r\ndf %>% colnames()\r\n\r\n\r\n [1] \"REF\"                             \r\n [2] \"Company (Manufacturer)\"          \r\n [3] \"Company Location\"                \r\n [4] \"Review Date\"                     \r\n [5] \"Country of Bean Origin\"          \r\n [6] \"Specific Bean Origin or Bar Name\"\r\n [7] \"Cocoa Percent\"                   \r\n [8] \"Ingredients\"                     \r\n [9] \"Most Memorable Characteristics\"  \r\n[10] \"Rating\"                          \r\n\r\nWe can use the janitor package to make the column names snake case\r\n(lower case with words separated by an underscore).\r\n\r\n\r\n# clean names\r\ndf <- df %>% \r\n    janitor::clean_names()\r\n\r\n# show names again\r\ndf %>% colnames()\r\n\r\n\r\n [1] \"ref\"                             \r\n [2] \"company_manufacturer\"            \r\n [3] \"company_location\"                \r\n [4] \"review_date\"                     \r\n [5] \"country_of_bean_origin\"          \r\n [6] \"specific_bean_origin_or_bar_name\"\r\n [7] \"cocoa_percent\"                   \r\n [8] \"ingredients\"                     \r\n [9] \"most_memorable_characteristics\"  \r\n[10] \"rating\"                          \r\n\r\nNow that we have a nice tibble with clean names, we can\r\nask what the data itself looks like. There are many ways to get summary\r\nstatistics of a dataset. I love the skim function from the\r\nskimr package.\r\n\r\n\r\n# skim the dataset\r\nskimr::skim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n2530\r\nNumber of columns\r\n10\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n6\r\nnumeric\r\n4\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\ncompany_manufacturer\r\n0\r\n1.00\r\n2\r\n39\r\n0\r\n580\r\n0\r\ncompany_location\r\n0\r\n1.00\r\n4\r\n21\r\n0\r\n67\r\n0\r\ncountry_of_bean_origin\r\n0\r\n1.00\r\n4\r\n21\r\n0\r\n62\r\n0\r\nspecific_bean_origin_or_bar_name\r\n0\r\n1.00\r\n3\r\n51\r\n0\r\n1605\r\n0\r\ningredients\r\n87\r\n0.97\r\n4\r\n14\r\n0\r\n21\r\n0\r\nmost_memorable_characteristics\r\n0\r\n1.00\r\n3\r\n37\r\n0\r\n2487\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nref\r\n0\r\n1\r\n1429.80\r\n757.65\r\n5.00\r\n802.0\r\n1454.00\r\n2079.00\r\n2712\r\n▆▇▇▇▇\r\nreview_date\r\n0\r\n1\r\n2014.37\r\n3.97\r\n2006.00\r\n2012.0\r\n2015.00\r\n2018.00\r\n2021\r\n▃▅▇▆▅\r\ncocoa_percent\r\n0\r\n1\r\n0.72\r\n0.06\r\n0.42\r\n0.7\r\n0.70\r\n0.74\r\n1\r\n▁▁▇▁▁\r\nrating\r\n0\r\n1\r\n3.20\r\n0.45\r\n1.00\r\n3.0\r\n3.25\r\n3.50\r\n4\r\n▁▁▅▇▇\r\n\r\nGreat! Our reviews are almost all complete.\r\nOnly 3 percent are missing information on the ingredients.\r\nThe reviews begin in 2006, the mean review is from 2014, and the\r\nlatest is from 2021.\r\nThe percent of the bar comprising of cocoa ranges from 42 to 100,\r\nwith a mean of 72.\r\nWe have 62 unique countries of origin for the beans, and 67\r\ncountries of manufacture.\r\nThere are 21 unique combinations of ingredients, comprising of seven\r\nelements in total.\r\nData cleaning and feature\r\nengineering\r\nLet’s have a look at that ingredients column.\r\n\r\n\r\n# count elements of ingredients column\r\ndf %>% \r\n    count(ingredients, sort = T)\r\n\r\n\r\n# A tibble: 22 x 2\r\n   ingredients      n\r\n   <chr>        <int>\r\n 1 3- B,S,C       999\r\n 2 2- B,S         718\r\n 3 4- B,S,C,L     286\r\n 4 5- B,S,C,V,L   184\r\n 5 4- B,S,C,V     141\r\n 6 <NA>            87\r\n 7 2- B,S*         31\r\n 8 4- B,S*,C,Sa    20\r\n 9 3- B,S*,C       12\r\n10 3- B,S,L         8\r\n# ... with 12 more rows\r\n\r\nSo we have a number of ingredients, a dash, and then a key for what\r\nthe ingredients are. Consulting the website reveals that there are seven\r\npossible ingredients:\r\n\r\nkey\r\nvalue\r\nB\r\nBeans\r\nS\r\nSugar\r\nS*\r\nSweetener other than white cane or beet\r\nsugar\r\nC\r\nCocoa Butter\r\nV\r\nVanilla\r\nL\r\nLecithin\r\nSa\r\nSalt\r\n\r\nThese key and value combinations are very sensible - if we have a lot\r\nof data we can save space by using the keys instead of the whole string.\r\nHowever, I would prefer to have them written out, because we’re going to\r\nsplit them into their own columns a little bit later.\r\nWe can use the str_replace_all function from the\r\nstringr package to replace items in the list of ingredients\r\nwith names.\r\n\r\n\r\ndf <- df %>% \r\n  mutate(ingredients = str_replace_all(ingredients, c(\"Sa\" = \"salt\",\r\n                                                      # the * is a special character \r\n                                                      # when writing Regex and so \r\n                                                      # we use the two backslashes to \r\n                                                      # \"escape\" the meaning\r\n                                                      \"S\\\\*\" = \"non_sugar_sweetener\",\r\n                                                      \"B\" = \"beans\",\r\n                                                      \"S\" =  \"sugar\",\r\n                                                      \"V\" = \"vanilla\",\r\n                                                      \"L\" = \"lecithin\",\r\n                                                      \"C\" = \"cocoa_butter\"\r\n                                                      )))\r\n\r\n\r\n\r\nLet’s look again at our ingredients column:\r\n\r\n\r\ndf %>% \r\n    count(ingredients, sort = T)\r\n\r\n\r\n# A tibble: 22 x 2\r\n   ingredients                                        n\r\n   <chr>                                          <int>\r\n 1 3- beans,sugar,cocoa_butter                      999\r\n 2 2- beans,sugar                                   718\r\n 3 4- beans,sugar,cocoa_butter,lecithin             286\r\n 4 5- beans,sugar,cocoa_butter,vanilla,lecithin     184\r\n 5 4- beans,sugar,cocoa_butter,vanilla              141\r\n 6 <NA>                                              87\r\n 7 2- beans,non_sugar_sweetener                      31\r\n 8 4- beans,non_sugar_sweetener,cocoa_butter,salt    20\r\n 9 3- beans,non_sugar_sweetener,cocoa_butter         12\r\n10 3- beans,sugar,lecithin                            8\r\n# ... with 12 more rows\r\n\r\nFantastic! Now we have the number of ingredients, a dash, and then\r\neach ingredient by name in one column. Let’s separate this information\r\ninto two columns so that we can use the number of ingredients as a\r\nfeature.\r\nThe separate function from the tidyr\r\npackage is made just for this purpose. It takes three arguments:\r\nthe name of the column to separate.\r\nnew column names corresponding to the number of elements.\r\nthe separator between elements.\r\n\r\n\r\ndf <- df %>% \r\n  separate(col = ingredients, \r\n           into = c(\"n_ingredients\", \"ingredients\"),\r\n           sep = \"-\") %>% \r\n    # parse_number looks for a number inside a character column and discards the rest\r\n  mutate(n_ingredients = parse_number(n_ingredients),\r\n         # str_squish removes whitespace around the elements in the ingredients column\r\n         ingredients = str_squish(ingredients))\r\n\r\ndf %>% \r\n  select(n_ingredients, ingredients)\r\n\r\n\r\n# A tibble: 2,530 x 2\r\n   n_ingredients ingredients                      \r\n           <dbl> <chr>                            \r\n 1             3 beans,sugar,cocoa_butter         \r\n 2             3 beans,sugar,cocoa_butter         \r\n 3             3 beans,sugar,cocoa_butter         \r\n 4             3 beans,sugar,cocoa_butter         \r\n 5             3 beans,sugar,cocoa_butter         \r\n 6             3 beans,sugar,cocoa_butter         \r\n 7             3 beans,sugar,cocoa_butter         \r\n 8             4 beans,sugar,cocoa_butter,lecithin\r\n 9             4 beans,sugar,cocoa_butter,lecithin\r\n10             4 beans,sugar,cocoa_butter,lecithin\r\n# ... with 2,520 more rows\r\n\r\nNow we have a numeric column with the number of ingredints and a\r\ncolumn called ingredients with each element separated by a comma.\r\nFinally, let’s break the ingredients from a comma separated list into\r\na binary variable for each ingredient. We can use the\r\nrecipes package that is part of the tidymodels\r\nmetapackage - a framework for doing statistical modelling in a tidy\r\nmanner.\r\nFirst we break our ingredients into 6 columns. The problem we run\r\ninto is that for bars that contain different ingredients, the order of\r\nthe ingredients split into the columns is not constant.\r\n\r\n\r\n# df <- df %>% \r\n#   separate(ingredients, into = c(paste0(\"ingredient_\", rep(1:6))),\r\n#            sep = \",\") \r\n\r\n# df %>% \r\n#     select(company_manufacturer , starts_with(\"ingredient_\")) %>%\r\n#     slice(c(1L, 51L, 54L))\r\n\r\n\r\n\r\nPerhaps there is a better way to do this? Separate rows and pivot\r\nwider?\r\n\r\n\r\ndf %>% \r\n    separate_rows(ingredients, sep = \",\") %>%\r\n    count(ingredients)\r\n\r\n\r\n# A tibble: 8 x 2\r\n  ingredients             n\r\n  <chr>               <int>\r\n1 beans                2443\r\n2 cocoa_butter         1668\r\n3 lecithin              493\r\n4 non_sugar_sweetener    76\r\n5 salt                   37\r\n6 sugar                2360\r\n7 vanilla               353\r\n8 <NA>                   87\r\n\r\ndf <- df %>% \r\n    separate_rows(ingredients, sep = \",\") %>%\r\n    filter(!is.na(ingredients)) %>% \r\n    pivot_wider(names_from = ingredients, values_from = ingredients) %>% \r\n    mutate(across(beans:non_sugar_sweetener, ~ ifelse(is.na(.), 0, 1)))\r\n\r\n\r\n\r\nTidymodels\r\n\r\n\r\n# library(tidymodels)\r\n# dummy_multi_choice_rec <- recipe(~ ., data = df) %>%\r\n#   step_dummy_multi_choice(starts_with(\"ingredient_\")) %>%\r\n#   prep()\r\n# \r\n# df <- bake(dummy_multi_choice_rec, new_data = NULL)\r\n\r\n\r\n\r\nAnalysing the data\r\nBasic descriptives\r\n\r\n\r\ndf %>% \r\n  count(rating, sort = T)\r\n\r\n\r\n# A tibble: 12 x 2\r\n   rating     n\r\n    <dbl> <int>\r\n 1   3.5    552\r\n 2   3      506\r\n 3   3.25   453\r\n 4   2.75   320\r\n 5   3.75   295\r\n 6   2.5    156\r\n 7   4      111\r\n 8   2       29\r\n 9   2.25    14\r\n10   1.5      5\r\n11   1        1\r\n12   1.75     1\r\n\r\nScore range between 1 and 4, and the modal value is 3.5.\r\n\r\n\r\n# histogram\r\ndf %>% \r\n  ggplot(aes(rating)) +\r\n  geom_histogram(bins = 14, alpha = .7, fill = \"midnightblue\") +\r\n    labs(x = \"Chocolate bar rating\",\r\n         y = \"Number of bars\")\r\n\r\n\r\n\r\n\r\nHave the ratings\r\nbeen increasing over time?\r\n\r\n\r\ndf %>% \r\n    group_by(review_date) %>% \r\n    summarise(mean_rating = mean(rating)) %>% \r\n    ungroup() %>% \r\n    knitr::kable(digits = 2)\r\n\r\n\r\nreview_date\r\nmean_rating\r\n2006\r\n3.06\r\n2007\r\n3.17\r\n2008\r\n3.04\r\n2009\r\n3.09\r\n2010\r\n3.19\r\n2011\r\n3.27\r\n2012\r\n3.21\r\n2013\r\n3.21\r\n2014\r\n3.20\r\n2015\r\n3.24\r\n2016\r\n3.24\r\n2017\r\n3.36\r\n2018\r\n3.19\r\n2019\r\n3.13\r\n2020\r\n3.26\r\n2021\r\n3.32\r\n\r\nIt certainly seems like the mean rating is increasing over time. What\r\nis driving this?\r\nWe can make a plot of the figures above to see the increasing\r\ntrend.\r\n\r\n\r\ndf %>% \r\n    group_by(review_date) %>% \r\n    summarise(mean_rating = mean(rating)) %>% \r\n    ungroup() %>% \r\n    ggplot(aes(review_date, mean_rating)) +\r\n    geom_point(colour = \"midnightblue\", alpha = .6, size = 5) +\r\n    geom_smooth(method = \"lm\", se = F, colour = \"grey20\") +\r\n    labs(x = \"Date of review\",\r\n         y = \"Mean rating\")\r\n\r\n\r\n\r\n\r\nLet’s make a boxplot to see how the spread of scores has changed over\r\ntime.\r\n\r\n\r\ndf %>%\r\n  ggplot(aes(review_date, rating, group = review_date)) +\r\n  geom_jitter(alpha = .2) +\r\n  geom_boxplot(varwidth = TRUE, fill = \"midnightblue\", alpha = .6)\r\n\r\n\r\n\r\n\r\nIt seems as if the share of bars with very low scores has decreased\r\nover time, while the median value has remained relatively stable over\r\ntime, shown by the bar in the centre of the boxplots.\r\nWhat about making a joy plot or ridgeline plot with the\r\nggridges package? This allows us to see how the spread of\r\nvalues has changed over time.\r\n\r\n\r\nlibrary(ggridges)\r\n\r\ndf %>%\r\n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\r\n  geom_density_ridges() +\r\n  scale_fill_viridis_c(option = \"magma\") +\r\n  theme(legend.position = \"bottom\") +\r\n  guides(fill = guide_colorbar(\r\n    title.position = \"bottom\",\r\n    barwidth = 25,\r\n    title.hjust = .5\r\n  )) +\r\n    labs(y = NULL,\r\n         x = \"Chocolate bar rating\",\r\n         fill = \"Date of review\")\r\n\r\n\r\n\r\n\r\nThis confirms what we saw in the boxplots above: fewer low scores in\r\nmore recent years mean that the mean has increased, while the top of the\r\ndistributions remain largely the same.\r\nWhat\r\nare the frequencies of ingredients and percentages?\r\n\r\n\r\ndf %>% \r\n  mutate(cocoa_percent = round(cocoa_percent, 1)) %>% \r\n  count(cocoa_percent, n_ingredients) %>% \r\n  ggplot(aes(cocoa_percent, n_ingredients, fill = n)) +\r\n  geom_tile() +\r\n  scale_fill_viridis_c() +\r\n  scale_x_continuous(labels = scales::percent_format()) +\r\n  labs(x = \"Cocoa percent\",\r\n       y = \"Number of ingredients\",\r\n       fill = \"Number of bars reviewed\") +\r\n  theme(legend.position = \"bottom\") +\r\n  guides(fill = guide_colorbar(title.position = \"bottom\",\r\n                               barwidth = 25,\r\n                               title.hjust = .5))\r\n\r\n\r\n\r\n\r\nWhat about the different\r\ncountries??\r\n\r\n\r\ndf %>% \r\n  count(country_of_bean_origin, sort = T)\r\n\r\n\r\n# A tibble: 62 x 2\r\n   country_of_bean_origin     n\r\n   <chr>                  <int>\r\n 1 Venezuela                246\r\n 2 Peru                     231\r\n 3 Dominican Republic       220\r\n 4 Ecuador                  201\r\n 5 Madagascar               171\r\n 6 Blend                    144\r\n 7 Nicaragua                100\r\n 8 Bolivia                   79\r\n 9 Colombia                  78\r\n10 Tanzania                  78\r\n# ... with 52 more rows\r\n\r\ndf %>% \r\n  add_count(country_of_bean_origin) %>%\r\n  # only include countries with more than 60 bars\r\n  filter(n > 60) %>% \r\n  group_by(country_of_bean_origin) %>% \r\n  summarise(mean_rating = mean(rating)) %>% \r\n  mutate(country_of_bean_origin = fct_reorder(country_of_bean_origin, mean_rating)) %>% \r\n  ggplot(aes(mean_rating, country_of_bean_origin)) +\r\n  geom_col(fill = \"midnightblue\", alpha = .8) +\r\n  # ensure that x-axis looks appropriate.\r\n  coord_cartesian(xlim = c(3,3.3)) +\r\n    labs(x = \"Average rating for countries of origin with more than 60 bars reviewed\",\r\n         y = NULL)\r\n\r\n\r\n\r\n\r\nCountry map\r\n\r\n\r\nlibrary(tmap)\r\ndata(\"World\")\r\n\r\nworld <- World %>% as_tibble()\r\n\r\n\r\n\r\nTo join our data on chocolate to this map, we need to get coutnry\r\ncodes, using the countrycode package.\r\n\r\n\r\nlibrary(countrycode)\r\n\r\ndf <- df %>% \r\n    mutate(iso_a3 = countrycode(sourcevar = country_of_bean_origin, origin = \"country.name\", destination = \"iso3c\"))\r\n\r\n\r\n\r\n\r\n\r\nlibrary(sf)\r\n\r\ndf_map <- df %>% \r\n    group_by(iso_a3) %>%\r\n    add_count() %>% \r\n    summarise(mean_rating = mean(rating),\r\n              n = n) %>% \r\n    ungroup() %>% \r\n    distinct() %>% \r\n    left_join(world, by = \"iso_a3\")\r\n\r\n\r\n\r\n\r\n\r\ndf_map %>% \r\n    filter(n > 3) %>% \r\n    st_as_sf() %>% ggplot() +\r\n    geom_sf(data = World, fill = \"grey80\", alpha = .5) +\r\n    geom_sf(aes(fill = mean_rating)) +\r\n    scale_fill_viridis_c(trans = \"sqrt\") +\r\n    labs(fill = \"Mean country rating\")\r\n\r\n\r\n\r\n\r\nWord model\r\n\r\n\r\ndf_characteristics <- df %>% \r\n  select(c(most_memorable_characteristics, rating)) %>% \r\n  separate_rows(most_memorable_characteristics, sep = \",\") %>% \r\n  mutate(most_memorable_characteristics = str_squish(most_memorable_characteristics))\r\n\r\n\r\n\r\n\r\n\r\ndf_characteristics %>% \r\n  count(most_memorable_characteristics, sort = T)\r\n\r\n\r\n# A tibble: 948 x 2\r\n   most_memorable_characteristics     n\r\n   <chr>                          <int>\r\n 1 sweet                            260\r\n 2 nutty                            256\r\n 3 cocoa                            242\r\n 4 roasty                           212\r\n 5 creamy                           187\r\n 6 earthy                           181\r\n 7 sandy                            164\r\n 8 fatty                            161\r\n 9 floral                           141\r\n10 intense                          139\r\n# ... with 938 more rows\r\n\r\nWe can start with a naive analysis that looks only at average score\r\nper word. These are the highest scoring words.\r\n\r\n\r\n# df_characteristics %>% \r\n#   group_by(most_memorable_characteristics) %>% \r\n#   add_count() %>% \r\n#   mutate(avg_rating = mean(rating)) %>% \r\n#   ungroup() %>% \r\n#   slice_max(avg_rating, n = 12, with_ties = F)\r\n\r\ndf_characteristics %>% \r\n  group_by(most_memorable_characteristics) %>% \r\n  add_count() %>% \r\n  filter(n > 3) %>% \r\n  mutate(avg_rating = mean(rating)) %>% \r\n  ungroup() %>% \r\n  distinct(most_memorable_characteristics, avg_rating) %>% \r\n  slice_max(avg_rating, n = 12, with_ties = F) %>% \r\n    mutate(avg_rating = round(avg_rating, 2)) %>% \r\n    knitr::kable(col.names = c(\"Most memorable characteristics\", \"Average rating\"))\r\n\r\n\r\nMost memorable characteristics\r\nAverage rating\r\npeanut\r\n3.75\r\nwine\r\n3.75\r\nbalanced\r\n3.73\r\nraspberry\r\n3.70\r\nmild tart\r\n3.69\r\nrobust\r\n3.69\r\nrich choco\r\n3.69\r\nlong lasting\r\n3.62\r\nblackberry\r\n3.61\r\ndark berry\r\n3.61\r\nsubtle\r\n3.61\r\ndelicate\r\n3.60\r\n\r\n\r\n\r\nlibrary(tidymodels)\r\nlibrary(textrecipes)\r\n\r\ndf_characteristics_folds <- vfold_cv(df_characteristics)\r\n\r\nglmnet_recipe <- \r\n  recipe(formula = rating ~ ., data = df_characteristics) %>% \r\n  step_tokenize(most_memorable_characteristics) %>% \r\n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %>% \r\n  step_tf(most_memorable_characteristics) %>% \r\n  step_normalize(all_predictors(), -all_nominal())\r\n\r\n\r\n\r\n\r\n\r\nglmnet_recipe %>% prep() %>% juice()\r\n\r\n\r\n# A tibble: 6,839 x 101\r\n   rating tf_most_memorable_charact~ tf_most_memorab~ tf_most_memorab~\r\n    <dbl>                      <dbl>            <dbl>            <dbl>\r\n 1   3.25                    -0.0767          -0.0630          -0.0805\r\n 2   3.25                    -0.0767          -0.0630          -0.0805\r\n 3   3.25                    -0.0767          -0.0630          -0.0805\r\n 4   3.5                     -0.0767          -0.0630          -0.0805\r\n 5   3.5                     -0.0767          -0.0630          -0.0805\r\n 6   3.5                     -0.0767          -0.0630          -0.0805\r\n 7   3.75                    -0.0767          -0.0630          -0.0805\r\n 8   3.75                    -0.0767          -0.0630          -0.0805\r\n 9   3.75                    -0.0767          -0.0630          -0.0805\r\n10   3                       -0.0767          -0.0630          -0.0805\r\n# ... with 6,829 more rows, and 97 more variables:\r\n#   tf_most_memorable_characteristics_balanced <dbl>,\r\n#   tf_most_memorable_characteristics_banana <dbl>,\r\n#   tf_most_memorable_characteristics_base <dbl>,\r\n#   tf_most_memorable_characteristics_basic <dbl>,\r\n#   tf_most_memorable_characteristics_berry <dbl>,\r\n#   tf_most_memorable_characteristics_bitter <dbl>, ...\r\n\r\n\r\n\r\nglmnet_spec <- \r\n  linear_reg(penalty = tune(), mixture = 1) %>% \r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"glmnet\") \r\n\r\nglmnet_workflow <- \r\n  workflow() %>% \r\n  add_recipe(glmnet_recipe) %>% \r\n  add_model(glmnet_spec) \r\n\r\nglmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20)) \r\n\r\nglmnet_tune <- \r\n  tune_grid(glmnet_workflow, df_characteristics_folds, grid = glmnet_grid)\r\n\r\n\r\n\r\n\r\n\r\nglmnet_tune %>% \r\n  autoplot()\r\n\r\n\r\n\r\n\r\n\r\n\r\nglmnet_model_final <- finalize_workflow(glmnet_workflow, glmnet_tune %>% \r\n  select_best())\r\n\r\nfinal_fit <- glmnet_model_final %>% \r\n  fit(df_characteristics)\r\n\r\n\r\n\r\nWhat does the fit look like? These are the terms that have the\r\ngreatest effect on bar rating.\r\n\r\n\r\nfinal_fit %>%\r\n  extract_fit_parsnip() %>%\r\n  tidy() %>%\r\n  filter(term != \"(Intercept)\") %>%\r\n  mutate(term = str_remove(term, \"tf_most_memorable_characteristics_\")) %>%\r\n  mutate(sign = estimate > 0) %>%\r\n  group_by(sign) %>%\r\n  mutate(estimate = abs(estimate)) %>% \r\n  slice_max(estimate, n = 12) %>%\r\n  ungroup() %>%\r\n  mutate(estimate = ifelse(sign == TRUE, estimate, -estimate)) %>% \r\n  mutate(term = fct_reorder(term, estimate)) %>%\r\n  ggplot(aes(estimate, term, fill = sign)) +\r\n  geom_col(show.legend = F) +\r\n  geom_vline(xintercept = 0, lty = 2) +\r\n  scale_fill_brewer(palette = \"Paired\") +\r\n  labs(x = \"Effect of term on chocolate bar score\",\r\n       y = \"Memorable characteristic\")\r\n\r\n\r\n\r\n\r\nWow! have a look at the terms up top - “creamy”, “complex” and “rich”\r\nare good chocolate words. On the other side, “bitter”, “off” and\r\n“chemical” are terms that lower the score of the bar.\r\n\r\nThe here package allows\r\nus to abstract from the specific file path on our local computer and use\r\na generic path that will work on any computer that we download our R\r\nproject to. For example, instead of specifying the path\r\n“C:/Users/Jonathan/Documents/R-work/jjayes_distill/_posts/2022-01-17-flavours-of-cocoa/data/chocolate-bar-ratings.xlsx”\r\nwe can just call the here function from the package with\r\nthe same name - here(“_posts”, “2022-01-17-flavours-of-cocoa”, “data”,\r\n“chocolate-bar-ratings.xlsx”). This is wonderful for switching between,\r\nfor example, a Windows and a Mac, where the slashes are in opposite\r\ndirections and can cause some frustration!↩︎\r\n",
    "preview": "https://raw.githubusercontent.com/j-jayes/screencasts/main/figures/Chocolate_bar_ratings.jpeg",
    "last_modified": "2022-04-19T21:44:24+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-24-fathom-twitter/",
    "title": "R vs Python? R & Python!",
    "description": "Mining the twitter archive of [Fathom Data](https://www.fathomdata.dev/)",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {
          "interludeone.com": {}
        }
      }
    ],
    "date": "2022-01-17",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nWhich language does Fathom Data tweet about more?\r\nTweets!\r\nHashtags\r\nWho is being retweeted?\r\nWhat is the overlap between private and public?\r\nConclusion\r\n\r\n\r\nWhich language does Fathom Data tweet about more?\r\nFathom data are an analytics and data science training firm which offers training in two of the most popular open source languages for statistical and general purpose programming: #Rstats and #Python. In their words:\r\n\r\n\r\nWe use 100% #opensource tools and we contribute to a number of Open Source projects. There's no doubt that Open Source adds value to @fathomdata, but it's hard to know what value we're creating. Some interesting perspectives on quantifying value.https://t.co/hcw5hylTwn\r\n\r\n— Fathom Data (@fathomdata) January 24, 2022\r\n\r\nSome of the staff state a preference for R, and I wanted to see if this preference was borne out in the tweets on their corporate twitter account.\r\nTweets!\r\nTo quantify the references to each language, I used the rtweet package from Michael W. Kearney to mine the Fathom Data timeline, as well as those of founder Andrew Collier.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(rtweet)\r\n# get the tweets!\r\ndf <- get_timelines(c(\"fathomdata\", \"datawookie\"), n = 3200)\r\n\r\n\r\n\r\nWe can see that we collected nearly 450 tweets on the Fathom Data account, and just shy of 3200 from Andrew’s account.\r\n\r\n\r\ndf %>% \r\n    count(screen_name) %>% \r\n    knitr::kable(col.names = c(\"Account\", \"Number of tweets\"))\r\n\r\n\r\nAccount\r\nNumber of tweets\r\ndatawookie\r\n3198\r\nfathomdata\r\n448\r\n\r\nHashtags\r\nLet’s start by looking at the hashtags used on the Fathom Data account, distinguishing between original tweets and retweets.\r\n\r\n\r\nhashtags <- df %>%\r\n  filter(screen_name == \"fathomdata\") %>%\r\n  unnest(cols = c(hashtags)) %>%\r\n  filter(!is.na(hashtags)) %>%\r\n  mutate(hashtags = str_to_lower(hashtags)) %>%\r\n  count(hashtags, is_retweet) %>%\r\n  mutate(is_retweet = if_else(is_retweet == TRUE,\r\n    \"Retweet\",\r\n    \"@fathomdata tweet\"\r\n  ))\r\n\r\n\r\n\r\nIn the column chart below we can visualize the number of times the 12 most frequently used hashtags appear by type of tweet. Evidently original tweets include hashtags about both Rstats and Python, as well as SQL and CSS. The retweets skew heavily towards Rstats though.\r\nI’ve made use of the lovely reorder_within command in combination with the scale_y_reordered command, both from the tidytext package to reorder the hashtags by frequency within the type of tweet. This means that though the same terms appear across facets, they can be in a different order within each.\r\n\r\n\r\nlibrary(tidytext)\r\n# ggsci package for nice blue fill scale\r\nlibrary(ggsci)\r\n\r\nhashtags %>% \r\n  group_by(is_retweet) %>% \r\n  slice_max(n, n = 12, with_ties = F) %>% \r\n  ungroup() %>% \r\n  mutate(language = str_detect(hashtags, \"rstats|python|sql|css|markdown\")) %>% \r\n  mutate(hashtags = reorder_within(hashtags, n, is_retweet)) %>% \r\n  ggplot(aes(n, hashtags, fill = n, colour = language)) +\r\n  geom_col(show.legend = F) +\r\n  scale_fill_material(\"blue\") +\r\n  scale_color_manual(values = c(\"white\", \"red\")) +\r\n  scale_y_reordered() +\r\n  facet_wrap(~ is_retweet, scales = \"free\") +\r\n  labs(x = \"Number of tweets tagging\",\r\n       y = \"Hashtag\",\r\n       caption = \"Note: red outline on bar indicates language\")\r\n\r\n\r\n\r\n\r\nFigure 1: Column chart of hashtag frequency by tweet type\r\n\r\n\r\n\r\nAnother way to compare these is the stacked column chart below, which shows the number of uses of hashtags for each language as a percentage of the total number of language tags. Rstats tags make us just less than half of the tweets posted to the Fathom Data account, but more than 90 percent of the retweet tags! Wow! It certainly seems like the person in charge of the twitter account enjoys sharing tweets about Rstats.\r\n\r\n\r\nhashtags %>%\r\n  filter(hashtags %in% c(\"rstats\", \"python\", \"sql\", \"css\")) %>%\r\n  mutate(hashtags = fct_reorder(hashtags, n),\r\n         is_retweet = fct_relevel(is_retweet, \"Retweet\")) %>%\r\n  ggplot(aes(n, is_retweet, fill = hashtags)) +\r\n  geom_col(position = \"fill\") +\r\n  scale_x_continuous(labels = scales::percent_format()) +\r\n  labs(\r\n    x = \"Share of tweets tagging langauge\",\r\n    fill = \"Language\",\r\n    y = NULL\r\n  ) +\r\n  scale_fill_npg(\r\n    guide = guide_legend(reverse = TRUE)\r\n  )\r\n\r\n\r\n\r\n\r\nFigure 2: Stacked column chart comparing language hashtag frequency by tweet type\r\n\r\n\r\n\r\nLooking at text instead of hashtags we see that Rstats make up 79 percent share of the langauge mentions of Rstats and Python.\r\n\r\n\r\ndf %>%\r\n  select(text) %>%\r\n  mutate(\r\n    rstats = str_detect(text, \"rstats\"),\r\n    python = str_detect(text, \"python\")\r\n  ) %>%\r\n  summarise(across(c(rstats, python), sum)) %>%\r\n  pivot_longer(everything(),\r\n    names_to = \"language\",\r\n    values_to = \"n\"\r\n  ) %>% \r\n    mutate(share = scales::percent(n / (496 + 134)))\r\n\r\n\r\n# A tibble: 2 x 3\r\n  language     n share\r\n  <chr>    <int> <chr>\r\n1 rstats     496 79%  \r\n2 python     134 21%  \r\n\r\nWho is being retweeted?\r\nIn the table below we can see that several accounts which aggregate informative Rstats content feature in the top retweets on the Fathom Data feed. One of the reasons I love Rstats is that the community is so willing to share their knowledge. The Fathom Data feed is an example of this.\r\n\r\n\r\ndf %>%\r\n  filter(\r\n    str_detect(text, \"rstats\"),\r\n    screen_name == \"fathomdata\",\r\n    !is.na(retweet_screen_name)\r\n  ) %>%\r\n  count(retweet_name, sort = T) %>%\r\n  slice_head(n = 12) %>%\r\n  knitr::kable(col.names = c(\"Accout name\", \"Number of retweets\"))\r\n\r\n\r\nAccout name\r\nNumber of retweets\r\nAndrew B. Collier\r\n43\r\nR-bloggers\r\n19\r\nMegan Beckett\r\n16\r\nR posts you might have missed!\r\n10\r\nsatRday South Africa\r\n6\r\nMara Averick\r\n5\r\nAstrid Radermacher\r\n3\r\nRStudio\r\n3\r\nTom Mock\r\n3\r\nFathom Data\r\n2\r\nR-Ladies Cape Town\r\n2\r\nR Weekly Live\r\n2\r\n\r\nWe can also see that Fathom founder Andrew Collier appears at the top of the list of Rstats retweets from the Fathom account, with 43. The next section looks at how his feed and the Fathom Data feed overlap.\r\nWhat is the overlap between private and public?\r\nFollowing the fantastic resource Text Mining with R the code chunk below calculates the frequency with which each word used in the tweets of the Fathom Data accound and by Andrew Collier as a share of the total number of distinct words used.\r\n\r\n\r\n# taking out URLs\r\nremove_bits <- \"&amp;|&lt;|&gt;\"\r\n\r\ntidy_tweets <- df %>%\r\n  filter(!str_detect(text, \"^RT\")) %>%\r\n  mutate(text = str_remove_all(text, remove_bits)) %>%\r\n  unnest_tokens(word, text, token = \"tweets\") %>%\r\n    # remove stopwords\r\n  filter(\r\n    !word %in% stop_words$word,\r\n    !word %in% str_remove_all(stop_words$word, \"'\"),\r\n    str_detect(word, \"[a-z]\")\r\n  ) \r\n\r\nfrequency <- tidy_tweets %>% \r\n  group_by(screen_name) %>% \r\n  count(word, sort = TRUE) %>% \r\n  left_join(tidy_tweets %>% \r\n              group_by(screen_name) %>% \r\n              summarise(total = n())) %>%\r\n  mutate(freq = n/total)\r\n\r\nfrequency <- frequency %>% \r\n  select(screen_name, word, freq) %>% \r\n  pivot_wider(names_from = screen_name, values_from = freq) %>%\r\n  arrange(desc(datawookie, fathomdata))\r\n\r\n\r\n\r\nThe visualization below shows the relative frequencies for terms used by both accounts. Words near the red line are used in about equal proportion by the Fathom account and by Andrew, while words above the line are more specific to the Fathom account and words below the line are more specific to Andrew.\r\nIn the top right of the plot we can see that #datascience, data and #Rstats are used frequently by both accounts, and roughly in equal proportions. Terms like courses, #sql and keynote are more specific to the Fathom account, while #linux and binance are more specific to Andrew’s tweets.\r\n\r\n\r\nfrequency %>% \r\n  ggplot(aes(datawookie, fathomdata)) +\r\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\r\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\r\n  scale_x_log10(labels = percent_format()) +\r\n  scale_y_log10(labels = percent_format()) +\r\n  geom_abline(color = \"red\") +\r\n  theme(legend.position = \"none\") +\r\n    labs(x = \"Andrew Collier\",\r\n         y = \"Fathom Data\")\r\n\r\n\r\n\r\n\r\nFigure 3: Comparing the frequency of words used by the Fathom Data account and Andrew Collier\r\n\r\n\r\n\r\nInterestingly, standard bank stands out in the bottom right of the plot, highlighted in the version below. It seems that Andrew uses his personal account to call out poor customer service!\r\n\r\n\r\nfrequency %>% \r\n  mutate(highlight = word == \"@standardbankza\") %>% \r\n  ggplot(aes(datawookie, fathomdata)) +\r\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\r\n  geom_text(aes(label = word, colour = highlight), check_overlap = TRUE, vjust = 1.5) +\r\n  scale_x_log10(labels = percent_format()) +\r\n  scale_y_log10(labels = percent_format()) +\r\n  geom_abline(color = \"red\") +\r\n  scale_color_manual(values = c(\"grey80\", \"midnightblue\")) +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\n\r\nFigure 4: Highlighting Standard Bank account mentions\r\n\r\n\r\n\r\nThis is clearly the case! Have a look at the top terms, haha!\r\n\r\n\r\nfrequency %>%\r\n  filter(is.na(fathomdata)) %>%\r\n  select(-fathomdata) %>%\r\n  mutate(datawookie = scales::percent(datawookie)) %>%\r\n  slice_head(n = 10) %>%\r\n  knitr::kable(col.names = c(\"Word\", \"Relative specificity to Andrew Collier's tweets\"))\r\n\r\n\r\nWord\r\nRelative specificity to Andrew Collier’s tweets\r\nkak\r\n0.6780%\r\n#customerservice\r\n0.6723%\r\n@rainsouthafrica\r\n0.5807%\r\nmind\r\n0.5264%\r\nmbps\r\n0.4291%\r\nprepared\r\n0.3805%\r\nlinux\r\n0.3376%\r\n#itcanbe\r\n0.3290%\r\n#fixit\r\n0.2374%\r\nworst\r\n0.2260%\r\n\r\nConclusion\r\nIt appears that Fathom has a good mix of open source languages in use, and on their twitter, with a slight skew towards Rstats.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-24-fathom-twitter/fathom-twitter_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-01-24T16:16:25+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-07-28-spanish-flu-mortality/",
    "title": "Draft: Spanish Flu Mortality",
    "description": "This paper examines mortality during the Spanish flu pandemic of 1918 in 15 districts in the Cape Province of South Africa.",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {}
      }
    ],
    "date": "2021-07-28",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nThe 1918 Spanish flu pandemic in South Africa: assesing mortality\r\nAbstract\r\nIntroduction\r\nHistorical background and related literature\r\nData\r\nClassification of race from ethnicity\r\n\r\nHow many died during the pandemic?\r\nWhat were the mortality rates in each district during the pandemic?\r\n\r\nDistrict features correlated with pandemic mortality rates\r\nPre-existing health\r\n\r\nConclusion\r\nAppendix\r\nLinear Support Vector Machine specification and workflow\r\nRespiratory disease deaths\r\n\r\n\r\n\r\nThe 1918 Spanish flu pandemic in South Africa: assesing mortality\r\nAbstract\r\nThis paper examines mortality during the Spanish flu pandemic of 1918 in 15 districts in the Cape Province of South Africa. I make use of 40,000 digitized Forms of Notice of a Death produced in the Cape districts between 1915 and 1920. I use three methods to assess excess mortality across the districts, and assess the extent to which measures of pre-existing health and social status which are correlated with mortality from the flu. I use a linear support vector machine to collapse the recorded ethnicity of individuals to a three-class race grouping. I find after assessing excess mortality that it is unlikely there was significant under counting of the number of deaths during the Spanish flu pandemic due to misclassification of cause. Regarding the features correlated with pandemic mortality, I find that pre-existing levels of health in a district indicated by child mortality are strongly correlated with mortality rates during the pandemic. I find that district with high levels of mortality from pneumonia and influenza prior to the pandemic suffered severely from mortality during the pandemic. Finally, I find no support for the notion that socio-economic factors proxied by the literacy of a family member or foreign born status are correlated with pandemic mortality rates.\r\nIntroduction\r\nThe Spanish flu was one of the worst pre-cursors to the current Covid-19 pandemic, extracting a high death toll on working age populations the world over. The disease spread rapidly, and took a heavy toll on populations with poor pre-existing health conditions, and those who could not take measures to lessen the burden of the disease once infected. It is worth looking back into the past to find out how the pandemic engulfed the civil apparatus who recorded mortality from the flu at the time, as well as trying to identify features that existed in communities prior to the pandemic which were correlated with high mortality during the flu. This is what I attempt in the paper below. I cover some of the literature which guides my investigation, examine excess mortality during the pandemic, and identify features associated with high mortality across 15 districts in the Cape.\r\nHistorical background and related literature\r\nThe Spanish flu reached South Africa in mid 1918. A first, less virulent strain spread through Natal and into the Witwatersrand in winter, before a more severe strain gripped the Cape in October of 1918 as soldiers returned from the First World War. Historian Howard Phillips argues that soldiers embarking on the Jaroslav and the Veronej from Freetown in Sierra Leone acquired the disease in port, and brought it with them on their homeward journey before transmitting it to their families and neighbours in the Cape (1984, 9). The Cape was ravaged by the pandemic during “Black October” of 1918, and like populations across the world, working age adults suffered high mortality, threatening livelihoods in the midst of and in the aftermath of the pandemic (Arthi and Parman 2021). Fourie and Jayes (2021) have shown that more than half of those who died during the pandemic in a sample of districts in the Cape were between 18 and 50 years of age, evidencing a “w-shaped” mortality curve, in contrast to the more common “u-shaped” curve. Taubenberger and Morens (2006) show that this pattern held across the world during the 1918 pandemic, with more than half of those recorded dead between 20 and 40 years of age.\r\nIn explaining the severity of the disease, Arthi and Parman (2021) note that the novelty of the virus “contributed to its destruction,” detailing the time that it takes for a community to detect the disease, appreciate its epidemiology, create medicines and reach immunity which allowed the flu to rapidly spread and harm populations across the world (2021, 13).\r\nThere is strong evidence of an immunizing effect that the less severe Natal strain provided in South Africa, though this was not known at the time. Phillips (1984) cites the Influenza Epidemic Commission of 1919 in South Africa suggest that different strains of organisms caused the two waves of the pandemic in the country, but that regarding the immunizing effect, “no definite pronouncement is possible on this subject” (1984, 293). However, modern research has shown that cross-protection certainly lessened the impact of the flu among those exposed to the earlier, milder variant. Shanks, Brundage, and Frean (2010) compare mortality rates among diamond miners in Kimberley and gold miners on the Rand during Black October. They find a mortality rate 35 times higher among Kimberley’s diamond miners than Rand gold miners which the authors attribute to the formers’ lack of exposure to the less lethal strain earlier in the year. The authors control for the characteristics of the miners in their comparison, including recruitment, working conditions, housing and medical care (Shanks, Brundage, and Frean 2010, 2).\r\nSimilarly, in Canada K. Bogaert (2015) studies protection from initial exposure to the less virulent strain among soldiers in the Canadian Expeditionary Force and finds that soldiers who were exposed to pneumonia and influenza prior to the second wave saw 82 percent protection from severe illness and 84 percent protection from death compared to compatriots who were later recruited from rural farming districts and had supposedly not been exposed to the first strain (2015, 34). This finding is replicated among military camps in the United States at the time. Barry, Viboud, and Simonsen (2008) find that as early as March 1918, a mild strain of the virus was circulating in military camps and provided naturally acquired immunity to the more virulent strain that followed in the fourth quarter of the year.\r\nThere is discussion by Shanks and Brundage (2016) regarding whether the two waves studied in soldiers in Canada, the United States, Australia and South Africa were caused by different viruses or different strains of the same virus, while K. L. Bogaert (2016) argues the distinction does not take away from the crucial point, initial exposure provided protection from the virulent wave in the second half of 1918.\r\nThe severity of the October wave may explain why South Africa was spared the third wave of early 1919 detailed by Taubenberger and Morens (2006) and the fourth wave that spread in Asia in mid 1919 noted by Lin and Liu (2014) and Ogasawara (2018) in their analysis of the long-run effects of in utero exposure (in Taiwan and China respectively). Phillips (1984) argues that although the epidemic’s presence was lethal, “it was short-lived and by the middle of November it had virtually abated” (1984, 294).\r\nA wealth of research has sought to examine what features explain the differing levels of mortality in disparate communities during the pandemic. Acuña-Soto, Viboud, and Chowell (2011) examine features correlated with mortality from the flu in 66 large cities across the United States. The authors find that baseline health status was important, as proxied by pre-pandemic pneumonia, which they argue is “a bacterial condition with a strong biological interaction with the influenza virus” and infant mortality rates as a surrogate for population health (2011, 13). Clay, Lewis, and Severnini (2018) study the effect of pollution from coal fired power plants on pandemic mortality in the United States, and find that poor air quality was “an important cause of mortality during the pandemic” (2018, 79). The authors find that pollution was a significant contributor to deaths during the pandemic. They find that the magnitude of the effect was approximately half of the effects of pre-existing levels of health in a community. A recent study on fog events in London corroborates the interaction between pollution and mortality from respiratory diseases (Hanlon 2018). Arthi and Parman (2021) in their meta-analysis, as well as Chowell et al. (2008) who study the flu in England and Wales, and Mamelund (2006) who examines the flu in Denmark all find that socio-economic status impact health outcomes. They collectively find that those who live in high density housing, for example the poor in cities, had poorer pre-existing health as measured by cases of and deaths from pneumonia and influenza, and high mortally rates during the pandemic. Clay, Lewis, and Severnini (2018) posit that further socio-economic indicators such as foreign born status and illiteracy may have precluded residents from implementing public health guidelines. Grantz et al. (2016) examine the differential mortality from the pandemic across Chicago and find that higher rates of illiteracy correlated with higher rates of mortality across districts. Importantly, these socio-demographic effects evidenced spillovers, as literate and native born residents in districts with high shares of effects of illiteracy and foreign born residents saw a higher mortality than compatriots with similar observable characteristics (Arthi and Parman 2021).\r\nData\r\nThe sources of data available to a researcher without doubt shape the design of their study. In his authoritative account of the Spanish flu pandemic in South Africa, historian Howard Phillips notes that prior to his own study, “extensive documentation… from [unpublished evidence] in the report of the Government-appointed Commission of Inquiry” had laid on the shelves of the Library of Parliament unexamined (1984, 9). His analysis of this information told the story of the pandemic in a way that had not been told before. Similarly, the digitization of death records allows different kinds of questions about the pandemic of 1918. Specifically it opens up an assessment of excess mortality and an indication of the importance of pre-existing levels of health in determining pandemic mortality rates within the districts of the Cape.\r\nThe dataset analysed here is made up of 40,000 digitized Forms of Information of a Death of individuals in 15 districts of the Cape Province between 1915 and 1920. The Forms are recorded as of Act No. 7 of 1894 of the Cape Colony. They contain demographic information about the deceased, details about their death, and details about the informant who told the Registrar about their passing. Most often the informant was a family member, as recorded on the Forms. In this study, the date of death, cause of death and demographics are of particular interest. The years prior to the pandemic serve as a baseline against which to compare excess mortality during the pandemic. There are some limitations regarding the recording of cause of death and duration of last illness. These are discussed in detail in Fourie and Jayes (2021). In short, where a doctor has tended to the deceased and completed a certificate, the Form will include the pertinent information from the Medical Practitioners certificate. In the absence of such a certificate, the Informant can provide this information if known. The text on the Form reads:\r\n\r\nN.B. – If the certificate of a Medical Practitioner is produced, the causes of Death and Duration of Illness must be recorded in the Registration Book by the Deputy Registrar and Assistant to the Deputy Registrar as stated in such Certificate, which is to be attached to the form.\r\nSource: Civil Deaths 1895-1972 database with images National Archives Pretoria (1918)\r\n\r\nClassification of race from ethnicity\r\nRace is one important feature of the analysis which follows. As noted in Fourie and Jayes (2021), health and wealth in the Cape Province of 1918 were “closely correlated to race… with race being a strong predictor of income and social status” (2021, 2). The Forms of Information of a Death provide a field for the race of the deceased to be recorded. However, often ethnicity of the deceased was recorded instead. See for example the record of Wills Bunu, who died in Queenstown on the 26th of October 1918 whose race is recorded as “Fingo.” Fingo is a term for a group of ethnicities who fled Natal in the aftermath of the Mfecane to reside in the Xhosa speaking Eastern region of the Cape (Stapleton 1995, 362). Mtumane (2017) argues that the Fingo were “were embraced and integrated” by Xhosa in the Eastern Cape by the time of the Spanish flu (Mtumane 2017, 68). In any event, some categorization has to be done to limit the number of different categories of recorded ethnicity.\r\nForm of Information of the death of Wills Bunu. Source: Civil Deaths 1895-1972 database with images National Archives Pretoria (1918)Further, more than 200 ethnicities appear only once among the records, such that the distribution of observed ethnicities is skewed to the right, with 419 different ethnicities recorded, evident in Figure 1.\r\n\r\n\r\nShow code\r\n\r\n# jpeg(\r\n#     filename=\"images/ethnicity-distribution.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf %>% \r\n  count(race_original, sort = T) %>% \r\n  ggplot(aes(n)) +\r\n  geom_histogram(fill = \"midnightblue\") +\r\n  scale_x_log10() +\r\n  labs(x = \"Number of observations of a recorded ethnicity (log scale)\",\r\n       y = \"Number of ethnicities\")\r\n\r\n\r\n\r\n\r\nFigure 1: Distribution of ethnicities recorded on Forms of Information of a Death\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nTo classify these 419 different categorical values down into a more manageable number; 3 broad race groupings in use in the Cape at the time and in South Africa today, I use a combination of regular expression and a linear support vector machine learning algorithm.\r\nTo simplify the problem I hand coded the minority class (white) using regular expression (for example, coding “European” and “English” to white), and used a linear support vector machine to classify the remaining observations to black and coloured based on a hand-coded training set. Suthaharan (2016) explains that a linear support vector machine is well suited to binary classification using text data, as it can make use of many sub-word features in a computationally cheap manner. It performs best when the classes in the training set are balanced. Because white individuals make up a minority of the dataset, it is possible to simply hand-code their observations as white and then make use of the machine learning algorithm to classify the remaining roughly equal classes. The binary linear SVM works by dividing the data domain along a hyperplane “to separate classes in the original domain” (Suthaharan 2016, 2).\r\nThe advantage of the linear SVM as opposed to a more flexible machine learning method, for example a neural network, is that the linear SVM is explainable and it is possible to pull out indicators of how the algorithm performs the classification. In this case, I use the sub-word features of the ethnicities as predictors of race. As detailed in the appendix, I achieve an accuracy of 99.8 percent in the classification problem on previously unseen data. This model is a useful research output as the trained model can be employed by other researches faced with similar data and classification problems. Specifics are detailed in the appendix.\r\nThe output of the classification process is shown in Figure 23 in the appendix for the 12 most common classes for each race.\r\nHow many died during the pandemic?\r\nIn quantifying the number of South Africans who died during the pandemic, there were clear challenges to the recording apparatus of the government at the time. Phillips (1984) notes that “even under normal conditions in 1918, the South African government lacked the means to record every death in the country” – a feature which shows up in the infant and child mortality rates that vary widely between districts calculated from the death notices prior to the pandemic (Phillips 1984, 288). Further, there are likely differences in the quality of records that vary with geography; Phillips (1984) notes that the legislation which stipulated the recording of deaths, the 1894 Births and Deaths Registration Act, “in theory applied to all inhabitants, but in practice was largely a dead letter in predominantly Black rural areas” (Phillips 1984, 288). The text contained on the death notices evidence sensible differences in recording standards by geography, with the direction “If informants in municipalities and special urban areas do not appear personally before [District Registrar] proper Declaration must be completed and attached hereto. In Rural Areas Informants can report in three ways – one being under declaration. Medical Certificates essential in Urban Areas” (Civil Deaths 1895-1972 database with images National Archives Pretoria 1918). Evidently, government officials at the time accounted for the difficulty in recording deaths in rural areas by lessening the requirements. Nonetheless, in comparing pre-pandemic figures with the information presented from the pandemic itself, we require not that under-reporting did not exist, but the less onerous assumption that the level of under-reporting remained constant within each district.\r\nFigure 2 shows a heatmap of the frequency of deaths in each district. The pandemic period is striking. Further, we see lighter coloured areas in larger districts like Oudtshoorn and Queenstown which have higher mortality prior to the pandemic (owing to their larger populations and hence greater number of deaths) than small districts such as Elliot and Sea Point.\r\n\r\n\r\nShow code\r\n\r\n# jpeg(\r\n#     filename=\"images/viridis-deaths.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf %>%\r\n  filter(between(death_date, ymd(\"1915-01-01\"), ymd(\"1920-12-30\"))) %>%\r\n  mutate(death_date = floor_date(death_date, unit = \"week\")) %>%\r\n  count(death_date, town) %>%\r\n  pivot_wider(names_from = \"town\", values_from = \"n\", values_fill = 0) %>%\r\n  pivot_longer(-death_date, names_to = \"town\", values_to = \"n\") %>%\r\n  mutate(town = fct_reorder(town, n)) %>%\r\n  ggplot(aes(death_date, town, fill = n)) +\r\n  geom_tile() +\r\n  scale_fill_viridis_c(trans = \"sqrt\") +\r\n  labs(\r\n    x = NULL,\r\n    y = NULL,\r\n    fill = \"Number of deaths per week\"\r\n  ) +\r\n  theme(legend.position = \"bottom\") +\r\n  guides(fill = guide_colourbar(\r\n    barwidth = 30, barheight = .5,\r\n    title.position = \"top\",\r\n    title.hjust = .5\r\n  ))\r\n\r\n\r\n\r\n\r\nFigure 2: Heatmap of deaths per week\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nThere is considerable variation in the crude death rate by district during the pandemic, as shown in Figure 3. Here we can see two important features of the pandemic in our sample of Cape districts. First, the pandemic arrived in different districts at different times. In districts close to Cape Town and along the train line to Kimberley, where the soldiers arrived soon after they were released from quarantine at port, the virus spread rapidly. The evidence in this paper corroborate that of Phillips (1984) regarding the swift abatement of the virus following the peak in October. Only in Oudtshoorn and Komga do the effects of the flu appear to have lingered, with elevated mortality rates into December of 1918. Second, the rate of death, and likely the spread of the disease, differed between districts. For example, the steep surge of cases in Queenstown contrasts with a sustained deadly spread in Oudtshoorn.\r\n\r\nCrude death rate is the total number of deaths during a given time interval divided by the population.\r\n\r\n\r\nShow code\r\n\r\n# jpeg(\r\n#     filename=\"images/ridge-plot.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf %>%\r\n  filter(between(death_date, ymd(\"1918-09-01\"), ymd(\"1918-12-30\"))) %>%\r\n  mutate(death_date = floor_date(death_date, unit = \"days\")) %>%\r\n  count(death_date, town) %>%\r\n  pivot_wider(names_from = \"town\", values_from = \"n\", values_fill = 0) %>%\r\n  pivot_longer(-death_date, names_to = \"town\", values_to = \"n\") %>%\r\n  inner_join(df %>%\r\n    filter(between(death_date, ymd(\"1918-09-01\"), ymd(\"1919-08-30\"))) %>%\r\n    mutate(death_date = floor_date(death_date, unit = \"3 days\")) %>%\r\n    count(death_date, town) %>%\r\n    group_by(town) %>%\r\n    slice_max(n, n = 1) %>%\r\n    ungroup() %>%\r\n    arrange(death_date) %>%\r\n    select(town) %>%\r\n    mutate(row_num = row_number())) %>%\r\n  mutate(town = fct_reorder(town, row_num, .desc = T)) %>%\r\n  ggplot(aes(x = death_date, y = town, height = n, fill = town, group = town)) +\r\n  geom_density_ridges(\r\n    stat = \"identity\", show.legend = F,\r\n    alpha = .8\r\n  ) +\r\n  scale_fill_manual(values = wesanderson::wes_palette(\"Zissou1\", 15, type = \"continuous\")) +\r\n  labs(\r\n    x = \"Date of death\",\r\n    y = NULL\r\n  )\r\n\r\n\r\n\r\n\r\nFigure 3: Ridge plot of number of deaths in district by date\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nWhat were the mortality rates in each district during the pandemic?\r\nCandidate methods for assessing (excess) mortality.\r\nA question that poses some difficulties to answer is how many South Africans died during the pandemic, and how many died specifically of the flu.\r\nA great benefit of the dataset at hand is the recording of a cause of death on the notice. This was either copied from the Certificate of the Medical Practitioner, or in the absence of such a certificate, told to the registrar by the informant. Hence, the most intuitive method to assess how many people died in each district of the Spanish flu is simply to count the number of deaths recorded with “Spanish flu” listed in the cause of death column. However, there are a number of drawbacks to this method.\r\nDoctors and families who reported to the registrar the cause of death may not have known that the deaths at the beginning of the pandemic were due to the Spanish flu as news of the disease likely travelled across the country slower than the disease itself, as explained by Arthi and Parman (2021).\r\nThere is likely some selection into the provision of a cause of death by location within a district. As noted in the data explanation, deaths in rural areas were not required to have a certificate of a medical practitioner who would have investigated the death and recorded its cause.\r\nThere are discrepancies in the reporting of the disease by district. Contrast for instance the labelling of deaths in Cradock and Paarl. In the top facet of Figure 4, it is evident that the vast majority of deaths during the pandemic are labelled as Spanish flu deaths. In contrast, fewer than half of the deaths that occurred during the pandemic are labelled as Spanish flu deaths in Paarl.\r\nTo avoid this complication it is tempting to simply count the number of deaths in each district during the pandemic period and conclude that these were caused by the flu. However, districts of different population sizes would have expected different numbers of deaths in the absence of the pandemic. Hence it is not advisable to conclude that all recorded deaths were due to the flu.\r\n\r\n\r\nShow code\r\n\r\n# jpeg(\r\n#     filename=\"images/cradock-paarl-causes.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf %>%\r\n  filter(between(death_date, ymd(\"1918-09-01\"), ymd(\"1919-01-01\")),\r\n         town %in% c(\"Cradock\", \"Paarl\")) %>%\r\n  mutate(death_date = floor_date(death_date, unit = \"week\"),\r\n         causes_of_death = str_to_lower(causes_of_death)) %>%\r\n  mutate(spanish = str_detect(causes_of_death, \"spanish\"),\r\n         spanish = ifelse(spanish == TRUE, \"Cause of death labelled 'Spanish flu'\", \"Other cause of death\")) %>%\r\n    count(death_date, spanish, town) %>%\r\n    pivot_wider(names_from = \"spanish\", values_from = \"n\", values_fill = 0) %>% \r\n    pivot_longer(-c(death_date, town)) %>% \r\n    filter(name != \"NA\") %>% \r\n    ggplot(aes(death_date, value, fill = name)) +\r\n    geom_area() + \r\n    scale_fill_manual(values = wesanderson::wes_palette(\"Zissou1\", 2, type = \"continuous\"),\r\n                      guide = guide_legend(reverse = T)) +\r\n    facet_wrap(~ town, nrow = 2) +\r\n  theme(legend.position = \"bottom\") +\r\n  labs(x = NULL, \r\n       y = \"Number of deaths per week\",\r\n       fill = NULL)\r\n\r\n\r\n\r\n\r\nFigure 4: Density plot comparing causes of death during the pandemic in Cradock and Paarl\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nTable 1 evidences that the majority of deaths in Paarl during the pandemic were labelled simply as “Influenza.”\r\n\r\n\r\nShow code\r\n\r\ndf %>%\r\n  filter(\r\n    between(death_date, ymd(\"1918-09-01\"), ymd(\"1919-01-01\")),\r\n    town %in% c(\"Cradock\", \"Paarl\")\r\n  ) %>%\r\n  count(causes_of_death, town, sort = T) %>%\r\n  group_by(town) %>%\r\n  slice_max(n, n = 12, with_ties = F) %>%\r\n  ungroup() %>%\r\n  select(Cradock = causes_of_death, n_cradock = n) %>%\r\n  head(12) %>%\r\n  bind_cols(df %>%\r\n    filter(\r\n      between(death_date, ymd(\"1918-09-01\"), ymd(\"1919-01-01\")),\r\n      town %in% c(\"Cradock\", \"Paarl\")\r\n    ) %>%\r\n    count(causes_of_death, town, sort = T) %>%\r\n    group_by(town) %>%\r\n    slice_max(n, n = 12) %>%\r\n    ungroup() %>%\r\n    select(Paarl = causes_of_death, n_paarl = n) %>%\r\n    tail(12)) %>% \r\n    knitr::kable(caption = \"Comparison of common causes of death in Cradock and Paarl during the pandemic\")\r\n\r\n\r\nTable 1: Comparison of common causes of death in Cradock and Paarl during the pandemic\r\nCradock\r\nn_cradock\r\nPaarl\r\nn_paarl\r\nSpanish Influenza\r\n444\r\nInfluenza\r\n367\r\nFlu\r\n21\r\nSpanish Influenza\r\n247\r\nInfluenza\r\n20\r\nPneumonia\r\n35\r\nBronchitis\r\n10\r\nSpanish Influenza, Pneumonia\r\n17\r\nBroncho Pneumonia\r\n7\r\nConvulsions\r\n16\r\nConvulsions\r\n6\r\nSpaanse Griep\r\n15\r\nTeething\r\n5\r\nSpanish Influenza (Pneumonia)\r\n15\r\nDiarrhoea\r\n3\r\nBronchitis\r\n14\r\nOld age\r\n3\r\nBroncho Pneumonia\r\n13\r\nConsumption\r\n2\r\nGastro Enteritis\r\n12\r\nInflammation of lungs\r\n2\r\nSpanish Flu\r\n12\r\nPneumonia\r\n2\r\nA) Spanish Influenza B) Pneumonia\r\n11\r\n\r\nAn alternate set of methods for quantifying excess mortality use history as a guide for the number of deaths that we can expect to occur in each district. I use three methods wherein the mortality figures for three years preceding the pandemic are used as a guide, first looking at the average number of deaths in each district during the pandemic peak (October, November and December), second taking an average of the whole year mortality in each district, and third creating regression models with the historical data to predict the expected number of deaths in the pandemic period. I explain each below.\r\nFirst, making use of the death notices from 1915 to 1917 and following the methodology laid out by the South Africa Medical Research Council (SAMRC) in Dorrington et al. (2021), I calculate the average number of deaths per month in each town during the pandemic peak of October, November and December in the three preceding years as a baseline. The SAMRC define excess mortality is calculated as “the number of all-cause deaths in the last week less the number that might have been expected to have occurred” (Dorrington et al. 2021, 2). I widen the bandwidth from one week to one month due to the coarsity of the data at district level. The resulting smoother estimates have less noise and appear reasonable. I then subtract this expected number of deaths from the observed number during the pandemic to calculate excess mortality during the 3 months of pandemic peak in 1918. The advantage of this method is that it captures the seasonal variation in mortality, as shown in Figure 6. It is interesting to note that the the smoothed line shows a peak in mortality in the third quarter of the year in the Cape between 1915 and 1920. This is in contrast to the seasonal pattern in mortality in South Africa as a whole in since 2015 as shown by Dorrington et al. (2021) which displays a dramatic peak during mid-winter shown in 5.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics('images/samrc.PNG')\r\n\r\n\r\n\r\n\r\nFigure 5: Seasonal pattern in mortality in South Africa\r\n\r\n\r\n\r\nSource: Dorrington et al. (2021)\r\n\r\n\r\nShow code\r\n\r\n# jpeg(\r\n#     filename=\"images/seasonality.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf %>% \r\n    filter(sf_indicator ==1) %>% \r\n    mutate(death_date = floor_date(death_date, \"week\")) %>% \r\n    count(death_date) %>% \r\n    mutate(year = year(death_date),\r\n           week = week(death_date)) %>%\r\n    filter(year != 1918,\r\n           year > 1914) %>% \r\n    ggplot(aes(week, n, colour = factor(year), group = year)) +\r\n    geom_point() +\r\n    geom_line() +\r\n    geom_vline(xintercept = c(40, 44), lty = 2) +\r\n    geom_smooth(aes(group = 1)) +\r\n    expand_limits(y = 0) +\r\n    scale_color_manual(values = wesanderson::wes_palette(5, name = \"Zissou1\", type = \"continuous\")) +\r\n    labs(x = \"Week of the year\",\r\n         y = \"Number of deaths per week\",\r\n         colour = NULL) +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\nFigure 6: Mortality by week across districts - excluding 1918\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nThis first method results in estimates of excess mortality shown in Table 2. Here it is evident that while the largest number of deaths occurred in Queenstown and Paarl, the districts of Peddie and Elliot saw more than 10 times as many deaths as they had in the period between October and December in the three years preceding the pandemic.\r\n\r\n\r\nShow code\r\n\r\ndf_samrc <- df %>%\r\n  mutate(\r\n    week = week(death_date),\r\n    year = year(death_date)\r\n  ) %>%\r\n  filter(\r\n    between(week, 40, 53),\r\n    death_date < ymd(\"1918-01-01\")\r\n  ) %>%\r\n  count(town, year) %>%\r\n  group_by(town) %>%\r\n  mutate(mean_deaths_1915_1917 = mean(n)) %>%\r\n  ungroup() %>%\r\n  distinct(town, mean_deaths_1915_1917) %>%\r\n  inner_join(df %>%\r\n    mutate(\r\n      week = week(death_date),\r\n      year = year(death_date)\r\n    ) %>%\r\n    filter(\r\n      between(week, 40, 53),\r\n      year == 1918\r\n    ) %>%\r\n    count(town, year) %>%\r\n    rename(deaths_1918 = n), by = \"town\") %>%\r\n  select(-year) %>%\r\n  arrange(desc(deaths_1918)) %>%\r\n  mutate(\r\n    excess_mortality = deaths_1918 - mean_deaths_1915_1917,\r\n    across(where(is.numeric), round),\r\n    excess_mort_perc = scales::percent(excess_mortality / mean_deaths_1915_1917, accuracy = 1)\r\n  )\r\n\r\ndf_samrc %>% \r\n  knitr::kable(\r\n    col.names = c(\"District\", \"Pre-pandemic mean\", \"Pandemic period\", \"Excess mortality\", \"Percent excess mortality\"),\r\n    caption = \"Excess mortality based on SAMRC method\",\r\n    align = c(\"l\", rep('r', 4))\r\n  )\r\n\r\n\r\nTable 2: Excess mortality based on SAMRC method\r\nDistrict\r\nPre-pandemic mean\r\nPandemic period\r\nExcess mortality\r\nPercent excess mortality\r\nQueenstown\r\n266\r\n1211\r\n945\r\n355%\r\nPaarl\r\n187\r\n951\r\n764\r\n409%\r\nPeddie\r\n75\r\n831\r\n756\r\n1 008%\r\nStellenbosch\r\n157\r\n816\r\n659\r\n420%\r\nStockenstrom\r\n90\r\n708\r\n618\r\n687%\r\nOudtshoorn\r\n245\r\n646\r\n401\r\n164%\r\nMafeking\r\n101\r\n627\r\n526\r\n521%\r\nKomga\r\n73\r\n593\r\n520\r\n712%\r\nCradock\r\n110\r\n551\r\n441\r\n401%\r\nElliot\r\n33\r\n432\r\n399\r\n1 209%\r\nVryburg\r\n58\r\n300\r\n242\r\n417%\r\nKuruman\r\n64\r\n276\r\n212\r\n331%\r\nFraserburg\r\n55\r\n152\r\n97\r\n176%\r\nRiversdale\r\n55\r\n84\r\n29\r\n53%\r\nSea Point\r\n14\r\n17\r\n3\r\n21%\r\n\r\nSecond, I follow the methodology of Acuña-Soto, Viboud, and Chowell (2011) who define excess mortality as “deaths observed during the pandemic years minus the average death counts in the pre-pandemic years” (Acuña-Soto, Viboud, and Chowell 2011, 23). While these authors had access to seven years of data preceding the pandemic, I calculate a simple average of the number of deaths per year in each district in the three prior years and subtract the number of deaths in 1918. Table 3 shows the mortality by district from 1915 to 1917, and the average across these three years.\r\n\r\n\r\nShow code\r\n\r\ndf_acuasoto <- df %>%\r\n  filter(death_date < ymd(\"1918-01-01\")) %>%\r\n  mutate(year = year(death_date)) %>%\r\n  count(town, year) %>%\r\n  group_by(town) %>%\r\n  mutate(average = mean(n)) %>%\r\n  ungroup() %>%\r\n  pivot_wider(names_from = \"year\", values_from = \"n\") %>%\r\n  relocate(average, .after = `1917`) %>%\r\n  mutate(across(-town, ~ round(., 0))) %>%\r\n  inner_join(df %>%\r\n    mutate(year = year(death_date)) %>%\r\n    filter(year == 1918) %>%\r\n    count(town) %>% rename(`1918` = n)) %>%\r\n  mutate(\r\n    excess_mortality = `1918` - average,\r\n    excess_mort_perc = scales::percent(excess_mortality / average, accuracy = 1)\r\n  ) %>%\r\n    arrange(desc(excess_mortality))\r\n\r\ndf_acuasoto %>% \r\n  knitr::kable(\r\n    caption = \"Excess mortality based on Acuña-Soto, Viboud, and Chowell method\",\r\n    col.names = c(\"District\", \"1915\", \"1916\", \"1917\", \"'15-'17 Average\", \"1918\", \"Excess mortality\", \"Percent excess mortality\"),\r\n    align = c(\"l\", rep(\"r\", 7))\r\n  )\r\n\r\n\r\nTable 3: Excess mortality based on Acuña-Soto, Viboud, and Chowell method\r\nDistrict\r\n1915\r\n1916\r\n1917\r\n’15-’17 Average\r\n1918\r\nExcess mortality\r\nPercent excess mortality\r\nPeddie\r\n266\r\n264\r\n270\r\n267\r\n1048\r\n781\r\n293%\r\nQueenstown\r\n843\r\n1083\r\n849\r\n925\r\n1701\r\n776\r\n84%\r\nPaarl\r\n731\r\n715\r\n755\r\n734\r\n1474\r\n740\r\n101%\r\nStockenstrom\r\n264\r\n331\r\n335\r\n310\r\n942\r\n632\r\n204%\r\nStellenbosch\r\n503\r\n556\r\n604\r\n554\r\n1165\r\n611\r\n110%\r\nOudtshoorn\r\n816\r\n768\r\n1019\r\n868\r\n1413\r\n545\r\n63%\r\nKomga\r\n241\r\n338\r\n301\r\n293\r\n823\r\n530\r\n181%\r\nMafeking\r\n344\r\n382\r\n344\r\n357\r\n781\r\n424\r\n119%\r\nElliot\r\n97\r\n143\r\n104\r\n115\r\n525\r\n410\r\n357%\r\nCradock\r\n378\r\n400\r\n382\r\n387\r\n757\r\n370\r\n96%\r\nVryburg\r\n198\r\n214\r\n187\r\n200\r\n453\r\n253\r\n126%\r\nKuruman\r\n182\r\n247\r\n194\r\n208\r\n424\r\n216\r\n104%\r\nFraserburg\r\n244\r\n230\r\n192\r\n222\r\n260\r\n38\r\n17%\r\nRiversdale\r\n199\r\n203\r\n204\r\n202\r\n213\r\n11\r\n5%\r\nSea Point\r\n19\r\n43\r\n70\r\n44\r\n54\r\n10\r\n23%\r\n\r\nFigure 7 shows the excess mortality based on method two. Here we can see that Peddie and Elliot stand out with large excess mortality relative to their mean mortality preceding the pandemic.\r\n\r\n\r\nShow code\r\n\r\n# jpeg(\r\n#     filename=\"images/excess-mortality-method-2.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf %>%\r\n  mutate(year = year(death_date)) %>%\r\n  count(year, town) %>%\r\n  filter(year == 1918) %>%\r\n  rename(deaths_1918 = n) %>%\r\n  inner_join(\r\n    df %>%\r\n      mutate(year = year(death_date)) %>%\r\n      count(year, town) %>%\r\n      filter(year <= 1917) %>%\r\n      group_by(town) %>%\r\n      summarise(avg_deaths_1915_1917 = mean(n))\r\n  ) %>%\r\n  select(-year) %>%\r\n  mutate(town = fct_reorder(town, deaths_1918)) %>%\r\n  pivot_longer(-town, names_to = \"type\", values_to = \"deaths\") %>%\r\n  mutate(type = case_when(\r\n    \r\n    str_detect(type, \"deaths_1918\") ~ \"Mortality in 1918\",\r\n    TRUE ~ \"Mean mortality between 1915 and 1917\"\r\n  )) %>% \r\n  ggplot(aes(deaths, town)) +\r\n  geom_line(aes(group = town)) +\r\n  geom_point(aes(colour = type)) +\r\n  scale_x_continuous(labels = scales::number_format()) +\r\n  theme(legend.position = \"bottom\") +\r\n  labs(\r\n    colour = NULL,\r\n    caption = \"Length of line represents absolute excess mortality\",\r\n    y = NULL,\r\n    x = \"Mortality\"\r\n  )\r\n\r\n\r\n\r\n\r\nFigure 7: Excess mortality based on methodology by Acuña-Soto, Viboud, and Chowell (2011)\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nThird, I use regression models to extrapolate the mortality we would expect in each district if the historical trends from 1915 to 1917 continued.\r\nI use three regression models. These are linear model with only an intercept - in effect the same as the averaging procedure from above. Specifically, I average the number of deaths in the year in the three years prior to 1918 as a guide for the average number of deaths per district.\r\nNext, I use a simple linear regression. This accounts, for instance, for population growth and consequent linear increases in the number of registered deaths per district per year that we would expect with a larger population.\r\nThe linear trend is shown in Figure 8 here, alongside a line of equality:\r\n\r\n\r\nShow code\r\n\r\nlibrary(ggrepel)\r\n\r\n# jpeg(\r\n#     filename=\"images/linear-trend.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf %>%\r\n  mutate(year = year(death_date)) %>%\r\n  count(year, town) %>%\r\n  filter(year <= 1917) %>%\r\n  nest(data = c(year, n)) %>%\r\n  mutate(lm_fit = map(data, ~ lm(n ~ year, data = .x))) %>%\r\n  inner_join(\r\n    df %>%\r\n      mutate(year = year(death_date)) %>%\r\n      count(year, town) %>%\r\n      filter(year > 1917) %>%\r\n      nest(post_data = c(year, n))\r\n  ) %>%\r\n  mutate(augmented = map2(lm_fit, post_data, ~ augment(.x, newdata = .y))) %>%\r\n  unnest(augmented) %>%\r\n  filter(year == 1918) %>%\r\n  ggplot(aes(.fitted, n, label = str_to_title(town))) +\r\n  geom_point() +\r\n  geom_abline() +\r\n  geom_smooth(method = \"lm\", se = F) +\r\n  geom_text_repel() +\r\n  labs(\r\n    x = \"Predicted 1918 mortality based on linear trend\",\r\n    y = \"Observed 1918 mortality\"\r\n  ) \r\n\r\n\r\n\r\n\r\nFigure 8: Linear trend for predicted and excess mortality\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nIs appears that towns with higher predicted mortality in 1918 based on the linear trend have higher actual mortality in 1918.\r\nFurther I use a regression model from Serfling that accounts for the seasonality of respiratory diseases like flu and influenza. According to Cowling et al. (2006), “Serfling’s method uses cyclic regression to model the weekly proportion of deaths from pneumonia and influenza and to define an epidemic threshold that is adjusted for seasonal effects.” (2006, 4).\r\nFigure 9 shows the mean and predicted ranges of expected mortality based on the three methods, as well as the observed pandemic mortality. It is evident that the districts with larger populations also have a larger range in the mortality protections. Oudtshoorn has a wide range in the projected mortality, followed by Paarl and Queenstown.\r\n\r\n\r\nShow code\r\n\r\ndf_reg <- df %>%\r\n  mutate(date = floor_date(death_date, \"week\")) %>%\r\n  count(date, town) %>%\r\n  filter(date <= ymd(\"1918-01-01\")) %>%\r\n  nest(data = c(date, n)) %>%\r\n  mutate(\r\n    lm_fit = map(data, ~ lm(n ~ date, data = .x)),\r\n    q_fit = map(data, ~ lm(n ~ poly(date, 2), data = .x))\r\n  ) %>%\r\n  inner_join(\r\n    df %>%\r\n      mutate(date = floor_date(death_date, \"week\")) %>%\r\n      count(date, town) %>%\r\n      filter(date >= ymd(\"1918-01-01\")) %>%\r\n      nest(post_data = c(date, n))\r\n  ) %>%\r\n  mutate(\r\n    augmented_lm = map2(lm_fit, post_data, ~ augment(.x, newdata = .y)),\r\n    augmented_q = map2(q_fit, post_data, ~ augment(.x, newdata = .y))\r\n  ) %>%\r\n  unnest(augmented_lm, augmented_q) %>%\r\n  filter(between(date, ymd(\"1918-01-01\"), ymd(\"1918-12-31\"))) %>%\r\n  select(town, date, n, .fitted, .fitted1) %>%\r\n  rename(\r\n    linear_trend = .fitted,\r\n    seasonal_trend = .fitted1,\r\n    observed = n\r\n  ) %>%\r\n  group_by(town) %>%\r\n  mutate(across(observed:seasonal_trend, .fns = sum)) %>%\r\n  ungroup() %>%\r\n  distinct(town, observed, linear_trend) %>%\r\n  inner_join(df %>% \r\n    mutate(death_date = floor_date(death_date, \"month\")) %>% \r\n    count(death_date, town) %>% \r\n    filter(death_date < ymd(\"1918-09-01\")) %>%\r\n    group_by(town) %>% \r\n    summarise(mean_deaths = mean(n),\r\n              n =) %>% \r\n    ungroup() %>%\r\n    mutate(mean_deaths = 12 * mean_deaths)) %>%\r\n  inner_join(df %>%\r\n  filter(between(death_date, ymd(\"1915-10-01\"), ymd(\"1915-12-31\"))) %>%\r\n  group_by(town) %>%\r\n  count() %>%\r\n  ungroup() %>%\r\n  mutate(n_1915 = n*3) %>%\r\n  select(-n) %>% \r\n  inner_join(df %>%\r\n    filter(between(death_date, ymd(\"1916-10-01\"), ymd(\"1916-12-31\"))) %>%\r\n    group_by(town) %>%\r\n    count() %>%\r\n    ungroup() %>%\r\n    mutate(n_1916 = n*3) %>% \r\n    select(-n)) %>%\r\n  inner_join(df %>%\r\n    filter(between(death_date, ymd(\"1917-10-01\"), ymd(\"1917-12-31\"))) %>%\r\n    group_by(town) %>%\r\n    count() %>%\r\n    ungroup() %>%\r\n    mutate(n_1917 = n*3) %>% \r\n    select(-n)) %>%\r\n  pivot_longer(-town) %>%\r\n  group_by(town) %>%\r\n  mutate(across(value, .fns = mean)) %>%\r\n  ungroup() %>%\r\n  distinct(town, value) %>%\r\n  rename(mean_deaths_oct_dec = value)) %>% \r\n  mutate(town = fct_reorder(town, observed)) %>% \r\n  pivot_longer(-c(town, observed), names_to = \"type\", values_to = \"n\") %>% \r\n  mutate(type = case_when(\r\n    str_detect(type, \"linear\") ~ \"Linear trend in deaths per month\",\r\n    str_detect(type, \"mean_deaths_oct_dec\") ~ \"Average number of deaths per month (Oct-Dec)\",\r\n    TRUE ~ \"Average number of deaths per month\"\r\n  )) %>% \r\n  group_by(town) %>% \r\n  mutate(mean_projected_deaths = mean(n)) %>% \r\n  ungroup()\r\n\r\n# jpeg(\r\n#     filename=\"images/regression-methods.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf_reg %>% \r\n  ggplot(aes(n, town)) +\r\n  geom_line(aes(group = town, colour = \"Estimate range from regression models\")) +\r\n  geom_point(aes(observed, town, shape = \"Observed pandemic mortality\")) +\r\n  geom_point(aes(mean_projected_deaths, town, shape = \"Mean projected mortality\")) +\r\n  scale_color_manual(values = \"grey50\") +\r\n  guides(color = guide_legend(order = 2),\r\n         shape = guide_legend(order = 1)) +\r\n  labs(\r\n    x = \"Mortality\",\r\n    colour = NULL,\r\n    shape = NULL,\r\n    y = NULL\r\n  ) +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\nFigure 9: Excess mortality based on regression methods\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nIn Table 4 I show the excess mortality based on these regression methods. The estimates of excess mortality are similar to the estimates produced following the Acua-Soto methodology. The excess mortality percentages are lower than that calculated by following the SAMRC methodology due to a lower smaller detonator in calculating the latter, just the three months of the pandemic peak.\r\n\r\n\r\nShow code\r\n\r\ndf_reg %>% \r\n  distinct(town, mean_projected_deaths, observed) %>% \r\n  mutate(excess_mortality = observed - mean_projected_deaths,\r\n         across(where(is.numeric), .fns = round),\r\n         excess_mortality_pct = scales::percent(excess_mortality / mean_projected_deaths, accuracy = 1)) %>% \r\n  arrange(desc(excess_mortality)) %>% \r\n  # mutate(across(where(is.numeric), .fns = ~ scales::number_format(.x, accuracy = 1))) %>%\r\n  knitr::kable(col.names = c(\"District\", \"Projected\", \"Observed\", \"Excess\", \" Percentage excess\"),\r\n               caption = \"Excess mortality based on regression methods\",\r\n               align = c(\"l\", rep(\"r\", 4)))\r\n\r\n\r\nTable 4: Excess mortality based on regression methods\r\nDistrict\r\nProjected\r\nObserved\r\nExcess\r\nPercentage excess\r\nQueenstown\r\n1693\r\n886\r\n807\r\n91%\r\nPaarl\r\n1465\r\n675\r\n790\r\n117%\r\nPeddie\r\n1048\r\n259\r\n789\r\n305%\r\nStockenstrom\r\n946\r\n325\r\n621\r\n191%\r\nStellenbosch\r\n1170\r\n550\r\n620\r\n113%\r\nKomga\r\n827\r\n283\r\n544\r\n192%\r\nOudtshoorn\r\n1410\r\n910\r\n500\r\n55%\r\nMafeking\r\n781\r\n332\r\n449\r\n135%\r\nElliot\r\n527\r\n112\r\n415\r\n371%\r\nCradock\r\n763\r\n364\r\n399\r\n110%\r\nVryburg\r\n455\r\n191\r\n264\r\n138%\r\nKuruman\r\n427\r\n220\r\n207\r\n94%\r\nFraserburg\r\n264\r\n178\r\n86\r\n48%\r\nRiversdale\r\n214\r\n189\r\n25\r\n13%\r\nSea Point\r\n54\r\n51\r\n3\r\n6%\r\n\r\nTable 5 shows the differences in the number and percentage estimates of excess mortality produced following the three methods detailed above. The estimates differ between the methods, and there is not a clear relationship amongst the different methods as to which produces the largest across all the districts.\r\n\r\n\r\nShow code\r\n\r\ndf_excess_mort_comp <- df_samrc %>%\r\n  select(town,\r\n    excess_mortality_samrc = excess_mortality,\r\n    excess_mort_perc_samrc = excess_mort_perc\r\n  ) %>%\r\n  inner_join(df_acuasoto %>%\r\n    select(town,\r\n      excess_mortality_acuasoto = excess_mortality,\r\n      excess_mort_perc_acuasoto = excess_mort_perc\r\n    ), by = \"town\") %>%\r\n  inner_join(df_reg %>%\r\n    mutate(\r\n      excess_mortality = round(observed - mean_projected_deaths, 0),\r\n      excess_mort_perc = scales::percent(excess_mortality / mean_projected_deaths)\r\n    ) %>%\r\n    mutate(\r\n      excess_mortality_regression = excess_mortality,\r\n      excess_mort_perc_regression = excess_mort_perc\r\n    ) %>%\r\n    select(town, excess_mortality_regression, excess_mort_perc_regression) %>%\r\n    distinct(), by = \"town\") %>% \r\n  arrange(desc(excess_mortality_acuasoto))\r\n\r\ndf_excess_mort_comp %>% \r\n    knitr::kable(col.names = c(\"District\", \"SAMRC\", \"SAMRC\", \"Acua-Soto\", \"Acua-Soto\", \"Regression\", \"Regression\"),\r\n               caption = \"Comparison of excess mortality estimates\",\r\n               align = c(\"l\", rep(\"r\", 6)))\r\n\r\n\r\nTable 5: Comparison of excess mortality estimates\r\nDistrict\r\nSAMRC\r\nSAMRC\r\nAcua-Soto\r\nAcua-Soto\r\nRegression\r\nRegression\r\nPeddie\r\n756\r\n1 008%\r\n781\r\n293%\r\n789\r\n305.1%\r\nQueenstown\r\n945\r\n355%\r\n776\r\n84%\r\n807\r\n91.1%\r\nPaarl\r\n764\r\n409%\r\n740\r\n101%\r\n790\r\n117.0%\r\nStockenstrom\r\n618\r\n687%\r\n632\r\n204%\r\n621\r\n191.0%\r\nStellenbosch\r\n659\r\n420%\r\n611\r\n110%\r\n620\r\n112.8%\r\nOudtshoorn\r\n401\r\n164%\r\n545\r\n63%\r\n500\r\n55.0%\r\nKomga\r\n520\r\n712%\r\n530\r\n181%\r\n544\r\n192.5%\r\nMafeking\r\n526\r\n521%\r\n424\r\n119%\r\n449\r\n135.3%\r\nElliot\r\n399\r\n1 209%\r\n410\r\n357%\r\n415\r\n370.6%\r\nCradock\r\n441\r\n401%\r\n370\r\n96%\r\n399\r\n109.7%\r\nVryburg\r\n242\r\n417%\r\n253\r\n126%\r\n264\r\n138.6%\r\nKuruman\r\n212\r\n331%\r\n216\r\n104%\r\n207\r\n94.0%\r\nFraserburg\r\n97\r\n176%\r\n38\r\n17%\r\n86\r\n48.4%\r\nRiversdale\r\n29\r\n53%\r\n11\r\n5%\r\n25\r\n13.2%\r\nSea Point\r\n3\r\n21%\r\n10\r\n23%\r\n3\r\n5.8%\r\n\r\nIn an effort to contextualize these estimates, I compare the excess mortality figures in each town with the number of deaths recorded as caused by respiratory diseases1, as well as figures from the Pandemic Commission of Inquiry referenced by Phillips (1984).\r\n\r\n\r\nShow code\r\n\r\nphillips <- read_rds(\"data/phillips-comparison-august.rds\") %>% \r\n  janitor::clean_names() %>% \r\n  mutate(stat = case_when(\r\n    str_detect(stat, \"Deaths from Death Notices\") ~ \"Respiratory deaths during pandemic\",\r\n    TRUE ~ \"Estimates cited in Phillips\"\r\n  ))\r\n\r\n# jpeg(\r\n#     filename=\"images/phillips-comparison-fig.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf_excess_mort_comp %>%\r\n  select(!contains(\"perc\")) %>%\r\n  pivot_longer(-town) %>%\r\n  group_by(town) %>%\r\n  mutate(value = mean(value)) %>%\r\n  ungroup() %>%\r\n  select(town, value) %>%\r\n  distinct() %>%\r\n  mutate(stat = \"Mean estimated excess mortality\") %>%\r\n  bind_rows(phillips) %>%\r\n  mutate(town = fct_reorder(town, value)) %>%\r\n  filter(town != \"Sea Point\") %>%\r\n  mutate(stat = fct_relevel(stat, \"Estimates cited in Phillips\")) %>% \r\n  ggplot(aes(value, town, fill = stat)) +\r\n  theme(legend.position = \"bottom\") +\r\n  geom_col(position = \"dodge\") +\r\n  scale_fill_manual(values = wesanderson::wes_palette(3, name = \"Zissou1\", type = \"continuous\")) +\r\n  labs(fill = NULL,\r\n       y = NULL,\r\n       x = \"Number of deaths\") +\r\n  guides(fill = guide_legend(nrow = 2))\r\n\r\n\r\n\r\n\r\nFigure 10: Comparison of excess mortality estimates, respiratory deaths and government estimates\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nHere we observe that in the districts with larger populations, the estimates cited in Phillips (1984) are greater than both the number of respiratory deaths, and the mean excess mortality calculated with the three methods detailed above. In contrast, in the smaller districts, from Fryburg to Riversdale, the number of deaths from respiratory illness recorded during the pandemic are greater than the estimates cited in Phillips (1984). This is potentially indicative of under counting on the part of the Commission of Inquiry in the smaller districts.\r\nIn all districts, my estimated excess mortality values are lower than the other two estimates. This means that we need not be concerned about under counting of pandemic deaths, if they are not recorded as such. We would be concerned about this if the excess mortality exceeded the estimates of Spanish flu deaths. In the remainder of the paper I calculate mortality and mortality rates based on respiratory deaths during the pandemic.\r\nDistrict features correlated with pandemic mortality rates\r\nPre-existing health\r\nA wealth of literature has shown that there is a strong correlation between the level of health and prevalence of illnesses in different communities and their outcomes facing a pandemic.\r\nI examine two measures of pre-existing health in the 15 districts which constitute the sample. The first uses infant and child mortality statistics as a proxy for the level of existing health and healthcare in a district. The second assess mortality from influenza and pneumonia prior to the pandemic to gauge levels of health in different communities.\r\nPre-pandemic infant and child mortality rates\r\nI begin with a proxy for pre-existing health in a district by calculating the infant mortality rate prior to the pandemic. I make use of the recorded infant deaths between January 1915 and December 1917, divided by the population of infants in each district as recorded in the Statistics of Population, Vital and Health Statistics of 1916 (Office of Census and Statistics Pretoria 1918). I extrapolate the full population of infants each year by multiplying the 1916 figure by three. This snapshot of the infants born in 1916 could be critiqued as unrepresentative of birth in the two surrounding years, yet it is the best data available.\r\nTable 6 shows the number of births in 1916 in each district, by race, multiplied by 3. Figure 11 shows these figures with a confidence interval around them. It is clear that there is heterogeneity in infant mortality by race and district, such that this may be correlated with pandemic mortality rates. While the infant mortality rates may appear very high, it is possible that there is under-reporting in the number of births by district, while the death of a child is so tragic that it is recorded at a higher rate than the birth of a child. It is not prescient to conclude on these anomalies without additional sources of data.\r\n\r\n\r\nShow code\r\n\r\nbirths_by_town <- read_rds(\"data/births_by_town.rds\")\r\n\r\ndf_infant <- births_by_town %>%\r\n  select(-total) %>%\r\n  pivot_longer(-town, names_to = \"race\", values_to = \"births\") %>%\r\n  mutate(race = str_to_title(race)) %>% \r\n  mutate(births = ifelse(births == 0, 1, births)) %>% \r\n  # for the three years\r\n  mutate(births = 3*births) %>% \r\n  inner_join(df %>%\r\n    filter(\r\n      death_date < ymd(\"1918-01-01\"),\r\n      age_years <= 1,\r\n      race != \"NA\"\r\n    ) %>%\r\n    count(town, race) %>%\r\n    rename(deaths = n), by = c(\"town\", \"race\")) %>% \r\n  mutate(infant_mort_rate = deaths/births,\r\n         low = qbeta(.025, deaths + .5, births - deaths + .5),\r\n         high = qbeta(.975, deaths + .5, births - deaths + .5)) %>% \r\n  filter(births > 5)\r\n\r\ndf_infant %>% \r\n  group_by(town) %>% \r\n  mutate(births_grouped = sum(births, na.rm = T),\r\n         deaths_grouped = sum(deaths, na.rm = T)) %>% \r\n  ungroup() %>% \r\n  mutate(infant_mort_rate_grouped = deaths_grouped / births_grouped) %>%\r\n  select(town, race, infant_mort_rate, infant_mort_rate_grouped) %>% \r\n  pivot_wider(names_from = \"race\", values_from = \"infant_mort_rate\") %>% \r\n  mutate(across(where(is.numeric), .fns = ~ .x*1000),\r\n         across(where(is.numeric), .fns = round)) %>%\r\n  arrange(desc(infant_mort_rate_grouped)) %>% \r\n  knitr::kable(col.names = c(\"District\", \"Aggregate\", \"White\", \"Black\", \"Coloured\"),\r\n               caption = \"Infant mortality by race\")\r\n\r\n\r\nTable 6: Infant mortality by race\r\nDistrict\r\nAggregate\r\nWhite\r\nBlack\r\nColoured\r\nStockenstrom\r\n346\r\n130\r\n448\r\n221\r\nQueenstown\r\n313\r\n76\r\n425\r\nNA\r\nCradock\r\n286\r\n92\r\n450\r\n320\r\nElliot\r\n257\r\n90\r\n574\r\nNA\r\nOudtshoorn\r\n225\r\n106\r\nNA\r\n320\r\nStellenbosch\r\n210\r\n111\r\nNA\r\n243\r\nPaarl\r\n203\r\n69\r\nNA\r\n239\r\nMafeking\r\n190\r\n96\r\n223\r\nNA\r\nFraserburg\r\n178\r\n85\r\nNA\r\n293\r\nKomga\r\n169\r\n48\r\n180\r\nNA\r\nRiversdale\r\n156\r\n91\r\nNA\r\n220\r\nPeddie\r\n144\r\nNA\r\n144\r\nNA\r\nKuruman\r\n132\r\n66\r\n165\r\n204\r\nVryburg\r\n75\r\n49\r\n159\r\n32\r\nSea Point\r\n17\r\n11\r\nNA\r\n52\r\n\r\n\r\n\r\nShow code\r\n\r\n# jpeg(\r\n#     filename=\"images/infant-mortality.jpeg\",\r\n#     width=6,\r\n#     height=6,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf_infant %>% \r\n  mutate(town = reorder_within(town, infant_mort_rate, race)) %>% \r\n  ggplot(aes(infant_mort_rate, town, colour = race)) +\r\n  geom_point() +\r\n  geom_errorbarh(aes(xmin = low, xmax = high)) +\r\n  scale_y_reordered() +\r\n  scale_x_continuous(labels = number_format(scale = 1000)) +\r\n  scale_color_manual(values = wesanderson::wes_palette(3, name = \"Zissou1\", type = \"continuous\")) +\r\n  theme(legend.position = \"none\") +\r\n  facet_wrap(~ race, nrow = 3, scales = \"free_y\") +\r\n  labs(x = \"Infant mortality per 1000 recorded births\",\r\n       y = NULL)\r\n\r\n\r\n\r\n\r\nFigure 11: Infant mortality per 1000 children\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nFigure 12 shows the relationship between infant mortality rates (detailed above) and pandemic mortality rates, calculated by dividing the number of respiratory deaths from the Forms by the population of individuals by race in each district from the 1911 census (in the case of black and coloured individuals) and the 1918 census (in the case of white individuals, conducted just prior to the pandemic). The aggregation of black and coloured mortality rates is a function of the denominator (population) reported in the census in aggregate.\r\nFigure 12 evidences a strong linear correlation between infant mortality rates and pandemic mortality rates across districts for black and coloured individuals, and across groups. The slope of the linear fit is less among white residents of the 14 districts2.\r\n\r\n\r\nShow code\r\n\r\ninfant_mortality <- read_rds(\"data/infant_mortality.rds\")\r\n\r\n# jpeg(\r\n#     filename=\"images/infant-mort-scatter.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ninfant_mortality %>% \r\n  filter(births > 5) %>% \r\n  ggplot(aes(infant_mort_rate, mort_rate)) +\r\n  geom_point(aes(size = total, colour = race)) +\r\n  # geom_smooth(method = \"lm\", se = F, aes(colour = \"Both groups\")) +\r\n  geom_smooth(aes(colour = race), method = \"lm\", se = F) +\r\n  scale_x_continuous(labels = number_format(scale = 1000)) +\r\n  scale_y_continuous(labels = number_format(scale = 1000)) +\r\n  scale_color_manual(values = wesanderson::wes_palette(3, name = \"Zissou1\", type = \"continuous\")) +\r\n  labs(x = \"Infant mortality rate\",\r\n       y = \"Pandemic mortality rate\",\r\n       size = \"District population\",\r\n       colour = \"Race group\") +\r\n  theme(legend.position = \"bottom\") +\r\n  guides(colour = guide_legend(nrow = 3),\r\n         size = guide_legend(nrow = 3))\r\n\r\n\r\n\r\n\r\nFigure 12: Scatterplot and linear fit of infant and pandemic mortality by district\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\nlm_1 <- infant_mortality %>%\r\n  filter(race == \"Both groups\") %>% \r\n  lm(mort_rate ~ infant_mort_rate, data = .)\r\n\r\nlm_2 <- infant_mortality %>%\r\n  filter(race != \"Both groups\") %>% \r\n  lm(mort_rate ~ infant_mort_rate + race, data = .)\r\n\r\n# stargazer::stargazer(lm_1, lm_2,\r\n#   type = \"text\",\r\n#   title = \"Infant and pandemic mortality\",\r\n#   dep.var.caption = \" \",\r\n#   dep.var.labels = \"Pandemic mortality rate\",\r\n#   covariate.labels = c(\"Infant mortality rate\", \"White\"),\r\n#   out=\"regs/infant-mort-scatter.txt\"\r\n# )\r\n\r\n\r\n\r\n\r\nInfant and pandemic mortality\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPandemic mortality rate\r\n\r\n\r\n\r\n\r\n(1)\r\n\r\n\r\n(2)\r\n\r\n\r\n\r\n\r\nInfant mortality rate\r\n\r\n\r\n0.139**\r\n\r\n0.015\r\n\r\n\r\n\r\n\r\n(0.046)\r\n\r\n\r\n(0.012)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWhite\r\n\r\n\r\n\r\n\r\n-0.028***\r\n\r\n\r\n\r\n\r\n\r\n(0.006)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nConstant\r\n\r\n\r\n0.003\r\n\r\n\r\n0.038***\r\n\r\n\r\n\r\n(0.010)\r\n\r\n\r\n(0.005)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nObservations\r\n\r\n\r\n14\r\n\r\n\r\n28\r\n\r\n\r\nR2\r\n\r\n0.428\r\n\r\n\r\n0.521\r\n\r\n\r\nAdjusted R2\r\n\r\n0.380\r\n\r\n\r\n0.482\r\n\r\n\r\nResidual Std. Error\r\n\r\n\r\n0.013 (df = 12)\r\n\r\n\r\n0.016 (df = 25)\r\n\r\n\r\nF Statistic\r\n\r\n\r\n8.975** (df = 1; 12)\r\n\r\n\r\n13.574*** (df = 2; 25)\r\n\r\n\r\n\r\n\r\nNote:\r\n\r\n\r\n*p<0.1; **p<0.05; ***p<0.01\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nHeteroskedasticity robust SEs from Sandwich package\r\n\r\nRegression table 12 shows before controlling for race, there is a strong correlation between infant mortality rate and pandemic mortality rates across districts, with a five percent significance level and a slope coefficient of 0.139. However, after accounting for race, infant mortality rate is not longer significantly correlated with pandemic mortality.\r\nChild mortality\r\nIn a similar vein, I assess the correlation between child mortality and pandemic mortality. Child mortality is calculated as the number of children who died prior to the pandemic below five years of age from the Forms, divided by the population below five years of age from the 1911 and 1918 census for black and coloured and white individuals, respectively.\r\nFigure 13 shows that there is heterogeneity amongst the districts in child mortality, and that on average child mortality for white residents is lower than for black and coloured residents.\r\n\r\n\r\nShow code\r\n\r\nchild_mortality <- read_rds(\"data/child_mortality.rds\")\r\n\r\n# jpeg(\r\n#     filename=\"images/child-mortality.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\nchild_mortality %>% \r\n  filter(race != \"Both groups\") %>% \r\n  mutate(town = reorder_within(town, child_mort_rate, race)) %>%\r\n  ggplot(aes(child_mort_rate, town, colour = race)) +\r\n  geom_point() +\r\n  geom_errorbarh(aes(xmin = low, xmax = high)) +\r\n  scale_y_reordered() +\r\n  facet_wrap(~ race, nrow = 2, scales = \"free_y\") +\r\n  scale_x_continuous(labels = number_format(scale = 1000)) +\r\n  scale_color_manual(values = wesanderson::wes_palette(2, name = \"Zissou1\", type = \"continuous\")) +\r\n  theme(legend.position = \"none\") +\r\n  labs(x = \"Child mortality per 1000 children\",\r\n       y = NULL)\r\n\r\n\r\n\r\n\r\nFigure 13: Child mortality per 1000 children\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nTable 7\r\n\r\n\r\nShow code\r\n\r\nchild_mortality %>% \r\n  filter(race != \"Both groups\") %>% \r\n  group_by(town) %>% \r\n  mutate(n_under_5_t = sum(n_under_5),\r\n         deaths_t = sum(deaths),\r\n         child_mort_rate_t =deaths_t / n_under_5_t) %>% \r\n  ungroup() %>% \r\n  select(town, race, child_mort_rate, child_mort_rate_t) %>% \r\n  pivot_wider(names_from = \"race\", values_from = \"child_mort_rate\") %>% \r\n  mutate(across(where(is.numeric), .fns = ~ round(.x * 1000, digits = 0))) %>% \r\n  arrange(child_mort_rate_t) %>% \r\n  knitr::kable(col.names = c(\"District\", \"Aggregate\", \"White\", \"Black and Coloured\"),\r\n               caption = \"Child mortality per 1000 recorded births\")\r\n\r\n\r\nTable 7: Child mortality per 1000 recorded births\r\nDistrict\r\nAggregate\r\nWhite\r\nBlack and Coloured\r\nVryburg\r\n67\r\n79\r\n65\r\nPeddie\r\n73\r\n50\r\n74\r\nKuruman\r\n81\r\n61\r\n86\r\nElliot\r\n110\r\n47\r\n159\r\nMafeking\r\n118\r\n126\r\n117\r\nRiversdale\r\n127\r\n62\r\n215\r\nKomga\r\n160\r\n42\r\n174\r\nFraserburg\r\n175\r\n81\r\n291\r\nCradock\r\n180\r\n62\r\n232\r\nQueenstown\r\n216\r\n77\r\n247\r\nOudtshoorn\r\n243\r\n93\r\n391\r\nPaarl\r\n263\r\n64\r\n343\r\nStellenbosch\r\n270\r\n108\r\n340\r\nStockenstrom\r\n279\r\n90\r\n318\r\n\r\nFigure 14 shows the linear fits superimposed on the scatter plots of child mortality rates and pandemic mortality rates. Visually, all three have a positive slope, with the steepest gradient on the aggregate sample.\r\n\r\n\r\nShow code\r\n\r\n# jpeg(\r\n#     filename=\"images/child-mort-scatter.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\nchild_mortality %>% \r\n  ggplot(aes(child_mort_rate, mort_rate)) +\r\n  geom_point(aes(size = total, colour = race)) +\r\n  geom_smooth(aes(colour = race), method = \"lm\", se = F) +\r\n  scale_x_continuous(labels = number_format(scale = 1000)) +\r\n  scale_y_continuous(labels = number_format(scale = 1000)) +\r\n  scale_color_manual(values = wesanderson::wes_palette(3, name = \"Zissou1\", type = \"continuous\")) +\r\n  labs(x = \"Child mortality rate\",\r\n       y = \"Pandemic mortality rate\",\r\n       size = \"District population\",\r\n       colour = \"Race group\")\r\n\r\n\r\n\r\n\r\nFigure 14: Scatterplot and linear fit of child and pandemic mortality by district\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\nlm_1 <- child_mortality %>%\r\n  filter(race == \"Both groups\") %>% \r\n  lm(mort_rate ~ child_mort_rate, data = .)\r\n\r\nlm_2 <- child_mortality %>%\r\n  filter(race != \"Both groups\") %>% \r\n  lm(mort_rate ~ child_mort_rate + race, data = .)\r\n\r\n# stargazer::stargazer(lm_1, lm_2,\r\n#   type = \"html\", \r\n#   title = \"Child and pandemic mortality\", \r\n#   dep.var.caption = \" \",\r\n#   dep.var.labels = \"Pandemic mortality rate\",\r\n#   covariate.labels = c(\"Child mortality rate\", \"White\")\r\n# )\r\n\r\n\r\n\r\n\r\nChild mortality and pandemic mortality\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPandemic mortality rate\r\n\r\n\r\n\r\n\r\n(1)\r\n\r\n\r\n(2)\r\n\r\n\r\n\r\n\r\nChild mortality rate\r\n\r\n\r\n0.095\r\n\r\n\r\n0.066*\r\n\r\n\r\n\r\n(0.058)\r\n\r\n\r\n(0.038)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWhite\r\n\r\n\r\n\r\n\r\n-0.020**\r\n\r\n\r\n\r\n\r\n\r\n(0.008)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nConstant\r\n\r\n\r\n0.017\r\n\r\n\r\n0.028***\r\n\r\n\r\n\r\n(0.011)\r\n\r\n\r\n(0.009)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nObservations\r\n\r\n\r\n14\r\n\r\n\r\n28\r\n\r\n\r\nR2\r\n\r\n0.184\r\n\r\n\r\n0.544\r\n\r\n\r\nAdjusted R2\r\n\r\n0.116\r\n\r\n\r\n0.508\r\n\r\n\r\nResidual Std. Error\r\n\r\n\r\n0.016 (df = 12)\r\n\r\n\r\n0.015 (df = 25)\r\n\r\n\r\nF Statistic\r\n\r\n\r\n2.707 (df = 1; 12)\r\n\r\n\r\n14.919*** (df = 2; 25)\r\n\r\n\r\n\r\n\r\nNote:\r\n\r\n\r\n*p<0.1; **p<0.05; ***p<0.01\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nHeteroskedasticity robust SEs from Sandwich package\r\n\r\nRegression table 14 shows that after accounting for race, child mortality is correlated with pandemic mortality at the ten percent level, with a slope coefficient of 0.066.\r\nPre-pandemic pneumonia and influenza mortality\r\nAcuña-Soto, Viboud, and Chowell (2011) show how influenza and pneumonia mortality prior to the pandemic are predictive of mortality rates during the pandemic in 66 large cities in the United States.\r\nI follow their methodology here to assess the same question. First I calculate the pre-pandemic mortality rates by district from influenza, pneumonia and both. These are shown in Table 8 below.\r\n\r\n\r\nShow code\r\n\r\npneumonia_influenza <- read_rds(\"data/pneumonia_influenza.rds\")\r\n  \r\npneumonia_influenza %>% \r\n  group_by(town) %>% \r\n  mutate(influenza_deaths_grouped = sum(influenza_deaths, na.rm = T),\r\n         pneumonia_deaths_grouped = sum(pneumonia_deaths, na.rm = T),\r\n         pneumonia_influenza_deahts_grouped = sum(pneumonia_influenza_deaths, na.rm = T),\r\n         total_grouped = sum(total, na.rm = T)) %>% \r\n  mutate(across(c(influenza_deaths_grouped, pneumonia_deaths_grouped, pneumonia_influenza_deahts_grouped), .fns = ~.x*3 / total_grouped)) %>% \r\n  ungroup() %>% \r\n  mutate(across(where(is.numeric), ~ round(.x*1000, 2))) %>% \r\n  distinct(town, .keep_all = T) %>% \r\n  arrange(pneumonia_influenza_deahts_grouped) %>% \r\n  select(town, influenza_deaths_grouped, pneumonia_deaths_grouped, pneumonia_influenza_deahts_grouped) %>% \r\n  knitr::kable(col.names = c(\"District\", \"Influenza\", \"Pneumonia\", \"Influenza and Pneumonia\"),\r\n               caption = \"Influenza and Pneumonia mortality per 1000 residents\")\r\n\r\n\r\nTable 8: Influenza and Pneumonia mortality per 1000 residents\r\nDistrict\r\nInfluenza\r\nPneumonia\r\nInfluenza and Pneumonia\r\nKuruman\r\n4.28\r\n4.28\r\n8.56\r\nRiversdale\r\n3.18\r\n5.76\r\n8.94\r\nVryburg\r\n4.30\r\n9.02\r\n13.32\r\nPeddie\r\n8.37\r\n5.89\r\n14.10\r\nMafeking\r\n0.49\r\n14.77\r\n15.26\r\nElliot\r\n7.28\r\n11.79\r\n18.72\r\nFraserburg\r\n5.21\r\n20.85\r\n26.06\r\nCradock\r\n2.31\r\n23.84\r\n26.15\r\nPaarl\r\n0.79\r\n27.82\r\n28.61\r\nStellenbosch\r\n3.29\r\n32.05\r\n35.07\r\nQueenstown\r\n8.02\r\n29.17\r\n37.02\r\nOudtshoorn\r\n1.95\r\n42.89\r\n44.59\r\nKomga\r\n28.62\r\n16.88\r\n45.31\r\nStockenstrom\r\n22.65\r\n75.60\r\n98.25\r\n\r\nFigure 15 shows that while the slopes separated by race are lower, there is still a strong correlation between pre-existing pneumonia and influenza mortality rates and the pandemic mortality rate in each district. The strongest correlation appears to be the aggregated sample.\r\n\r\n\r\nShow code\r\n\r\npneumonia_influenza_mort <- read_rds(\"data/pneumonia_infleunza_mort.rds\")\r\n\r\n# jpeg(\r\n#     filename=\"images/pneumonia-influenza-mort.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\npneumonia_influenza_mort %>%\r\n  ggplot(aes(pneumonia_influenza_mort, pandemic_mort_rate)) +\r\n  geom_point(aes(colour = race, size = total_pop)) +\r\n  geom_smooth(method = \"lm\", aes(colour = race), se = F) +\r\n  scale_x_continuous(labels = number_format(scale = 1000)) +\r\n  scale_y_continuous(labels = number_format(scale = 1000)) +\r\n  # theme(legend.position = \"bottom\") +\r\n  scale_color_manual(values = wesanderson::wes_palette(3, name = \"Zissou1\", type = \"continuous\")) +\r\n  labs(\r\n    x = \"Pre-existing district level pneumonia and infleunza mortality\",\r\n    y = \"Pandemic mortality\",\r\n    caption = \"Note: mortality is measured as deaths per 1000 residents\",\r\n    colour = \"Race group\",\r\n    size = \"District population\"\r\n  )\r\n\r\n\r\n\r\n\r\nFigure 15: Scatterplot and linear fit of pneumonia mortality and pandemic mortality by district\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\nlm_1 <- read_rds(\"data/influenza_mort.rds\") %>% \r\n  lm(pandemic_mort_rate ~ influenza_mort + race, data = .)\r\n\r\nlm_2 <- read_rds(\"data/pneumonia_mort.rds\") %>% \r\n  lm(pandemic_mort_rate ~ pneumonia_mort + race, data = .)\r\n\r\nlm_3 <- pneumonia_influenza_mort %>% \r\n lm(pandemic_mort_rate ~ pneumonia_influenza_mort + race, data = .)\r\n\r\n# stargazer::stargazer(lm_1, lm_2, lm_3,\r\n#   type = \"html\", \r\n#   title = \"Infant and pandemic mortality\", \r\n#   dep.var.caption = \" \",\r\n#   dep.var.labels = \"Pandemic mortality rate\",\r\n#   covariate.labels = c(\"Influenza mortality\", \"Pneumonia mortality\", \"Pneumonia and influenza mort.\", \"White\")\r\n# )\r\n\r\n\r\n\r\n\r\nPneumonia and influenza and pandemic mortality\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPandemic mortality rate\r\n\r\n\r\n\r\n\r\n(1)\r\n\r\n\r\n(2)\r\n\r\n\r\n(3)\r\n\r\n\r\n\r\n\r\nInfluenza mortality\r\n\r\n\r\n3.228**\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n(1.437)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPneumonia mortality\r\n\r\n\r\n\r\n\r\n1.370**\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n(0.511)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPneumonia and influenza mort.\r\n\r\n\r\n\r\n\r\n\r\n\r\n1.596***\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n(0.336)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWhite\r\n\r\n\r\n-0.023***\r\n\r\n-0.019***\r\n\r\n-0.014**\r\n\r\n\r\n\r\n(0.007)\r\n\r\n\r\n(0.007)\r\n\r\n\r\n(0.006)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nConstant\r\n\r\n\r\n0.033***\r\n\r\n0.029***\r\n\r\n0.022***\r\n\r\n\r\n\r\n(0.006)\r\n\r\n\r\n(0.006)\r\n\r\n\r\n(0.006)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nObservations\r\n\r\n\r\n26\r\n\r\n\r\n28\r\n\r\n\r\n42\r\n\r\n\r\nR2\r\n\r\n0.567\r\n\r\n\r\n0.602\r\n\r\n\r\n0.617\r\n\r\n\r\nAdjusted R2\r\n\r\n0.530\r\n\r\n\r\n0.571\r\n\r\n\r\n0.587\r\n\r\n\r\nResidual Std. Error\r\n\r\n\r\n0.015 (df = 23)\r\n\r\n\r\n0.014 (df = 25)\r\n\r\n\r\n0.013 (df = 38)\r\n\r\n\r\nF Statistic\r\n\r\n\r\n15.075*** (df = 2; 23)\r\n\r\n\r\n18.945*** (df = 2; 25)\r\n\r\n\r\n20.387*** (df = 3; 38)\r\n\r\n\r\n\r\n\r\nNote:\r\n\r\n\r\n*p<0.1; **p<0.05; ***p<0.01\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nHeteroskedasticity robust SEs from Sandwich package\r\n\r\nRegression table 15 shows that after controlling for race, mortality rates from influenza, pneumonia and both diseases are highly correlated with mortality rates during the pandemic, at the 5 percent level and below. The combination are significantly correlated with pandemic mortality rates at the one percent level.\r\nLiteracy proxy\r\nI use a proxy for literacy of the individual’s family to assess if literacy has an impact on how well residents of a district fare during the pandemic. The hypothesis is that districts with higher literacy rates will better adopt the best practices proposed by health authorities in an attempt to quell the pandemic and stop the spread of the disease, according to Clay, Lewis, and Severnini (2018).\r\nTable 9 shows that there is considerable variation in our literacy proxy by district and race. Figure 16 shows that the share of informants who sign the Forms in Sea Point, Fraserburg and Elliot is 100 percent. Given that it is likely the registrar signed each form on behalf of the informant, I will exclude these three from the regression.\r\n\r\n\r\nShow code\r\n\r\ndf %>%\r\n  filter(death_date < ymd(\"1918-09-01\")) %>% \r\n  mutate(signed_yes_or_no = str_to_lower(signed_yes_or_no)) %>%\r\n  mutate(literate_proxy = case_when(\r\n    str_detect(signed_yes_or_no, \"maark|mark|merk|no|x\") ~ \"No\",\r\n    str_detect(signed_yes_or_no, \"yes\") ~ \"Yes\",\r\n    TRUE ~ \"No\"\r\n  )) %>% \r\n  filter(!race %in% c(NA, \"NA\")) %>% \r\n  count(race, town, literate_proxy) %>% \r\n  pivot_wider(names_from = \"literate_proxy\", values_from = \"n\") %>% \r\n  mutate(across(c(No, Yes), ~ replace_na(., 0))) %>% \r\n  mutate(share_literate = Yes/(Yes + No),\r\n         town = fct_reorder(town, share_literate)) %>% \r\n  group_by(town) %>% \r\n  mutate(aggregate = sum(Yes)/(sum(Yes) + sum(No))) %>% \r\n  ungroup() %>% \r\n  select(town, race, share_literate, aggregate) %>% \r\n  pivot_wider(names_from = \"race\", values_from = \"share_literate\") %>% \r\n  arrange(desc(aggregate)) %>% \r\n  mutate(across(where(is.numeric), .fns = ~ percent(., accuracy = 1))) %>% \r\n    knitr::kable(col.names = c(\"District\", \"Aggregate\", \"Black\", \"Coloured\", \"White\"),\r\n               caption = \"Percentage of district death notices wherein Informant signed\")\r\n\r\n\r\nTable 9: Percentage of district death notices wherein Informant signed\r\nDistrict\r\nAggregate\r\nBlack\r\nColoured\r\nWhite\r\nFraserburg\r\n100%\r\n100%\r\n100%\r\n100%\r\nSea Point\r\n100%\r\n100%\r\n100%\r\n100%\r\nElliot\r\n99%\r\n99%\r\n100%\r\n99%\r\nStellenbosch\r\n49%\r\n49%\r\n37%\r\n96%\r\nPaarl\r\n45%\r\n46%\r\n37%\r\n94%\r\nCradock\r\n45%\r\n33%\r\n36%\r\n97%\r\nVryburg\r\n45%\r\n29%\r\n30%\r\n99%\r\nRiversdale\r\n43%\r\n50%\r\n15%\r\n90%\r\nMafeking\r\n42%\r\n32%\r\n51%\r\n100%\r\nQueenstown\r\n40%\r\n33%\r\n30%\r\n98%\r\nOudtshoorn\r\n37%\r\n27%\r\n21%\r\n90%\r\nKuruman\r\n33%\r\n16%\r\n36%\r\n96%\r\nStockenstrom\r\n17%\r\n7%\r\n32%\r\n93%\r\nKomga\r\n11%\r\n6%\r\n14%\r\n95%\r\nPeddie\r\n9%\r\n6%\r\n20%\r\n97%\r\n\r\n\r\n\r\nShow code\r\n\r\n# jpeg(\r\n#     filename=\"images/literacy-fig.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf %>%\r\n  filter(death_date < ymd(\"1918-09-01\")) %>% \r\n  mutate(signed_yes_or_no = str_to_lower(signed_yes_or_no)) %>%\r\n  mutate(literate_proxy = case_when(\r\n    str_detect(signed_yes_or_no, \"maark|mark|merk|no|x\") ~ \"No\",\r\n    str_detect(signed_yes_or_no, \"yes\") ~ \"Yes\",\r\n    TRUE ~ \"No\"\r\n  )) %>% \r\n  filter(!race %in% c(NA, \"NA\")) %>% \r\n  count(race, town, literate_proxy) %>% \r\n  pivot_wider(names_from = \"literate_proxy\", values_from = \"n\") %>% \r\n  mutate(across(c(No, Yes), ~ replace_na(., 0))) %>% \r\n  mutate(share_literate = Yes/(Yes + No),\r\n         town = fct_reorder(town, share_literate)) %>%\r\n  ggplot(aes(share_literate, town, fill = race)) +\r\n  scale_fill_manual(values = wesanderson::wes_palette(3, name = \"Darjeeling1\")) +\r\n  geom_col(position = \"dodge\", show.legend = F) +\r\n  facet_wrap( ~ race) +\r\n  scale_x_continuous(labels = percent_format()) +\r\n  labs(x = \"Share of informants who sign Form\",\r\n       y = NULL)\r\n\r\n\r\n\r\n\r\nFigure 16: Share of informants who sign\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nIn Figure 17, the best fit line for “Both groups,” shown in yellow, leads us to believe that as the proportion of our district with the literacy proxy increases, the pandemic mortality decreases, in line with our hypothesis. Further, amongst black and coloured individuals, an increasing share of literate informants appears to result in marginally lower mortality rates during the pandemic. This is in support of the hypothesis. Yet amongst the white population of the districts, there appears to be an upward slope at the top end of the literacy distribution.\r\n\r\n\r\nShow code\r\n\r\ndf_literacy <- read_rds(\"data/df_literacy.rds\")\r\n\r\n# jpeg(\r\n#     filename=\"images/literacy-scatter.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf_literacy %>% \r\n  filter(!town %in% c(\"Elliot\", \"Fraserburg\", \"Sea Point\")) %>% \r\n  ggplot(aes(share_literate, mort_rate)) +\r\n  geom_point(aes(colour = race, size = total)) +\r\n  geom_smooth(method = \"lm\", aes(colour = race), se = F) +\r\n  scale_x_continuous(labels = percent_format()) +\r\n  scale_y_continuous(labels = number_format(scale = 1000)) +\r\n  # need to choose a better colour\r\n  scale_color_manual(values = wesanderson::wes_palette(3, name = \"Zissou1\", type = \"continuous\")) +\r\n  labs(x = \"Percentage of population with literacy proxy\",\r\n       y = \"Pandemic mortality per 1000 residents\",\r\n       size = \"District population\",\r\n       colour = NULL)\r\n\r\n\r\n\r\n\r\nFigure 17: Scatter plot and linear fit of literacy proxy and pandemic mortality by district\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\nlm_1 <- df_literacy %>%\r\n  filter(!town %in% c(\"Elliot\", \"Fraserburg\", \"Sea Point\")) %>%\r\n  filter(race == \"Both groups\") %>%\r\n  lm(mort_rate ~ share_literate, data = .)\r\n\r\nlm_2 <- df_literacy %>%\r\n  filter(!town %in% c(\"Elliot\", \"Fraserburg\", \"Sea Point\")) %>%\r\n  filter(race != \"Both groups\") %>%\r\n  lm(mort_rate ~ share_literate + race, data = .)\r\n\r\n# stargazer::stargazer(lm_1, lm_2,\r\n#   type = \"html\", \r\n#   title = \"Infant and pandemic mortality\", \r\n#   dep.var.caption = \" \",\r\n#   dep.var.labels = \"Pandemic mortality rate\",\r\n#   covariate.labels = c(\"Share of informants who sign\", \"White\")\r\n# )\r\n\r\n\r\n\r\n\r\nLiteracy of the informant and pandemic mortality\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPandemic mortality rate\r\n\r\n\r\n\r\n\r\n(1)\r\n\r\n\r\n(2)\r\n\r\n\r\n\r\n\r\nShare of informants who sign\r\n\r\n\r\n-0.063*\r\n\r\n-0.009\r\n\r\n\r\n\r\n\r\n(0.031)\r\n\r\n\r\n(0.036)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWhite\r\n\r\n\r\n\r\n\r\n-0.021\r\n\r\n\r\n\r\n\r\n\r\n\r\n(0.026)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nConstant\r\n\r\n\r\n0.054***\r\n\r\n0.042***\r\n\r\n\r\n\r\n(0.012)\r\n\r\n\r\n(0.009)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nObservations\r\n\r\n\r\n12\r\n\r\n\r\n24\r\n\r\n\r\nR2\r\n\r\n0.284\r\n\r\n\r\n0.512\r\n\r\n\r\nAdjusted R2\r\n\r\n0.212\r\n\r\n\r\n0.465\r\n\r\n\r\nResidual Std. Error\r\n\r\n\r\n0.015 (df = 10)\r\n\r\n\r\n0.014 (df = 21)\r\n\r\n\r\nF Statistic\r\n\r\n\r\n3.958* (df = 1; 10)\r\n\r\n\r\n11.005*** (df = 2; 21)\r\n\r\n\r\n\r\n\r\nNote:\r\n\r\n\r\n*p<0.1; **p<0.05; ***p<0.01\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nHeteroskedasticity robust SEs from Sandwich package\r\n\r\nRegression table 17 shows that when we control for race, we see that the effect of literacy is not significant and magnitude of the effect fall. We cannot reject the hypothesis that our literacy proxy has no impact on mortality rate at the district level after controlling for race, even at the ten percent level.\r\nPercentage of foreign-born individuals\r\nAnother measure of social capital is foreign born status, with the hypothesis being that foreign born individuals are less likely to adopt the health measures suggested by local governments. Figure 18 shows the proportion of residents of each district born in various regions outside of the Cape (shown in red). This data is from the Statistics of Population of 1918 (Office of Census and Statistics Pretoria 1918). This means that we would expect the mortality rate during the flu to increase with the share of residents born outside the Cape.\r\n\r\n\r\nShow code\r\n\r\nbirth_place_shares <- read_rds(\"data/birth_place_shares.rds\")\r\n\r\n# jpeg(\r\n#     filename=\"images/birth-places.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\nbirth_place_shares %>% \r\n  mutate(town = fct_reorder(town, share_not_cape)) %>% \r\n  # select(-contains(\"share\"), -grand_total) %>% \r\n  rename_with( ~ str_remove(.x, \"total_\"), contains(\"total_\")) %>%\r\n  pivot_longer(c(cape, sa, africa, europe, other), values_to = \"n\", names_to = \"region\") %>% \r\n  mutate(region = case_when(\r\n    str_detect(region, \"cape\") ~ \"Cape Province\",\r\n    str_detect(region, \"sa\") ~ \"Rest of South Africa\",\r\n    str_detect(region, \"africa\") ~ \"Rest of Africa\",\r\n    TRUE ~ region\r\n  )) %>% \r\n  mutate(region = str_to_title(region)) %>% \r\n  ggplot(aes(n, town, fill = region)) +\r\n  geom_col(position = \"fill\") +\r\n  scale_x_continuous(labels = percent) +\r\n  scale_fill_manual(values = wesanderson::wes_palette(\"Darjeeling1\")) +\r\n  labs(fill = \"Proportion of residents born in\",\r\n       x = NULL,\r\n       y = NULL)\r\n\r\n\r\n\r\n\r\nFigure 18: Birth places of district residents\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nFigure 19 shows that across our districts, there is a slight negative correlation between the share of residents born outside of the Cape. However, the confidence interval around this linear fit is wide. Regression table 19 confirms that there is no significant correlation between the share of residents born outside of the Cape and pandemic mortality rates, even at the ten percent level.\r\n\r\n\r\nShow code\r\n\r\n# jpeg(\r\n#     filename=\"images/birth-place-corr.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\nbirth_place_shares %>% \r\n  group_by(town) %>% \r\n  mutate(pandemic_mort_grouped = sum(deaths) / sum(total_pop)) %>% \r\n  ungroup() %>% \r\n  distinct(town, pandemic_mort_grouped, share_not_cape, grand_total) %>% \r\n  ggplot(aes(share_not_cape, pandemic_mort_grouped)) +\r\n  geom_point(aes(size = grand_total), colour = \"midnightblue\") +\r\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\r\n  scale_y_continuous(labels = number_format(scale = 1000)) +\r\n  geom_smooth(method = \"lm\") +\r\n  labs(x = \"Share of residents born outside the Cape Province\",\r\n       y = \"Pandemic mortality per 1000 residents\",\r\n       size = \"District population\")\r\n\r\n\r\n\r\n\r\nFigure 19: Scatter plot and linear fit of residents born outside of the Cape Province and pandemic mortality by district\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\nlm_1 <- birth_place_shares %>% \r\n  lm(pandemic_mort_rate ~ share_not_cape, data = .)\r\n\r\n# stargazer::stargazer(lm_1,\r\n#   type = \"html\", \r\n#   title = \"Infant and pandemic mortality\", \r\n#   dep.var.caption = \" \",\r\n#   dep.var.labels = \"Pandemic mortality rate\",\r\n#   covariate.labels = c(\"Share of residents born outside of the Cape\")\r\n# )\r\n\r\n\r\n\r\n\r\nForeign born residents and pandemic mortality\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPandemic mortality rate\r\n\r\n\r\n\r\n\r\nShare of residents born outside of the Cape\r\n\r\n\r\n-0.032\r\n\r\n\r\n\r\n\r\n(0.034)\r\n\r\n\r\n\r\n\r\n\r\n\r\nConstant\r\n\r\n\r\n0.032***\r\n\r\n\r\n\r\n(0.006)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nObservations\r\n\r\n\r\n28\r\n\r\n\r\nR2\r\n\r\n0.032\r\n\r\n\r\nAdjusted R2\r\n\r\n-0.005\r\n\r\n\r\nResidual Std. Error\r\n\r\n\r\n0.022 (df = 26)\r\n\r\n\r\nF Statistic\r\n\r\n\r\n0.869 (df = 1; 26)\r\n\r\n\r\n\r\n\r\nNote:\r\n\r\n\r\n*p<0.1; **p<0.05; ***p<0.01\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nHeteroskedasticity robust SEs from Sandwich package\r\n\r\nConclusion\r\nAfter analysing a dataset of 40,000 deaths in the Cape, and supplementing this information with birth and census data from prior to the pandemic, I find after assessing excess mortality that it is unlikely there was significant under counting of the number of deaths during the Spanish flu pandemic due to misclassification of cause of death. Regarding the features correlated with pandemic mortality, I find that pre-existing levels of health in a district indicated by child mortality are strongly correlated with mortality rates during the pandemic. I find that district with high levels of mortality from pneumonia and influenza prior to the pandemic suffered severely from mortality during the pandemic. Finally, I find no support for the notion that socio-economic factors proxied by the literacy of a family member or foreign born status are correlated with pandemic mortality rates.\r\nAppendix\r\nLinear Support Vector Machine specification and workflow\r\nI use a linear support vector machine to classify ethnicity to race for black and coloured individuals based on a hand coded training set of 15 000 observations. I make use of sub-word features in my pre-processing tokenization. I use 10-fold cross validation in order to prevent data leakage and ensure that the classification results are robust.\r\n\r\n\r\ndf_classification <- read_rds(\"data/race_classification_15000.rds\") \r\n\r\ndf_classification %>% sample_n(6)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  race     ethnicity_as_recorded\r\n  <chr>    <chr>                \r\n1 Coloured Coloured             \r\n2 Black    Fingo                \r\n3 Black    Kaffir               \r\n4 Coloured Gemengd              \r\n5 Black    Kaffir               \r\n6 Black    Kaffir               \r\n\r\nAt outset I use my data budget by separating a training and assessment set from my labelled training set of 33,500 observations. This ensures that I can assess the performance of the model on data that it has not been trained on. The initial split uses 75 percent of the labelled data as training data and reserved 25 percent for testing. This means that there are 3 750 observations that are excluded from the training set, and put aside for assessing model accuracy, sensitivity and specificity after tuning of the hyper parameter. I also stratify the training and testing data based on race in order to ensure that the classes are as balanced as possible.\r\n\r\n\r\nset.seed(123)\r\n\r\ninitial_split <- df_classification %>% initial_split(prop = .75, strata = race)\r\ninitial_split\r\n\r\n\r\n<Analysis/Assess/Total>\r\n<11250/3750/15000>\r\n\r\n# creating training and testing data from initial split\r\ndf_train <- training(initial_split)\r\ndf_test <- testing(initial_split)\r\n\r\n\r\n\r\nIn an attempt to avoid overfitting on the training data in tuning of the hyper parameter, I create 10 folds of cross validation data. In essence I create 10 datasets that sample from the training data to tune my model parameter before testing the model performance on the testing data. Again I stratify by race in my folds.\r\n\r\n\r\n# creating 10-fold cross validation sets\r\ndf_folds <- vfold_cv(df_train, \r\n                     v = 10, \r\n                     strata = race)\r\n\r\n\r\n\r\nPreprocessing and feature engineering\r\nNext I create a recipe for pre-processing and tokenizing the text prior to modelling.\r\nI use the textrecipes package to tokenize the text into subword features. Using the tokenizers.bpe (where bpe stands for Byte-Pair Encoding) argument means that instead of breaking down our words into characters, the tokenizer algorithm will iteratively merge together frequently occurring subword pairs to find sub-words that are important in my dataset (Hvitfeldt and Silge 2021, 223). It is helpful in this case as we get a sweet spot between character level and word level information. Sub-word information like this is important in NLP models. Further, sub-word models can handle new/unknown words at prediction time, which bag of words models cannot.\r\nI use the themis package to deal with class imbalance. The step_smote argument generates new examples of the minority class using the nearest neighbours of those cases. In this case coloured is the minority class and as such the smote algorithm will create new examples of coloured observations with existing data from coloured observations.\r\nI set the maximum number of tokens to be tuned later with the tune() argument.\r\n\r\n\r\nlibrary(textrecipes)\r\nlibrary(themis)\r\n\r\nclassifcation_rec <- recipe(race ~ ethnicity_as_recorded, data = df_train) %>% \r\n    step_tokenize(ethnicity_as_recorded, \r\n                  engine = \"tokenizers.bpe\",\r\n                  training_options = list(vocab_size = 200)) %>% \r\n    step_tokenfilter(ethnicity_as_recorded, max_tokens = tune()) %>% \r\n    step_tf(ethnicity_as_recorded) %>% \r\n    step_normalize(all_predictors()) %>% \r\n    step_smote(race)\r\n\r\n\r\n\r\nModel specification\r\nI use a linear support vector machine, set the mode to classification and the engine that does the computation is LiblineaR, a wrapper for the C++ library LIBLINEAR. The LIBLINEAR library was the winner of ICML 2008 large-scale learning challenge, and can be used for binary as well as multi-class classification (Fan et al. 2008, 35).\r\n\r\n\r\nsvm_spec <- svm_linear() %>% \r\n    set_mode(\"classification\") %>% \r\n    set_engine(\"LiblineaR\")\r\n\r\nsvm_spec\r\n\r\n\r\nLinear Support Vector Machine Specification (classification)\r\n\r\nComputational engine: LiblineaR \r\n\r\nModel workflow\r\nHere the workflow keeps together the pre-processor or recipe and the model specification. Later the workflow can be finalized with the hyper paramter that performs the best on a specified metric.\r\n\r\n\r\nwf <- workflow() %>% \r\n    add_recipe(classifcation_rec) %>% \r\n    add_model(svm_spec)\r\n\r\n\r\n\r\nTuning grid\r\nThe tuning grid specifies the values across which the model will search for the best performing number of tokens or predictors. The choices are specified manually in this case given the simple search space and single tuning parameter.\r\n\r\n\r\ngrid <- crossing(max_tokens = c(50, 100, 150, 200))\r\n\r\n\r\n\r\nTuning parameters\r\nBy tuning the number of tokens using the 10 folds of cross validation data I can select the optimal parameters for the best performance of the model across the 10 folds of my training data.\r\n\r\n\r\nresults <- tune_grid(\r\n    wf, \r\n    df_folds,\r\n    grid = grid,\r\n    metrics = metric_set(accuracy, sens, spec)\r\n)\r\n\r\n\r\n\r\nVisualizing the results\r\nIn Figure 20 I draw a plot of model performance across the three metrics we specified above, accuracy, sensitivity and specificity. Balancing type 1 and type 2 errors here is not too important given that both sensitivity and specificity are so high. We can see that moving from 100 to 150 retained tokens, or predictors in our model, improves accuracy, sensitivity and specificity. Moving to 200 tokens worsens specificity marginally.\r\n\r\n\r\n# jpeg(\r\n#     filename=\"images/tuning-fig.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\nresults %>% autoplot()\r\n\r\n\r\n\r\n\r\nFigure 20: Model metrics for classification model and hyperparamter values\r\n\r\n\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nFinalize workflow\r\nHere I select the best model based on specificity across the 10 folds of cross validation data.\r\n\r\n\r\nwf_final <- wf %>% \r\n  finalize_workflow(select_best(results, \"spec\"))\r\n\r\n\r\n\r\nFinal fit on the holdout data\r\nFinally, with the hyper parameter value chosen, I fit the model a last time, training on the training data and tested on the testing data created with the initial split.\r\n\r\n\r\nfinal_fit <- last_fit(wf_final, initial_split,\r\n         metrics = metric_set(accuracy, sens, spec))\r\n\r\n\r\n\r\nThe performance of the model on the testing data is almost identical to the cross validation folds. A success.\r\n\r\n\r\ncollect_metrics(final_fit)\r\n\r\n\r\n# A tibble: 3 x 4\r\n  .metric  .estimator .estimate .config             \r\n  <chr>    <chr>          <dbl> <chr>               \r\n1 accuracy binary         0.998 Preprocessor1_Model1\r\n2 sens     binary         0.998 Preprocessor1_Model1\r\n3 spec     binary         0.999 Preprocessor1_Model1\r\n\r\nConfusion matrix\r\nThe confusion matrix shown in Figre 21 displays the number of predictions that the classifier made correctly and incorrectly. It is evident that it made 6 mistakes in total, out of the 3750 observations kept in the testing set. Two were predicted black when in truth they were coloured, and four predicted coloured when they were in fact black.\r\n\r\n\r\n# jpeg(\r\n#     filename=\"images/conf-mat-fig.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ncollect_predictions(final_fit) %>%\r\n  conf_mat(race, .pred_class) %>%\r\n  autoplot(type = \"heatmap\") +\r\n  scale_fill_viridis_c()\r\n\r\n\r\n\r\n\r\nFigure 21: Confusion matrix for classification model\r\n\r\n\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nModel performance\r\nThe model achieves 99.8 percent accuracy on previously unseen testing data, evidencing that it performs well on the sample of 15 000 hand labelled examples. It was then used on the remaining 25 000 individuals in order to collapse ethnicity to race in the full dataset. These predictions were checked by hand in order to correct any errors.\r\nInterpreting the model\r\n\r\n\r\nfit <- pull_workflow_fit(final_fit$.workflow[[1]])\r\n\r\nliblinear_obj <- fit$fit$W\r\n\r\nliblinear_df <- tibble(term = colnames(liblinear_obj),\r\n                       estimate = liblinear_obj[1,])\r\n\r\nwords <- liblinear_df %>% \r\n    filter(term != \"Bias\") %>% \r\n    mutate(term = str_replace_all(term, \"Kaffir\", \"K*****\"),\r\n           term = str_replace_all(term, \"Hottentot\", \"H********\")) %>% \r\n    group_by(estimate > 0) %>% \r\n    slice_max(abs(estimate), n = 20) %>% \r\n    ungroup() %>% \r\n    mutate(term = str_remove(term, \"tf_ethnicity_as_recorded_\"))\r\n\r\n\r\n\r\n\r\n\r\n# jpeg(\r\n#     filename=\"images/words-fig.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\nwords %>% \r\n    mutate(term = str_extract(term, \"\\\\w.*\"),\r\n           term = str_to_lower(term)) %>% \r\n    filter(!term %in% c(\"h\", \"b\", \"g\"),\r\n           !is.na(term)) %>% \r\n    ggplot(aes(estimate, fct_reorder(term, estimate), fill = estimate > 0)) +\r\n    geom_col(show.legend = F) +\r\n    labs(y = \"Words and sub-words\",\r\n         x = \"Imprtance of feature\")\r\n\r\n\r\n\r\n\r\nFigure 22: Determinants of classification model\r\n\r\n\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nFigure 22 allows us to see that if the ethnicity contains the sub-word features “KA,” “Z,”AT“,”OL“, and”AN“, the linear SVM is more likely to classify the individual as Black. In contrast, sub-word features”AR“, and”AM“,”M\" and “L” are associated with coloured individuals.\r\nClassification output\r\nFigure 23 shows the 12 most common classes in each race grouping.\r\n\r\n\r\nShow code\r\n\r\n# jpeg(\r\n#     filename=\"images/ethnicity-race-classification.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf %>% \r\n  filter(race != \"NA\",\r\n         race_original != \"Assyrian\") %>% \r\n  count(race, race_original, sort = T) %>% \r\n  group_by(race) %>% \r\n  slice_max(n, n = 12, with_ties = F) %>% \r\n  ungroup() %>% \r\n  mutate(race_original = str_replace_all(race_original, \"Kaffir\", \"K*****\"),\r\n         race_original = str_replace_all(race_original, \"Hottentot\", \"H********\")) %>% \r\n  mutate(race_original = reorder_within(race_original, n, race)) %>% \r\n  ggplot(aes(n, race_original, fill = race)) +\r\n  geom_col(show.legend = F) +\r\n  scale_y_reordered() +\r\n  facet_wrap(~ race, scales = \"free\") +\r\n  scale_fill_manual(values = wesanderson::wes_palette(3, name = \"Zissou1\", type = \"continuous\")) +\r\n  labs(x = NULL,\r\n       y = \"Recorded ethnicity\")\r\n\r\n\r\n\r\n\r\nFigure 23: Output of classification model\r\n\r\n\r\n\r\nShow code\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nRespiratory disease deaths\r\nI classify any death occurring during the pandemic period a respiratory death if the recorded cause of death includes any of the following phrases.\r\n\r\n\r\nrespiratory_terms <- c(\"respiratory\", \"Spanish influenza\",\"Pneumonia\",\"Bronchitis\",\"Infleuzna\",\"broncho pneumonia\",\"consumption\",\"whooping cough\",\"inflammation of lungs\",\"inflammation of the lungs\",\"pulmonary tuberculosis\",\"tuberculosis\",\"phthisis\",\"influenza pneumonia\",\"inflammation in lungs\",\"croup\",\"phthisis pulmonalis\",\"epidemic influenza\",\"inflammation in the lungs\",\"inflamation of lungs\",\"spanish influenza pneumonia\",\"flu\",\"spanish flu\",\"pneumonia following on an attack of influenza\",\"pulmonary pneumonia\",\"pneumonia influenza\",\"inflamation of the lungs\",\"pulmonary phthisis\",\"inflamation in the lungs\",\"double pneumonia\",\"lobar pneumonia\",\"tuberculosis Pulmonum\",\"spanish influenza\",\"general tuberculosis\", \"flu\", \"spanish\", \"epidemic\", \"lung\", \"Spaanse Griep\")\r\n\r\nrespiratory_terms\r\n\r\n\r\n [1] \"respiratory\"                                  \r\n [2] \"Spanish influenza\"                            \r\n [3] \"Pneumonia\"                                    \r\n [4] \"Bronchitis\"                                   \r\n [5] \"Infleuzna\"                                    \r\n [6] \"broncho pneumonia\"                            \r\n [7] \"consumption\"                                  \r\n [8] \"whooping cough\"                               \r\n [9] \"inflammation of lungs\"                        \r\n[10] \"inflammation of the lungs\"                    \r\n[11] \"pulmonary tuberculosis\"                       \r\n[12] \"tuberculosis\"                                 \r\n[13] \"phthisis\"                                     \r\n[14] \"influenza pneumonia\"                          \r\n[15] \"inflammation in lungs\"                        \r\n[16] \"croup\"                                        \r\n[17] \"phthisis pulmonalis\"                          \r\n[18] \"epidemic influenza\"                           \r\n[19] \"inflammation in the lungs\"                    \r\n[20] \"inflamation of lungs\"                         \r\n[21] \"spanish influenza pneumonia\"                  \r\n[22] \"flu\"                                          \r\n[23] \"spanish flu\"                                  \r\n[24] \"pneumonia following on an attack of influenza\"\r\n[25] \"pulmonary pneumonia\"                          \r\n[26] \"pneumonia influenza\"                          \r\n[27] \"inflamation of the lungs\"                     \r\n[28] \"pulmonary phthisis\"                           \r\n[29] \"inflamation in the lungs\"                     \r\n[30] \"double pneumonia\"                             \r\n[31] \"lobar pneumonia\"                              \r\n[32] \"tuberculosis Pulmonum\"                        \r\n[33] \"spanish influenza\"                            \r\n[34] \"general tuberculosis\"                         \r\n[35] \"flu\"                                          \r\n[36] \"spanish\"                                      \r\n[37] \"epidemic\"                                     \r\n[38] \"lung\"                                         \r\n[39] \"Spaanse Griep\"                                \r\n\r\n\r\n\r\n\r\nAcuña-Soto, R., C. Viboud, and G. Chowell. 2011. “Influenza and Pneumonia Mortality in 66 Large Cities in the United States in Years Surrounding the 1918 Pandemic.” PLoS ONE 6.\r\n\r\n\r\nArthi, Vellore, and John Parman. 2021. “Disease, downturns, and wellbeing: Economic history and the long-run impacts of COVID-19.” https://doi.org/10.1016/j.eeh.2020.101381.\r\n\r\n\r\nBarry, John M., Cécile Viboud, and Lone Simonsen. 2008. “Cross-protection between successive waves of the 1918-1919 influenza pandemic: Epidemiological evidence from US Army camps and from Britain.” Journal of Infectious Diseases. https://doi.org/10.1086/592454.\r\n\r\n\r\nBogaert, Kandace. 2015. “Cross protection between the first and second waves of the 1918 influenza pandemic among soldiers of the Canadian Expeditionary Force (CEF) in Ontario.” Vaccine. https://doi.org/10.1016/j.vaccine.2015.10.120.\r\n\r\n\r\nBogaert, Kandace L. 2016. “Reply to Brundage and Shanks.” https://doi.org/10.1016/j.vaccine.2016.02.071.\r\n\r\n\r\nChowell, Gerardo, Luís M. A. Bettencourt, Niall Johnson, Wladimir J. Alonso, and Cécile Viboud. 2008. “The 1918-1919 influenza pandemic in England and Wales: Spatial patterns in transmissibility and mortality impact.” Proceedings of the Royal Society B: Biological Sciences. https://doi.org/10.1098/rspb.2007.1477.\r\n\r\n\r\nCivil Deaths 1895-1972 database with images National Archives Pretoria. 1918. “Death Certificate of Wills Bunu.” Pretoria.\r\n\r\n\r\nClay, Karen, Joshua Lewis, and Edson Severnini. 2018. “Pollution, infectious disease, and mortality: Evidence from the 1918 Spanish influenza pandemic.” Journal of Economic History. https://doi.org/10.1017/S002205071800058X.\r\n\r\n\r\nCowling, Benjamin J, Irene O L Wong, Lai-Ming Ho, Steven Riley, and Gabriel M Leung. 2006. “Methods for monitoring influenza surveillance data.” International Journal of Epidemiology 35 (5): 1314–21. https://doi.org/10.1093/ije/dyl162.\r\n\r\n\r\nDorrington, Rob, Debbie Bradshaw, Ria Laubscher, Pam Groenewald, and Tom Moultrie. 2021. “METHODOLOGICAL NOTE: PREDICTED NUMBERS OF DEATHS BY EPI-WEEK FOR SOUTH AFRICA IN 2020 AND 2021.” South African Medical Research Council. https://www.samrc.ac.za/sites/default/files/files/2021-01-24/Methodological Note on Predicted Weekly Deaths 20{\\_}Jan{\\_}2021.pdf.\r\n\r\n\r\nFan, Rong En, Kai Wei Chang, Cho Jui Hsieh, Xiang Rui Wang, and Chih Jen Lin. 2008. “LIBLINEAR: A library for large linear classification.” Journal of Machine Learning Research. https://doi.org/10.1145/1390681.1442794.\r\n\r\n\r\nFourie, Johan, and Jonathan Jayes. 2021. “Health Inequality and the 1918 Influenza in South Africa.” World Development 141: 105407. https://doi.org/https://doi.org/10.1016/j.worlddev.2021.105407.\r\n\r\n\r\nGrantz, Kyra H., Madhura S. Rane, Henrik Salje, Gregory E. Glass, Stephen E. Schachterle, and Derek A. T. Cummings. 2016. “Disparities in influenza mortality and transmission related to sociodemographic factors within Chicago in the pandemic of 1918.” Proceedings of the National Academy of Sciences of the United States of America. https://doi.org/10.1073/pnas.1612838113.\r\n\r\n\r\nHanlon, W. 2018. “London Fog: A Century of Pollution and Mortality, 1866-1965.” Urban Economics & Regional Studies eJournal.\r\n\r\n\r\nHvitfeldt, E., and J. Silge. 2021. Supervised Machine Learning for Text Analysis in r. Chapman & Hall/CRC Data Science Series. Taylor & Francis Limited. https://books.google.co.za/books?id=WQ1qzgEACAAJ.\r\n\r\n\r\nLin, Ming Jen, and Elaine M. Liu. 2014. “Does in utero exposure to illness matter? The 1918 influenza epidemic in taiwan as a natural experiment.” Journal of Health Economics. https://doi.org/10.1016/j.jhealeco.2014.05.004.\r\n\r\n\r\nMamelund, Svenn Erik. 2006. “A socially neutral disease? Individual social class, household wealth and mortality from Spanish influenza in two socially contrasting parishes in Kristiania 1918-19.” Social Science and Medicine. https://doi.org/10.1016/j.socscimed.2005.06.051.\r\n\r\n\r\nMtumane, Zilibele. 2017. “The Practice of Ubuntu with Regard to amaMfengu Among amaXhosa as Depicted in s. E. K. Mqhayi’s Ityala Lamawele.” International Journal of African Renaissance Studies - Multi-, Inter- and Transdisciplinarity 12 (2): 68–80. https://doi.org/10.1080/18186874.2017.1392146.\r\n\r\n\r\nOffice of Census and Statistics Pretoria. 1918. “Statistics of Population of the Union of South Africa 1918 and previous years including vital and health statistics for the year 1916 and previous years.”\r\n\r\n\r\nOgasawara, Kota. 2018. “The long-run effects of pandemic influenza on the development of children from elite backgrounds: Evidence from industrializing Japan.” Economics and Human Biology. https://doi.org/10.1016/j.ehb.2018.08.012.\r\n\r\n\r\nPhillips, Howard. 1984. “’BLACK OCTOBER’: THE IMPACT OF THE SPANISH INFLUENZA EPIDEMIC OF 1918 ON SOUTH AFRICA.” PhD thesis, University of Cape Town.\r\n\r\n\r\nShanks, G. Dennis, and John F. Brundage. 2016. “Response to Bogaert.” https://doi.org/10.1016/j.vaccine.2016.02.068.\r\n\r\n\r\nShanks, G. Dennis, John Brundage, and John Frean. 2010. “Why did many more diamond miners than gold miners die in South Africa during the 1918 influenza pandemic?” International Health. https://doi.org/10.1016/j.inhe.2009.12.001.\r\n\r\n\r\nStapleton, Timothy J. 1995. “Oral Evidence in a Pseudo-Ethnicity: The Fingo Debate.” History in Africa 22: 359–68. http://www.jstor.org/stable/3171922.\r\n\r\n\r\nSuthaharan, Shan. 2016. “Support Vector Machine.” In Machine Learning Models and Algorithms for Big Data Classification: Thinking with Examples for Effective Learning, 207–35. Boston, MA: Springer US. https://doi.org/10.1007/978-1-4899-7641-3_9.\r\n\r\n\r\nTaubenberger, Jeffery K., and David M. Morens. 2006. “1918 Influenza: the Mother of All Pandemics.” Emerging Infectious Diseases. https://doi.org/10.3201/eid1201.050979.\r\n\r\n\r\nThe specific terms are detailed in the appendix↩︎\r\nSea Point lacks a denominator as it was not recorded separately from Cape Town↩︎\r\n",
    "preview": "posts/2021-07-28-spanish-flu-mortality/images/ridge-plot.jpeg",
    "last_modified": "2021-09-21T13:18:17+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-06-students-speak/",
    "title": "Students Speak",
    "description": "Text analysis of students' diary entries during the Covid-19 lockdown in South Africa",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {}
      }
    ],
    "date": "2021-07-06",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nStudents Speak\r\nContext\r\nData\r\nWord Cloud\r\nEvolution of students’ diary entry sentiment over time.\r\nWhat are the words most specific to each week of the student diary entries?\r\nSentiment and week-specific words\r\nComparison of students’ diary entries with Covid-19 statistics.\r\nContextualization of timing of diary entries\r\n\r\n\r\nStudents Speak\r\nHi Johan - here are some visualizations I have put together from your students’ diary entries. I think that they tell quite a nice story. I hope that some are useful. I’ve done them in black and white - I’m not sure where you want to publish them in the end. If you’d like some colour I can add it easily.\r\nI’ve written up the process mostly so that I can remember. The visualizations are at the bottom of the post.\r\nContext\r\nThe Stellenbosch students of Economic History 281 were encouraged to keep a diary during the lockdown as the Covid-19 pandemic overtook the world in March 2020. This post is a short text analysis of the content of their diary entries.\r\nData\r\nThe students’ diary entries have been ingested to form a dataset such that each row is one student’s observation on one day. Additional columns specify the date and the week of the log. There are 333 observations in total. Three examples are shown in the table below.\r\n\r\n\r\n\r\nThese data were supplemented to include the number of Covid-19 cases in South Africa, the number of deaths, and the number of tests performed. These may provide some context around the change in content of the diary entries over time.\r\n\r\n\r\n\r\nWord Cloud\r\nWe start with a word cloud which shows the words used by the students in their diary entries.\r\nThe size of the word is correlated to how frequently it is used. The sentiment of the word is scored with the bing sentiment lexicon, a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 1: Word cloud\r\n\r\n\r\n\r\nWe can see that common positive words include “support”, “privileged”, “healthy”, “productive”, and “excited”. Common negative words are dominated by “virus”, followed by “difficult”, “struggling”, and “infected”.\r\nThis is slightly more informative than a generic word cloud showing word frequency. However, it should be noted that the words must occur in both the students’ diary entries and the bing sentiment lexicon in order to be shown in the word cloud.\r\nTable 1 shows some common words in the students’ diary entries which are excluded from the wordcloud in Figure 1.\r\n\r\nTable 1: Common words excluded from wordcloud\r\nWord\r\nNumber of uses\r\nLockdown\r\n251\r\nPeople\r\n211\r\nDay\r\n183\r\nTime\r\n172\r\nFamily\r\n111\r\nFeel\r\n92\r\nSouth\r\n90\r\nHome\r\n80\r\nWorld\r\n78\r\nDays\r\n71\r\nAfrica\r\n61\r\nLife\r\n54\r\nOnline\r\n51\r\n19\r\n50\r\nFriends\r\n50\r\n\r\nWe can also include a conventional word cloud beside the comparison cloud, and shown in Figure 2.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 2: Conventional word cloud and comparison cloud\r\n\r\n\r\n\r\nEvolution of students’ diary entry sentiment over time.\r\nFigure 3 below shows the change in sentiment of the student responses over the course of the lockdown. It requires some explanation: the words used by the students are grouped by week, scored according to a sentiment lexicon, the score is averaged across the week. The points on the graph represent the average sentiment of the students’ diary entries in a particular week.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: Evolution of sentiment\r\n\r\n\r\n\r\nWe can see that at the outset, sentiment is poor, this improves, and then drops dramatically at the end of the period. It is noteworthy that the average sentiment is negative for the entirety of the period, highlighted by the dotted line at zero.\r\nThis can be explained by the choice of sentiment lexicon used to score the words. The AFINN-111 dataset is a lexicon of English words rated for valence with an integer between minus five and plus five. The words were manually labelled by Finn Årup Nielsen in 2009-2011. An example of the scores assigned to words in the students’ diary entries is shown in Table 2 below.\r\n\r\nTable 2: AFINN sentiment scores\r\nWord\r\nSentiment score\r\nBullshit\r\n-4\r\nCatastrophic\r\n-4\r\nPanic\r\n-3\r\nFake\r\n-3\r\nWorse\r\n-3\r\nFunny\r\n4\r\nFun\r\n4\r\nWonderful\r\n4\r\nThrilled\r\n5\r\n\r\nWhat are the words most specific to each week of the student diary entries?\r\nThe wordcloud in Figure 1 showed the most common words. What if we want to see the words that are most specific to each week of the diary entries? We can use the tidylo package that provides the weighted log odds ratio for each word across the weeks of diary entries. This provides a quantification of how specific each word is to the week that it is used in. For more information see Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict by Monroe, Colaresi, and Quinn (2008).\r\nTable 3 below shows the words most specific to each week.\r\n\r\n\r\n\r\n\r\nTable 3: Week specific words calculated with weighted log odds\r\nWeek 1\r\nWeek 2\r\nWeek 3\r\nWeek 4\r\nWeek 5\r\nWeek 6\r\nNSFAS\r\nTownships\r\nContinues to rise\r\nPayment\r\nFresh produce\r\nLevel 5\r\nAirports\r\nInternet access\r\nAn obligation\r\nExtension\r\nSUN Learn\r\nUnemployed\r\nNational lockdown\r\nLockdown starts\r\nPrivilege\r\nEmotions\r\nGrade\r\nPresident Ramaphosa\r\nAfrican leaders\r\nCases recorded\r\nConspiracy theories\r\nGood Friday\r\nR500 billion\r\nLockdown restrictions\r\n\r\nNice! We can see that we capture some elements of the experience in each week of lockdown.\r\n\r\n\r\n\r\nSentiment and week-specific words\r\nThis figure superimposes the week-specific words above the line graph that shows the evolution of the students’ sentiment across the weeks.\r\n\r\n\r\n\r\nFigure 4: Sentiment and week-words figure\r\n\r\n\r\n\r\nI think it captures a bit of the experience - at the outset there was anxiety about the lockdown, difficulties with internet access and a worry about the rise in cases. This was followed by conspiracy theories and discussions of obligation and privilege. The collective mood improved toward Easter, and was further buoyed by the announcement of a large stimulus package by the government. Finally there was exasperation about the state of employment and livelihoods.\r\nComparison of students’ diary entries with Covid-19 statistics.\r\nFigure 5 shows the evolution of the sentiment of the students’ diary entries beside the rising Covid-19 case numbers in South Africa.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 5: Comparison figure\r\n\r\n\r\n\r\nIt is difficult to conclude about a relationship between the number of cases and the sentiment of the students’ reflections. While there appears to be a relationship between average sentiment and number of tests at the outset of the lockdown, I think this is statistical noise rather than some sort of correlation.\r\nContextualization of timing of diary entries\r\nThe purpose of this selection of figures is to emphasize that the diary entries were recorded at the outset of the pandemic in South Africa. The number of cases was relatively low compared to the steep increase in cases which followed in winter of 2020.\r\nThe figures below compare the period of diary entries to the number of cases and deaths in the first year of the pandemic.\r\nI think option two conveys the message clearly and without clutter.\r\n\r\nOption 1\r\nHere we have a two panel plot of the Covid-19 statistics and number of diary entries recorded by the students.\r\n\r\n\r\n\r\nOption 2\r\nNext we have a single panel with the period of diary entries superimposed on the Covid-19 statistics.\r\n\r\n\r\n\r\nOption 3\r\nAlternatively we can annotate a thick line to show where the diary entries occur.\r\n\r\n\r\n\r\nOption 4\r\nAlternatively we can have a legend variant of option 2.\r\n\r\n\r\n\r\n\r\nFinancial markets comparison\r\n\r\n\r\n\r\nFigure 6 shows the mean sentiment of the students’ diary entries alongside the JSE All Share Index for the same period, as well as the Rand to US Dollar exchange rate. Several students question in their entries what will happen to the stock market, with one stating, “I was looking at good stock picks on the JSE today. Every disaster can be an opportunity…”.\r\n\r\n\r\n\r\nFigure 6: Financial markets comparison figure\r\n\r\n\r\n\r\nAgain the trends displayed may constitute statistical noise. The JSE was rising out of an enormous trough created as investors panicked with Covid-19 spreading into Europe and the US. The Rand is a notoriously volatile currency. Yet, these trends are interesting to show in the context of the early weeks of lockdown.\r\nInteractive figure\r\nWe can also make this a little more attractive as an interactive chart with some colour and hover labels.\r\n\r\n\r\n\r\nFigure 7 shows the same information as above with a hover field to show the week-specific words. Mouse over the points to see the words most specific to each week and the average sentiment of the students’ diary entries.\r\n\r\n\r\n\r\n\r\nFigure 7: Interctive figure\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-06-students-speak/images/preview.jpg",
    "last_modified": "2021-07-08T16:41:36+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-21-ggplot-resources/",
    "title": "Resources for Learning R and Mastering ggplot",
    "description": "A collection of videos and resources to get you started using R and the tidyverse for exploratory data analysis and visualization.",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {}
      }
    ],
    "date": "2021-05-21",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPurpose\r\nWhy use R?\r\n\r\nResources\r\n\r\nPurpose\r\nData visualisation is super fun and a great tool for communication. I have put together some slides for talk on data visualisation for economic historians at the Laboratory for the Economics of Africa’s Past. These resources are collected here to help when you think ‘where do I start with R?’.\r\nI hope that you find these resources useful. They are divided up into different sections in the tabs below.\r\nWhy use R?\r\nThere are a multitude of different pieces of software and different workflows that you can use to create beautiful and intelligible data visualizations.\r\nI think R is a very good choice. There are three reasons I love R for data viz.\r\nggplot2 is a fantastic library for making data visualizations - it is customizable and logical.\r\nThe R community is very supportive and makes learning and improving a pleasure.\r\nThe entire process of importing and cleaning data, visualizing it, and then sharing it is made easy in Rstudio.\r\nResources\r\nIn the tabs below, I have collected some resources that I hope will be beneficial on your journey.\r\n\r\n\r\nInstalling R\r\nGetting started\r\nBefore we can do the fun stuff, we need to install R and RStudio.\r\nThe Modern Dive textbook has a great primer on getting started in R with an explanation of the difference between R and RStudio:\r\nModern Dive analogyThe Rstudio Education team also have a great beginner guide to R that covers installation.\r\n\r\n\r\nLearning R\r\nVideos and textbooks for learning how to use R\r\nThe R bible\r\nThe definitive guide to using R in a modern manner is R for Data Science by Hadley Wickham and Garrett Grolemund. I am not sure that is the best place to start, but it is certainly a helpful resource to come back to once you’re up and running.\r\nThe R for Data Science textbookRStudio Education\r\nRstudio Education have put together a set of instructions for learning R at different levels of competency. These include great guides for installing R, R Studio and the tidyverse packages right at the beginning.\r\nThe beginner, intermediate and expert start guides are linked here.\r\nThe RStudio team have put a lot of effort into creating resources that are useful for many R users, from helping teachers take their course work online, to financial analysis, to creating a personal website and academic publishing. Have a look around their website for more.\r\nRstudio Education graphicsOther great video resources\r\nI am a visual learner and as a consequence I like to watch how things are done and copy them. Because Youtube allows for pausing and recapping at your own pace, I think this is a great way to learn. I’ve collected some videos in these resources which I think are a good place to start.\r\nDavid Robinson’s Youtube Channel\r\nDavid Robinson does a weekly screencast where he shows how to do exploratory data analysis and makes plots in R.\r\nHere is a video about analysing text from reviews of ramen noodles. It has some useful tips for drawing tie-fighter plots and interpreting linear regressions.\r\n\r\n\r\nHe also has a talk about tricks in the tidyverse.\r\n\r\n\r\nJulia Silge’s blog\r\nJulia Silge’s blog is mostly about doing modelling in R, and text modelling in particular. It is a very useful place to look if you want to understand the tidymodels framework which provides access to a host of machine learning techniques in a very easy to use manner. She also has a Youtube channel.\r\nHere is a video from Julia looking at sentiment analysis in R:\r\n\r\n\r\nThe tidy text mining textbook\r\nWritten by Julia Silge and David Robinson, this book gives a lovely introduction to text mining in R. It has lots of examples and includes all the code necessary. As a bonus it is free!\r\nTidy Text Mining TextbookAndrew Couch’s Youtube Channel has a marvelous set of walk throughs of innovative and exciting packages in R. He’s a young data scientist who is creating fantastic content focussed on R and the tidyverse.\r\nHere is one of his videos on creating functions with the tidyverse.\r\n\r\n\r\nAnother resource collection!\r\nThe LearnR4Free website compiled by Mine Dogucu has a great set of resources that are free to use for the learner.\r\n\r\n\r\nDatasets\r\nUseful sources of data to practice your skills on\r\nHaving great datasets available helps to keep me interested in learning new skills. Here are three sources of interesting data:\r\nThe Tidy Tuesday Project\r\nTidy Tuesday is a project run by the R for Data Science foundation - each week they share a real world data set and encourage R users to practice their skills. It has a really fantastic community aspect and if you share your work on Twitter people are always helpful and kind. Others share their work and their code so you can see how their plots were made. In their words,\r\n\r\nThe intent of Tidy Tuesday is to provide a safe and supportive forum for individuals to practice their wrangling and data visualization skills independent of drawing conclusions.\r\n\r\nTidy TuesdayData is Plural\r\n“Data is Plural is a weekly newsletter of useful/curious datasets, published by Jeremy Singer-Vine”. This sums it up - really interesting stuff from all over the internet.\r\nData is pluralNaked Data\r\n“Naked Data is a weekly newsletter which rounds up the best in data-driven storytelling created in or about the Global South.” Get the best bits of twitter without having to spend all day scrolling. Really great newsletter based in SA.\r\nNaked Data\r\n\r\nVisualization\r\nGreat places to get inspiration, figures and code from\r\nFundementals of data visualization\r\nThere are many books on the theory and practice of data visualization. I think Fundamentals of Data Visualization by Claus Wilke is an excellent choice to start with. The preface reads:\r\n\r\nIf you are a scientist, an analyst, a consultant, or anybody else who has to prepare technical documents or reports, one of the most important skills you need to have is the ability to make compelling data visualizations, generally in the form of figures. Figures will typically carry the weight of your arguments. They need to be clear, attractive, and convincing.\r\n\r\nThe book provides great advice for matching your data to a good chart type. For how to make the chosen chart in R, have a look at his course, linked below.\r\n\r\nThe best course in data viz\r\nClause Wilke’s Course on Data Visualization in R is a fantastic accompaniment to the book linked above. The description reads:\r\n\r\nIn this class, students will learn how to visualize data sets and how to reason about and communicate with data visualizations. A substantial component of this class will be dedicated to learning how to program in R. In addition, students will learn how to compile analyses and visualizations into reports, how to make the reports reproducible, and how to post reports on a website or blog.\r\n\r\nIt has wonderful sets of slides including this delightful primer on visualizing uncertainty, and accompnaying code that can be downloaded and worked through. Bonus: all of the slides are hosted on github such that you can see how they’re put together.\r\nThe ggplot2 bible\r\nThe definitive guide to ggplot2, The ggplot2 Book is a fantastic resource for novices and experts alike. It is easy to navigate and has helpful examples with code used to create each figure. (I love to consult the section on annotations as I never remember exactly how they work for some reason.)\r\nThe ggplot2 bookOther great viz resources\r\nThe Storytelling with Data blog has an excellent selection of articles that give tips and tricks on how to design graphics that are easy to understand.\r\nTheir Chart Guide is also a fantastic starting point for how to pick a chart.\r\nSWD Chart GuideJonathan Tan’s exploration of Household Solar Energy in the US is an amazing data storytelling piece that shows all of the code required to create each figure. It makes great use of annotations to explain each component of the visualizations.\r\nJonathan Tan’s ggridge plotThe R Graph Gallery is a useful website which showcases different kinds of visualizations and provides the code that you need to use to recreate the figure yourself.\r\nR Graph Gallery\r\n\r\nCustom themes\r\nFor style that is consistent and graphics that are clear\r\nIf you want to create a style of graphics that communicate clearly to the general public (or are consistent across your organization), have a look at the BBC Visual and Data Journalism cookbook for R graphics. The guide is filled with fantastic examples of visualizations you might like to emulate, and code to do so. They also walk through customizing fonts, colours and annotations for consistency over time.\r\nBBC style graphicsIf you want to create your own theme, Thomas Mock has a lovely guide on graphs that communicate well and look good.\r\nHe walks through a great example of where adding custom images on the plot makes sense:\r\nNFL team images example\r\n\r\nAesthetics\r\nIf you’re interested in learning more about how to make your charts look great.\r\nWilliam Chase’s talk on ‘The Glamour of Graphics’ is a fantastic resource on choosing colours and how to make your graphs pop.\r\n\r\n\r\nStephen Tracy’s breakdown of DataViz Sins You Should Never Commit is a walk-through on how not to confuse your audience. It explains the use of colour and secondary axes in Microsoft Excel plots.\r\nIf you are interested in learning more about fonts, Practical Typography is a great place to start. If you’re interested in choosing a font for a website, have a look at Google’s free fonts here.\r\nLato is a free Google fontThis page from the R Graph Gallery gives a breakdown of the RcolorBrewer package and when to use its different sets of palettes.\r\n\r\nThe color brewer webpage gives fantastic advice on how to choose colours for maps, using the same theory as the package above. It also gives you the hex codes for each colour that you may copy to use yourself.\r\n\r\nThis video on colour theory is a useful primer on colour in data viz.\r\n\r\n\r\n\r\n\r\nEconometrics in R\r\nIf you’re looking to implement econometric techqniques in R\r\nGrant McDermott’s Data science for economists course is a great place to start. He has one lecture on regression that is super useful.\r\nGrant McDermott\r\n\r\nR Shiny\r\nIf you’re looking to make interactive web pages with R and R Markdown\r\nThe simplest way to get a quick and dirty interactive dashboard together is to use the flexdashboard package. Here is a link to the package. One of the great advantages of flexdashboard is that you can get amazing interactivity with just htmlwidgets, you do not need to use a shiny-runtime. In other words, you can create and host a static html page anywhere, for example on GitHub Pages.\r\nFlexdashboardIf you want more interactivity, David Robinson’s video on global crop yields shows a simple Shiny app, with code here.\r\n\r\n\r\nAnother video example of R Shiny comes from Andrew Couch’s YouTube channel.\r\n\r\n\r\nMastering Shiny is a more advanced authoritative book on Shiny, written by Hadley Wickham. It’s a great place to get an understanding of the core of the package.\r\nMastering Shiny\r\n\r\nRecreating Published Figures\r\nWhen you can’t get the authors of an article to share their data\r\nSometimes you may want to recreate a figure for your own paper from a journal article, but the data is not publicly accessible.\r\nOne option is to snip the image from the article and cite it in your own. The disadvantage of this is that the figure will not fit your colour scheme and may be horribly pixalated.\r\nAn alternative is to recreate the figure yourself by extracting the data from the image.\r\nA truly fantastic free tool for this purpose is WebPlotDigitizer. This web application allows you to upload a figure and grab the data from it.\r\nHave a look at a short video I made on how to use it here:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-21-ggplot-resources/images/preview.png",
    "last_modified": "2021-07-09T17:37:17+02:00",
    "input_file": {},
    "preview_width": 1080,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-05-06-swedenmap/",
    "title": "Mapping Sweden's Regional GDP",
    "description": "Visualizing the relationship between population and regional GDP share in 1571, based on data from Enflo and Missiaia (2018).",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {}
      }
    ],
    "date": "2021-05-06",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nWhat was the relationship between population and regional GDP in Sweden in 1571?\r\nScatter plot and interactive map\r\nWhat do we learn?\r\n\r\n\r\n\r\n\r\n\r\nWhat was the relationship between population and regional GDP in Sweden in 1571?\r\nScatter plot and interactive map\r\nThe scatter plot below shows the relationship between population in the Swedish counties in 1571 (x-axis) and regional GDP share (y-axis).\r\nHovering over a point shows the name of the county, and highlights its location on the map of Sweden to the right (shown with their current borders).\r\nClicking on a county on the map will take you to it’s page on Wikipedia.\r\n\r\n\r\n\r\n\r\n\r\n\r\nSource: Enflo and Missiaia (2018), Data accessible here.\r\n\r\nFigure inspiration and code from Claus Wilke\r\nWhat do we learn?\r\nIn the scatter plot above we can see that the two northernmost counties, though large in land are are very sparsely populated. The line of best fit shows that Västmanland County has a high relative share of GDP compared to it’s population, and that Kalmar Län has a high population relative to it’s share of GDP in 1571.\r\n\r\n\r\n\r\nEnflo, Kerstin, and Anna Missiaia. 2018. “Regional GDP Estimates for Sweden, 1571–1850.” Historical Methods: A Journal of Quantitative and Interdisciplinary History 51 (2): 115–37. https://doi.org/10.1080/01615440.2018.1429971.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-06-swedenmap/images/preview.png",
    "last_modified": "2021-05-06T19:00:54+02:00",
    "input_file": {},
    "preview_width": 1199,
    "preview_height": 813
  },
  {
    "path": "posts/2021-04-03-container-ships/",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "description": "In this post I walk through scraping data on cargo ships from Wikipedia as part of a series on shipping. I make use of R, the rvest package for webscraping and the SelectorGadget tool for CSS selection.",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {}
      }
    ],
    "date": "2021-04-08",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nAnother one?How have cargo ships increased in size?\r\nIncreasing size of container ships by length, width and tonnage.\r\nBirfucation in cargo ship capacity.\r\nWhy have container ships become so large?\r\n\r\nScraping Wikipedia’s list of cargo ships.Function to grab the page link from the list of cargo ships.\r\nFunction to scrape the information about each ship.\r\nScraping each page\r\n\r\n\r\nAnother one?\r\nThis week we learned about a data leak at Facebook which took place in 2019, where more than 500 million phone numbers, email addresses and names were scraped from the site and leaked online. Then, on Thursday we heard about another 500 million records including names, email addresses and more personal details were scraped from Linkedin, though the company argues that this was not a data breach. If you want to learn more about scraping, and get in on the (non-nefarious) action yourself, read along. As a bonus you will learn about how cargo ships have become so large.\r\nIn this post I want to show how easy it is to scrape data from the internet. It is the first post in a series looking at ships. I walk through scraping data from Wikipedia, one of the best places on the internet to ingest tabular data from.\r\nBefore we begin with the scraping walkthrough, I want to visualize the data and show how cargo ships have become larger over time.\r\nHow have cargo ships increased in size?\r\nThe small multiples plot below shows the evolution of cargo ship size from 1870 to today. On the x-axis is the length of the average cargo ship per decade from Wikipedia’s list of cargo ships1. On the y-axis is the average ship beam, or width at the widest point. The colour of the rectangle shows the deadweight tonnage of the average ship, or amount of cargo that the ship can carry.\r\n\r\nDeadweight tonnage or tons deadweight is a measure of how much weight a ship can carry. It is the sum of the weights of cargo, fuel, fresh water, ballast water, provisions, passengers, and crew.\r\n\r\n\r\n\r\nCargo ships have increased dramatically in size over time! The oldest ship in our dataset is the R. J. Hackett, one of the first Great Lakes freighters. It was just 63m long and 10m wide, with a wooden hull. According to historian Mark Thompson, the R. J. Hackett’s boxy hull, hatch-lined deck, and placement of the deckhouses meant the ship was ideally suited for moving cargo through inland waterways. This steamer greatly influenced the development of cargo ships which followed.\r\nToday, container ships like the Ever Given are nearly 400m long, 60m wide, and can carry more than 20,000 twenty-foot containers. That’s enough space for 745 million bananas!\r\n\r\nTEUs, or twenty-foot equivalent units, is the measure that shipping companies use to compare volume. A TEU is 6.1m long, 2.4m wide and usually 2.6m high. Source: Wikipedia\r\nIncreasing size of container ships by length, width and tonnage.\r\nIn the plots below we focus only on container ships built after 1970. This era saw the construction of the first ships purpose built to carry ISO containers, which could be loaded and unloaded rapidly at port, repacked and shipped onward on any compatible ship, truck or train. The ISO standard container transformed the shipping industry and replaced the prior status quo of break bulk carriers. One of the consequences was a dramatic reduction in demand for “stevedores” or “longshoremen”; workers who would manually unload cargo from break bulk carriers.\r\n\r\nIf you’re interested in containerization, I highly reccomend this episode from the podcast Trade Talks, and this eight-part series from Alexis Madrigal.\r\n\r\n\r\n\r\n\r\n\r\nTonnage over time\r\nHow have cargo ship deadweight tonnages, or how much cargo a ship can carry, changed over time? Mouse over a point to see the name of the ship.\r\n\r\n\r\n\r\nContainer ships can carry more cargo today than ever before. It’s hard to get my mind around 220 000 tons of cargo!\r\n\r\n\r\nLength over time\r\nHow has the length of cargo ships changed over time? Mouse over a point to see the name of the ship.\r\n\r\n\r\n\r\nThe Ever Given is among the longest container ships operating today at 400m in length. The linear fit line shows that there has been a steady increase in container ship length over time.\r\n\r\n\r\nWidth over time\r\nHow have cargo ship beams, or widths of ships at their widest point, changed over time? Mouse over a point to see the name of the ship.\r\n\r\n\r\n\r\nContainer ships have also become wider, with lumping at beams of 32m, 40m and 59m.\r\n\r\n\r\nBirfucation in cargo ship capacity.\r\nSo it certainly seems that cargo ships have been becoming larger over time. Interestingly, it appears that while the largest container ships continue to get larger and carry more cargo, there is still a need for relatively small ships. There are a significant number of container ships that can carry less than 50 000 tons launched since 2010, shown in the density plot below. We could say that there has been a bifurcation in ship capacity, with a few enormous ships, and a greater number of smaller ships operating in tandem today.\r\n\r\n\r\n\r\nWhy have container ships become so large?\r\nEconomies of scale describes a decrease in the per unit cost as scale of an operation increases. This perfectly fits the shipping industry’s relentless path towards upsizing ships, cranes and ports. One of the reasons has to do with fluid dynamics. Hull resistance is one of the key factors impacting fuel consumption. For vessels in water, drag loss is less than proportional to increasing cargo carried. In other words, making a ship larger usually results in less fuel consumption per ton of cargo, holding all else constant. As we have seen in the figures above, container ships have become larger and larger as they carry more cargo per ship, in an effort to save fuel.\r\nOther methods of drag reduction include improved hull design, injecting air around the hull surface and reducing hull roughness from slime and weeds. See Resistence and powering of ships\r\nAccording to Marc Levinson, author of The Box: How the Shipping Container Made the World Smaller and the World Economy Bigger, the shippers applied this idea to every element of the industry. He says:\r\n\r\nBigger ships lowered the cost of carrying each container. Bigger ports with bigger cranes lowered the cost of handling each ship. Bigger containers — the 20-foot box, shippers’ favorite in the early 1970s, was yielding to the 40-footer — cut down on crane movements and reduced the time needed to turn a vessel around in port, making more efficient use of capital. A virtuous circle had developed: lower costs per container permitted lower rates, which drew more freight, which supported yet more investments in order to lower unit costs even more. If ever there was a business in which economies of scale mattered, container shipping was it.\r\n\r\nThe consequences of containerization are fascinating – including rapidly falling costs of trade, increasingly intermeshed global supply chains, a proliferation of robots at ports, and the environmental challenges associated with ships, trucks and trains meeting at transshipping nodes around the world.\r\nIn the remainder of this post I walk through scraping some of the data presented above.\r\nScraping Wikipedia’s list of cargo ships.\r\nNow that we have had a look at the data, I want to walk through how it can easily be collected and processed for visualizing with R.\r\nThe source of the pre-1970 data is Wikipedia’s list of cargo ships, a screenshot of which I include below. The plan to get the data involves collecting a list of links to each page and then scraping the information about each ship from it’s page. The code chunks below show the process of scraping the data and you can access the scripts on my GitHub repository.\r\n\r\nIf you want to follow along with a video of the scraping process, I highly recommend this video on ramen reviews from David Robinson.\r\n\r\nList of cargo shipsScreenshot of the list\r\n\r\nThe list contains the names of the cargo ships in alphabetical order. We want to grab the links to each article from the list. We can use the SelectorGadget tool to highlight the CSS that leads to each ship’s page. SelectorGadget is an open source tool that makes CSS selector generation and discovery on complicated sites a breeze. It allows you to point to a web page element with the mouse and find the CSS selector for that element. It highlights everything matched by the selector.\r\nI show here a picture of the interface with Google’s Chrome browser:\r\n\r\nSelectorGadget interfaceScreenshot showing SelectorGadget interface\r\n\r\nOnce we have the path we want to collect the links from, we can use the rvest package to scrape the data. Written by Hadley Wickham, this is a package that makes it easy to scrape data from HTML web pages.\r\nWe start with the url of the list:\r\n\r\n\r\nlink <- \"https://en.wikipedia.org/wiki/List_of_cargo_ships\"\r\n\r\n\r\n\r\nFunction to grab the page link from the list of cargo ships.\r\nNext we write a function that gets the list of links from the page. We begin by reading the HTML from the link, then selecting the nodes with the links, and then the attribute called “href” – the url of the page for each ship. We format the output as a tibble, a data frame object that is convenient to work with. Notably this function will work for any list of pages on Wikipedia, neat!\r\n\r\n\r\n# function that gets the urls for each page from the list\r\nget_ship_links <- function(link){\r\n  \r\n  html <- read_html(link)\r\n  \r\n  # nice little message\r\n  message(glue(\"Getting links from {link}\"))\r\n  \r\n  html %>% \r\n    # this html node is found with the selector gadget tool.\r\n    html_nodes(\"#mw-content-text li a\") %>% \r\n    # gets the links or href\r\n    html_attr(\"href\") %>% \r\n    # stores them as a tibble, very convenient alternative to a dataframe.\r\n    as_tibble()\r\n    \r\n}\r\n\r\n\r\n\r\nHere we apply the function to our link and get back a list of links to each page.\r\n\r\n\r\n# apply function to link.\r\nlist_of_links <- get_ship_links(link)\r\n\r\n\r\nGetting links from https://en.wikipedia.org/wiki/List_of_cargo_ships\r\n\r\nlist_of_links\r\n\r\n\r\n# A tibble: 1,024 x 1\r\n   value\r\n   <chr>\r\n 1 #top \r\n 2 #0–9 \r\n 3 #A   \r\n 4 #B   \r\n 5 #C   \r\n 6 #D   \r\n 7 #E   \r\n 8 #F   \r\n 9 #G   \r\n10 #H   \r\n# ... with 1,014 more rows\r\n\r\nWe can see we get 1 024 links back, but the problem is that there are multiple instances of the Table of Contents links, “#A”, “#B” etc. We will filter these out by only selecting links with 5 or more characters.\r\n\r\n\r\nlist_of_links <- list_of_links %>% \r\n  filter(nchar(value) > 4) %>% \r\n  # this sticks the stub to the link so that we can use it later\r\n  mutate(value = str_c(\"https://en.wikipedia.org/\", value)) %>% \r\n  select(url = value)\r\n\r\nlist_of_links\r\n\r\n\r\n# A tibble: 268 x 1\r\n   url                                                                \r\n   <chr>                                                              \r\n 1 https://en.wikipedia.org//w/index.php?title=Al_Rekayyat,_LNG_Tanke~\r\n 2 https://en.wikipedia.org//wiki/Algosteel                           \r\n 3 https://en.wikipedia.org//wiki/SS_Amasa_Stone                      \r\n 4 https://en.wikipedia.org//wiki/MV_Algocape                         \r\n 5 https://en.wikipedia.org//wiki/Algorail                            \r\n 6 https://en.wikipedia.org//wiki/Algosoo_(1974_ship)                 \r\n 7 https://en.wikipedia.org//wiki/Algolake                            \r\n 8 https://en.wikipedia.org//wiki/Algoma_Equinox                      \r\n 9 https://en.wikipedia.org//wiki/MV_Buffalo                          \r\n10 https://en.wikipedia.org//wiki/Algoma_Compass                      \r\n# ... with 258 more rows\r\n\r\nNow that we have the list of links we can get the data about each ship from its Wikipedia page. We want it’s date of launch, it’s type, tonnage, length, beam and status (in service, retired, destroyed etc.). This is all stored helpfully in the infobox on each page:\r\n\r\nAlgosteelScreenshot showing page for ship called Algosteel\r\n\r\nSelectorGadget helps us out again, returning a path to the infobox:\r\n\r\nAlgosteel infoboxScreenshot the infobox selection for the Algosteel\r\n\r\nFunction to scrape the information about each ship.\r\nHere is little function that returns infobox from page about the Algosteel. It has a nice helper message to tell us that it is working, grabs the tabular information, and returns it as a tibble.\r\n\r\n\r\nurl <- \"https://en.wikipedia.org/wiki/Algosteel\"\r\n\r\nget_ship_info_wiki_list <- function(url){\r\n  # store the html from the page\r\n  html <- read_html(url)\r\n  # nice message so that we can see progress.\r\n  message(glue(\"Getting info from {url}\"))\r\n  \r\n  data <- html %>% \r\n    html_node(\".infobox\") %>% \r\n    html_table() %>% \r\n    rename(title = X1,\r\n           value = X2)\r\n  \r\n  data\r\n  \r\n}\r\n\r\n\r\n\r\nHere is the output we get back from the Algosteel.\r\n\r\n\r\nget_ship_info_wiki_list(url)\r\n\r\n\r\nGetting info from https://en.wikipedia.org/wiki/Algosteel\r\n# A tibble: 18 x 2\r\n   title                       value                                  \r\n   <chr>                       <chr>                                  \r\n 1 Algosteel passes under the~ \"Algosteel passes under the Blue Water~\r\n 2 History                     \"History\"                              \r\n 3 Name:                       \"A.S. Glossbrenner (1966–1987)\\nAlgogu~\r\n 4 Owner:                      \"Algoma Central\"                       \r\n 5 Builder:                    \"Davie Shipbuilding, Lauzon, Quebec\"   \r\n 6 Yard number:                \"658\"                                  \r\n 7 Launched:                   \"10 May 1966\"                          \r\n 8 Completed:                  \"July 1966\"                            \r\n 9 Out of service:             \"April 2018\"                           \r\n10 Identification:             \"IMO number: 6613299\"                  \r\n11 Fate:                       \"Scrapped 26 June 2018\"                \r\n12 General characteristics     \"General characteristics\"              \r\n13 Type:                       \"Bulk carrier\"                         \r\n14 Tonnage:                    \"17,955 GRT\\n26,690 DWT\"               \r\n15 Length:                     \"222.4 m (729.7 ft) oa\\n216.2 m (709.3~\r\n16 Beam:                       \"23.0 m (75.5 ft)\"                     \r\n17 Installed power:            \"1 × shaft, diesel engine\"             \r\n18 Speed:                      \"14.5 knots (26.9 km/h; 16.7 mph)\"     \r\n\r\nWe do not care about all of this information, so we will filter for what we want to retain later.\r\nScraping each page\r\nThe code below iterates through the list of links and returns the information in the infobox from each page. The possibly statement is a helpful function from the purrr package that allows us to return a message if the operation fails, in this case a string that just says “failed”.\r\n\r\nFor more about the usefulness of purrr’s safely functions see Dealing with failure in R for Data Science by Hadley Wickham and Garrett Grolemund.\r\n\r\n\r\n# mapping through each url\r\ndf <- list_of_links %>%\r\n        # the possibly statement here means that we will record if there is a failure, for example if there is no infobox in an article.\r\n        mutate(text = map(url, possibly(get_ship_info_wiki_list, \"failed\")))\r\n\r\n\r\n\r\nWe get back a tibble with a nested column that contains the information scraped from each page. We remove the ships that failed (the ones that succeeded will have an NA in the text field, while the ones that failed will say “failed”).\r\n\r\n\r\ndf <- df %>% \r\n  unnest(text) %>% \r\n  filter(is.na(text)) %>% \r\n  select(-text)\r\n\r\n\r\n\r\nNext we select the information from the infobox that we want to keep; name, launch date, status, tonnage, length and width.\r\n\r\n\r\ndf <- df %>% \r\n  mutate(value = str_squish(value)) %>% \r\n  group_by(url) %>% \r\n  mutate(row_id = row_number()) %>%\r\n  filter(title %in% c(\"Name:\",\r\n           \"Launched:\",\r\n           \"Status:\",\r\n           \"Tonnage:\",\r\n           \"Length:\",\r\n           \"Beam:\")) %>%\r\n  # then we pivot wider so that each ship is one row\r\n  pivot_wider(names_from = title, values_from = value) %>% \r\n  # cleaning the names makes it easier to use these columns later\r\n  janitor::clean_names() %>% \r\n  ungroup() %>% \r\n  select(-row_id) %>% \r\n  group_by(url) %>% \r\n  # removing the superfluous columns\r\n  summarise_all(funs(na.omit(.)[1]))\r\n\r\nwrite_rds(df, \"data/cargo_ship_info.rds\")\r\n\r\n\r\n\r\nHere is the output! Easy as that. We can put it in a nice table and then hyperlink the article in the url column.\r\n\r\n\r\n\r\nThanks for reading along, and I hope that you come back for the next article in the series.\r\nPart 2 will look at unsupervised machine learning methods.\r\n\r\nI focus on all cargo ships until the 1970s, whereafter I focus only on container ships. Additional data on container ships is scraped from VesselFinder.com↩︎\r\n",
    "preview": "posts/2021-04-03-container-ships/images/preview_ship_linkedin.jpeg",
    "last_modified": "2021-04-15T13:14:21+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/",
    "title": "Swedish trade in goods and exchange students",
    "description": "I wanted to see if there was an association between the volume of trade between a country and Sweden, and the number of exchange students that have come to Lund Univesity from there, this semester.",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {}
      }
    ],
    "date": "2021-02-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPurpose\r\nDataTrade data\r\nData on exchange students\r\n\r\nFindingsPlotting the correlation between imports of goods to Sweden and exchange students to Lund\r\n\r\n\r\nPurpose\r\nIn a presentation last week organized by the Laboratory for the Economics of Africa’s Past I learned about the persistence of economic connectivity between areas from the Roman era until today. It was fascinating. The presenter, Prof. Erik Hornung, mentioned offhand that the choice of where to spend a semester exchange was likely a function of the economic and social connections between your home country and your host country – whether you know someone there, have family, or are familiar due to cultural similarities.\r\nI am in Sweden this semester, on an exchange at Lund University, in the South of the country. This is due in part to my supervisor’s connections with the economic historians here.\r\nI wanted to see if I could back up this assertion about exchange students with some data. More specifically, I wanted to see if there was an association between the volume of trade between a country and Sweden, and the number of exchange students that have come to Lund from there, this semester.\r\nAs you see in the graphic below, there appears to be a strong positive correlation between Swedish imports of goods and imports of exchange students at Lund.\r\n\r\nShow code\r\ndf_plotly <- read_rds(\"data/Sweden_trade_exchange/plotly_data.rds\")\r\n\r\nc <- df_plotly %>% \r\n  ggplot(aes(trade_value, exchange_students)) +\r\n  geom_point(aes(size = pop_est, colour = Continent)) +\r\n  geom_smooth(group = 1, method = \"lm\", se = F) +\r\n  geom_text_repel(aes(trade_value, exchange_students, label = name), alpha = .7, cex = 3) +\r\n  scale_y_log10() +\r\n  scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\r\n  scale_size_continuous(labels = scales::comma_format()) +\r\n  scale_color_brewer(palette = \"Paired\") +\r\n  geom_hline(yintercept = 0, lty = 2) +\r\n  labs(title = \"Correlation between Sweden's imports of goods and exchange students\",\r\n       x = \"Value of imports into Sweden in 2018 (USD)\",\r\n       y = \"Number of exchange students in Lund Uni Whatsapp group\",\r\n       size = \"Population\")\r\n\r\nc\r\n\r\n\r\n\r\n\r\nThe most populous European nations of Germany and France lead the rankings, with 46 and 37 exchange students respectively. The largest non-European senders are China and India, followed by the United States. South Africa has the largest representation for Africa, with 4 students.\r\nRead on to see the creation of the graphic.\r\nData\r\nTo answer my question, I link two pieces of data: trade statistics on Sweden’s imports, and the nationalities of the incoming exchange students at Lund.\r\nTrade data\r\nThe first I downloaded from an amazing trade tool called the Observatory of Economic Complexity (Linked here). It has wonderful visualizations of trade by type of goods and by country. Have a look at this tree map below for an example.\r\n\r\nTree MapThe tree map shows the origins of Sweden’s imports by value in 2018. Sweden cares about limiting carbon emissions, and so it makes sense that the majority of their imports are sourced within Europe.\r\n\r\nData on exchange students\r\nThe second data source is a rough proxy for the nationalities of my colleagues. I collected phone numbers from a big WhatsApp group called “Lund University ’21” and extract the international dialing codes. There are several shortcomings to this data source – self-selection into WhatsApp use may differ by country, as might the desire to be part of a large group. Further, WhatsApp groups are limited in size at 256 members, just more than half the total number of exchange students at Lund this semester. My sample is unlikely to perfectly represent my population of interest, but it is a good enough starting point.\r\nData processing\r\nIn the chunks of code below I scrape a list of international dialing codes from the web, along with other country level information including GDP per capita and land area.\r\n\r\nShow code\r\n# website with data on dialing codes\r\nurl <- \"https://countrycode.org/\"\r\n\r\n# scrape table with rvest\r\ntable <- read_html(url) %>% \r\n  html_nodes(\"table\") %>% \r\n  html_table()\r\n\r\n# processing\r\ntable <- table[1] %>% as_tibble(.name_repair = \"minimal\") %>% unnest()\r\n\r\ntable <- table$V1 %>% as_tibble() %>% janitor::clean_names()\r\n\r\n# cleaning names and formatting columns as numbers rather than characters\r\ntable_df <- table %>% \r\n  mutate(population = parse_number(population),\r\n         area_km2 = parse_number(area_km2),\r\n         gdp_usd = parse_number(gdp_usd),\r\n         gdp_usd = gdp_usd*10e9)\r\n\r\n\r\n\r\nHere is a table of the data scraped from countrycode.org.\r\n\r\nShow code\r\n# display table\r\ndatatable(table_df)\r\n\r\n\r\n\r\n\r\nWe can show the regions of the world by the first digit of their dialing code in a neat map. Expand the chunk by clicking code to see how easy it is to make an interactive graphic with ggplotly.\r\n\r\nShow code\r\n# packages for map plotting and matching countries.\r\np_load(rnaturalearth, countrycode)\r\n\r\n# extracting first digit of dialing code\r\ntable_df <- table_df %>% \r\n  mutate(iso_a3 = countrycode(country, origin = \"country.name\", destination = \"iso3c\")) %>% \r\n  mutate(first_digit = substring(country_code, 1, 1))\r\n\r\n# creating dataframe with mapping geometry\r\nworld <- ne_countries(scale = \"medium\", returnclass = \"sf\")\r\n\r\n# joining up to table of dialing codes\r\nmap_df <- world %>% as_tibble() %>% left_join(table_df, by = \"iso_a3\")\r\n\r\n# creating plot\r\na <- map_df %>% \r\n  filter(!is.na(first_digit)) %>% \r\n  ggplot(aes(geometry = geometry, fill = first_digit)) +\r\n  geom_sf() +\r\n  scale_fill_brewer(palette = \"Set3\") +\r\n  labs(title = \"Countries coloured by first digit of international dialing code\",\r\n       fill = \"First digit\")\r\n\r\n# display interactive plot\r\nggplotly(a)\r\n\r\n\r\n\r\n\r\nDialing codes\r\nThe trickiest part was matching a country name with an international dialing code from the number alone. I used Google’s open source library for parsing, formatting, and validating international phone numbers. It is written in Java but someone has kindly written a wrapper for R. I show the process in the code chunk below, but do not display the phone numbers themselves for privacy reasons.\r\n\r\nShow code\r\n# Sys.setenv(JAVA_HOME='C:\\\\Program Files\\\\Java\\\\jre1.8.0_281')\r\n# install.packages(\"dialrjars\", INSTALL_opts = c(\"--no-multiarch\"))\r\n\r\nlibrary(dialrjars)\r\nlibrary(dialr)\r\n\r\ndf <- read_excel(\"data/Sweden_trade_exchange/Whatsapp numbers.xlsx\")\r\n\r\n# uniform formatting of numbers\r\ndf <- df %>% \r\n  mutate(clean = ifelse(str_detect(raw, \"\\\\+.*\"), raw, str_c(\"+\", raw))) %>% select(number = clean)\r\n\r\ndf <- df %>% \r\n  mutate(phone = phone(number, \"SE\"))\r\n\r\n# get the region from number\r\ndf <- df %>% \r\n  mutate(region = get_region(phone))\r\n\r\n# count number of phone numbers per country\r\ncounts <- df %>% count(region, sort = T)\r\n\r\n# match name of country to two letter country code\r\ncounts <- counts %>% \r\n  mutate(name = countrycode(region, origin = \"iso2c\", destination = \"country.name\"),\r\n         iso_a3 = countrycode(region, origin = \"iso2c\", destination = \"iso3c\"))\r\n\r\ndatatable(counts)\r\n\r\n\r\n\r\n\r\nA quick plot of counts. Wow! Look at that. Go Germany!\r\n\r\nShow code\r\ndf_col <- counts %>% \r\n  left_join(world %>% as_tibble(), by = \"iso_a3\") %>% \r\n  rename(name = name.x)\r\n\r\ndf_col %>%   \r\n  filter(name != \"Sweden\") %>% \r\n  mutate(name = fct_reorder(name, n),\r\n         continent = factor(continent, levels = c(\"Europe\", \"Asia\", \"North America\", \"Africa\", \"South America\"))) %>% \r\n  ggplot(aes(n, name, fill = continent)) +\r\n  geom_col() +\r\n  scale_fill_brewer(palette = \"Dark2\") +\r\n  labs(title = \"Number of exchange students in Lund Uni exchange WhatsApp group\",\r\n       subtitle = \"By country\",\r\n       y = \"\",\r\n       x = \"Number of exchange students\",\r\n       fill = \"Continent\")\r\n\r\n\r\n\r\n\r\nFindings\r\nPlotting the correlation between imports of goods to Sweden and exchange students to Lund\r\nI plot the correlation between the value of Sweden’s imports in 2018 on the x-axis, and the number of exchange students on the y-axis. Both axes are on a log scale.\r\nBelow is an interactive version of the static plot above.\r\n\r\nShow code\r\nc <- df_plotly %>% \r\n  ggplot(aes(trade_value, exchange_students)) +\r\n  geom_point(aes(size = pop_est, colour = Continent, text=paste(\"Country:\", name))) +\r\n  geom_smooth(group = 1, method = \"lm\", se = T) +\r\n  scale_y_log10() +\r\n  scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\r\n  scale_size_continuous(labels = scales::comma_format()) +\r\n  scale_color_brewer(palette = \"Paired\") +\r\n  geom_hline(yintercept = 0, lty = 2) +\r\n  labs(x = \"Value of imports into Sweden in 2018 (USD)\",\r\n       y = \"Number of exchange students in Lund Uni Whatsapp group\",\r\n       size = \"\",\r\n       colour = \"\")\r\n\r\n\r\nggplotly(c)\r\n\r\n\r\n\r\nShow code\r\n# static figure\r\n\r\n\r\n# map_df_short <- map_df %>%\r\n#   select(iso_a3, continent, pop_est, gdp_md_est)\r\n# \r\n# jpeg(\r\n#   filename= \"data/Sweden_trade_exchange/reg_2.jpg\",\r\n#   width = 8,\r\n#   height = 6,\r\n#   units = \"in\",\r\n#   res = 500\r\n# )\r\n# \r\n# df %>%\r\n#   inner_join(map_df_short) %>%\r\n#   filter(iso_a3 != \"SWE\",\r\n#          Continent != \"Antarctica\") %>%\r\n#   rename(exchange_students = n) %>%\r\n#   ggplot(aes(trade_value, exchange_students)) +\r\n#   geom_point(aes(size = pop_est, colour = Continent)) +\r\n#   geom_smooth(group = 1, method = \"lm\", se = F) +\r\n#   geom_text_repel(aes(trade_value, exchange_students, label = name), alpha = .7, cex = 3) +\r\n#   scale_y_log10() +\r\n#   scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\r\n#   scale_size_continuous(labels = scales::comma_format()) +\r\n#   scale_color_brewer(palette = \"Paired\") +\r\n#   geom_hline(yintercept = 0, lty = 2) +\r\n#   labs(x = \"Value of imports into Sweden in 2018 (USD)\",\r\n#        y = \"Number of exchange students in Lund Whatsapp group\",\r\n#        size = \"Population\")\r\n# \r\n# dev.off()\r\n\r\n\r\n\r\nThank you for reading along!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-03-18T13:34:35+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-13-south-africa-car-colour-exploration/",
    "title": "South African Car Colour Exploration",
    "description": "In this document I walk through creating a stacked area chart showing the evolution of car colour over time in South Africa. The data comes from nearly 45,000 adverts for cars that I scraped from Gumtree.",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {}
      }
    ],
    "date": "2020-11-24",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPurposeWhat does the data look like?\r\nWhat are the most common car colours advertised?\r\nHow has the colour of cars changed over time?\r\nFirst stacked area chart attempt\r\nSolution to gaps in area chart\r\nPutting it together\r\nTightening up the graphic\r\nConclusion\r\nPost script\r\n\r\n\r\nPurpose\r\nWhile driving to Cape Town this past weekend in my beautiful white Hyundai i10, a German friend asked why every second car in South Africa was white in colour. While I didn’t have a compelling reason, I did have a some data to assess whether it was true that half of cars on the road are white.\r\nIn this document I walk through creating a stacked area chart showing the evolution of car colour over time in South Africa. The data comes from nearly 45,000 adverts for cars that I scraped from Gumtree.\r\nI show a useful trick for filling in missing data that creates gaps in the area chart using dplyr’s pivot_wider and pivot_longer commands.\r\nHere is the final interactive graphic:\r\n\r\nShow code\r\ndf_plotly <- read_rds(\"data/car_colour/df_plotly.rds\")\r\n\r\np_load(plotly)\r\n\r\ng <- df_plotly %>% \r\n  ggplot(aes(year, share, fill = colour)) +\r\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\r\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\r\n  scale_y_continuous(labels = scales::percent_format()) +\r\n  theme(legend.position = \"bottom\",\r\n        panel.grid = element_blank()) +\r\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\r\n       subtitle = \"Based on Gumtree vehicle adverts\",\r\n       x = \"Year of production\",\r\n       y = \"Share of cars in each colour per year\",\r\n       fill = \"\",\r\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\r\n\r\nggplotly(g)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\ndf <- readRDS(\"data/car_colour/gumtree_ads_2020-10-01-11-33-am.rds\")\r\n\r\ndf <- df %>% filter(!is.na(make),\r\n                    !is.na(colour),\r\n                    !is.na(model)) %>% \r\n             # Removing the duplicated province names   \r\n             mutate(province = str_remove(province, \"\\\\,.*\"))\r\n\r\n\r\n\r\nWhat does the data look like?\r\n\r\nShow code\r\nskimr::skim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n44644\r\nNumber of columns\r\n30\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n22\r\nnumeric\r\n8\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nlink\r\n0\r\n1.00\r\n68\r\n86\r\n0\r\n2218\r\n0\r\nad_url\r\n0\r\n1.00\r\n78\r\n193\r\n0\r\n38392\r\n0\r\nsite\r\n0\r\n1.00\r\n7\r\n7\r\n0\r\n1\r\n0\r\nseller_type\r\n0\r\n1.00\r\n19\r\n30\r\n0\r\n2\r\n0\r\ntitle\r\n0\r\n1.00\r\n4\r\n100\r\n0\r\n26140\r\n0\r\ntext\r\n0\r\n1.00\r\n10\r\n3885\r\n0\r\n38343\r\n0\r\nad_date\r\n0\r\n1.00\r\n19\r\n19\r\n0\r\n42416\r\n0\r\nseller_name\r\n0\r\n1.00\r\n1\r\n50\r\n0\r\n8872\r\n0\r\nlocation\r\n0\r\n1.00\r\n24\r\n61\r\n0\r\n428\r\n0\r\nseller_age\r\n0\r\n1.00\r\n17\r\n22\r\n0\r\n79\r\n0\r\nn_all_time_ads\r\n0\r\n1.00\r\n1\r\n7\r\n0\r\n3481\r\n0\r\nscrape_time\r\n0\r\n1.00\r\n19\r\n19\r\n0\r\n35812\r\n0\r\nlocation_2\r\n0\r\n1.00\r\n11\r\n47\r\n0\r\n428\r\n0\r\nfor_sale_by\r\n226\r\n0.99\r\n5\r\n6\r\n0\r\n2\r\n0\r\nmake\r\n0\r\n1.00\r\n3\r\n13\r\n0\r\n37\r\n0\r\nmodel\r\n0\r\n1.00\r\n1\r\n20\r\n0\r\n459\r\n0\r\nbody_type\r\n226\r\n0.99\r\n3\r\n12\r\n0\r\n11\r\n0\r\ntransmission\r\n206\r\n1.00\r\n6\r\n9\r\n0\r\n2\r\n0\r\nfuel_type\r\n104\r\n1.00\r\n6\r\n8\r\n0\r\n4\r\n0\r\ncolour\r\n0\r\n1.00\r\n3\r\n8\r\n0\r\n16\r\n0\r\nna\r\n44644\r\n0.00\r\nNA\r\nNA\r\n0\r\n0\r\n0\r\nprovince\r\n0\r\n1.00\r\n7\r\n13\r\n0\r\n9\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\npage\r\n0\r\n1.00\r\n14.51\r\n12.66\r\n1\r\n4.00\r\n11.0\r\n22.00\r\n50\r\n▇▃▂▁▁\r\nad_number\r\n0\r\n1.00\r\n23570.95\r\n14487.85\r\n1\r\n11471.75\r\n22889.5\r\n34258.25\r\n56454\r\n▇▇▇▆▂\r\nprice\r\n229\r\n0.99\r\n218706.68\r\n692268.63\r\n1\r\n109900.00\r\n165000.0\r\n259900.00\r\n140000000\r\n▇▁▁▁▁\r\nn_photos\r\n1099\r\n0.98\r\n8.63\r\n3.08\r\n1\r\n7.00\r\n9.0\r\n12.00\r\n12\r\n▁▂▃▃▇\r\nn_views\r\n0\r\n1.00\r\n588.85\r\n1281.69\r\n0\r\n91.00\r\n233.5\r\n580.00\r\n46956\r\n▇▁▁▁▁\r\nn_active_ads\r\n0\r\n1.00\r\n130.20\r\n225.27\r\n1\r\n5.50\r\n29.0\r\n100.00\r\n997\r\n▇▁▁▁▁\r\nyear\r\n100\r\n1.00\r\n2013.76\r\n6.66\r\n1900\r\n2011.00\r\n2016.0\r\n2018.00\r\n2021\r\n▁▁▁▁▇\r\nkilometers\r\n1189\r\n0.97\r\n92267.35\r\n157493.13\r\n1\r\n26890.50\r\n72000.0\r\n130000.00\r\n9999999\r\n▇▁▁▁▁\r\n\r\nThe dataset has some really nice information on the cars advertised, including their characteristics, as well as the location and name of the seller.\r\nFor this graphic, we only need the car’s year and colour.\r\nWhat are the most common car colours advertised?\r\nWe start by plotting the number of cars in each colour advertised on Gumtree. In this plot I show the 12 most common colours.\r\nIt is evident that white and silver cars are very common. In as far as our data reflects cars on the road, white cars make up nearly 1/3 of all cars advertised! A little less than the half quoted in the discussion above, but certainly the dominant colour.\r\n\r\nShow code\r\ndf %>%\r\n  mutate(colour = fct_lump(colour, 12)) %>%\r\n  select(colour) %>% \r\n  count(colour, sort = T) %>% \r\n  mutate(colour = fct_reorder(colour, n)) %>% \r\n  ggplot(aes(n, colour)) +\r\n  geom_segment(aes(xend = 0, yend = colour)) +\r\n  geom_point(size = 4, colour = \"orange\") +\r\n  geom_text(aes(label = scales::comma(n)), hjust = -.3) +\r\n  scale_x_continuous(labels = scales::comma_format(), limits = c(0, 16000)) +\r\n  labs(x = \"Number of vehicles\",\r\n       title = \"What are the most common vehicle colours advertised on Gumtree?\",\r\n       subtitle = \"Data from 45,000 adverts from across the country\",\r\n       y = \"\")\r\n\r\n\r\n\r\n\r\nHow has the colour of cars changed over time?\r\nNext we want to know if some colours have become more popular over time. Given that our data source is a snapshot of cars for sale at one point in time and on one online website, this is unlikely to be representative of the entire population of cars in South Africa. However, it will be interesting to see the trends that do exist in this data.\r\nWe start with a simple plot of the number of cars advertised in the none more common colours, by year.\r\n\r\nShow code\r\ndf_colours <- df %>% \r\n  group_by(year, colour) %>% \r\n  summarise(n = n()) %>%\r\n  ungroup() %>% \r\n  filter(between(year, 1990, 2020)) \r\n\r\n\r\n\r\nIt makes sense that the majority of adverts are for cars that are produced recently, and it is striking that white and silver cars see such a dramatic increase, even relative to the third most common car colour, grey.\r\n\r\nShow code\r\ndf_colours %>% \r\n  mutate(colour = fct_lump(colour, 10)) %>%\r\n  filter(colour != \"Other\") %>% \r\n  mutate(colour = fct_reorder(colour, n, .fun = sum, .desc = T)) %>% \r\n  ggplot(aes(year, n, fill = colour)) +\r\n  geom_area() +\r\n  facet_wrap(~ colour)\r\n\r\n\r\n\r\n\r\nFirst stacked area chart attempt\r\nIn this plot we show the evolution of car colour between 1990 and the present.\r\nThere are a few problems with this plot. First, there are too many colours, the least common of which have just a few entries. Second, the missing values for the less common colours create unappealing white gaps in the bottom left of the plot.\r\n\r\nShow code\r\ndf_colours %>% \r\n  group_by(year) %>% \r\n  add_tally(n) %>% \r\n  ungroup() %>% \r\n  mutate(colour = fct_reorder(colour, nn, .fun = sum, .desc = T)) %>% \r\n  mutate(share = n / nn) %>% \r\n  ggplot(aes(year, share, fill = colour)) +\r\n  geom_area()\r\n\r\n\r\n\r\n\r\nSolution to gaps in area chart\r\nTo solve this problem, we can fill in the missing values by making the dataset wider and then long gain.\r\nUsing dplyr’s pivot_wider and the values_fill = 0 option, we create a wide dataset with one row for each year, shown below.\r\nWe go from a long dataset like this:\r\n\r\nShow code\r\nhead(df_colours)\r\n\r\n\r\n# A tibble: 6 x 3\r\n   year colour     n\r\n  <dbl> <chr>  <int>\r\n1  1990 Black      2\r\n2  1990 Blue      14\r\n3  1990 Gold       3\r\n4  1990 Green      3\r\n5  1990 Grey       5\r\n6  1990 Other      3\r\n\r\nTo a wide one, like this:\r\n\r\nShow code\r\ndf_colours <- df_colours %>% \r\n  mutate(colour = fct_lump(colour, 10)) %>%\r\n    group_by(year, colour) %>% \r\n    mutate(n = sum(n)) %>% \r\n  ungroup() %>% \r\n  distinct() %>% \r\n  pivot_wider(names_from = colour, values_from = n, values_fill = 0) \r\n\r\nhead(df_colours)\r\n\r\n\r\n# A tibble: 6 x 11\r\n   year Black  Blue  Gold Green  Grey Other   Red Silver White\r\n  <dbl> <int> <int> <int> <int> <int> <int> <int>  <int> <int>\r\n1  1990     2    14     3     3     5     4    10      3    10\r\n2  1991     3    11     2     4     4     3     9      3    12\r\n3  1992     1    16     2     4     5     7    11      3    14\r\n4  1993     0    13     2     1     4     1    13      7    18\r\n5  1994     2    15     0     8     7     1    13      6    41\r\n6  1995     6    36     0    17     6     3    32      7    34\r\n# ... with 1 more variable: Burgundy <int>\r\n\r\nNow we have a ‘wide’ dataset with the missing values filled in with zeros\r\nWe can then make it long again, with the pivot_longer command, specifying that we want the original column names back.\r\n\r\nShow code\r\ndf_colours <- df_colours %>% \r\n  pivot_longer(!year, names_to = \"colour\", values_to = \"n\")\r\n\r\nhead(df_colours)\r\n\r\n\r\n# A tibble: 6 x 3\r\n   year colour     n\r\n  <dbl> <chr>  <int>\r\n1  1990 Black      2\r\n2  1990 Blue      14\r\n3  1990 Gold       3\r\n4  1990 Green      3\r\n5  1990 Grey       5\r\n6  1990 Other      4\r\n\r\nPutting it together\r\nNow we have a plot with no white gaps, and an appropriate number of colours. However, it would be nice to have the colours on the plot match the colours of the cars, and have some labels to make it clear to the reader how to interpret the graphic.\r\n\r\nShow code\r\ndf_colours %>% \r\n  group_by(year) %>% \r\n  add_tally(n) %>% \r\n  ungroup() %>% \r\n  mutate(share = n / nn) %>% \r\n  ggplot(aes(year, share, fill = colour)) +\r\n  geom_area()\r\n\r\n\r\n\r\n\r\nTightening up the graphic\r\nWe use ggplot’s scale_fill_manual to specify the hex codes for the colours in the order that they appear in the plot.\r\nWe add in a title and axes labels, and make the plot interactive with ggplotly.\r\n\r\nShow code\r\ng <- df_colours %>% \r\n  filter(year > 1994) %>% \r\n  group_by(year) %>% \r\n  add_tally(n) %>% \r\n  ungroup() %>% \r\n  mutate(share = n / nn) %>% \r\n  ggplot(aes(year, share, fill = colour)) +\r\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\r\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\r\n  scale_y_continuous(labels = scales::percent_format()) +\r\n  theme(legend.position = \"bottom\",\r\n        panel.grid = element_blank()) +\r\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\r\n       subtitle = \"Based on Gumtree vehicle adverts\",\r\n       x = \"Year of production\",\r\n       y = \"Share of cars in each colour per year\",\r\n       fill = \"\",\r\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\r\n\r\nggplotly(g)\r\n\r\n\r\n\r\n\r\nConclusion\r\nWow! Blue and Green cars really lost out to white and Silver ones over time. Of the cars advertised that were produced in 2020, nearly 42% are white!\r\nBlack cars seemed to have a peak in popularity around 2007, and then decline again, coinciding with the first round of load-shedding in South Africa.\r\nThanks for reading along, and I hope you enjoyed learning about ggplot as well as the cars on South Africa’s roads!\r\nPost script\r\nHere is the static figure with accompanying caption.\r\n\r\nShow code\r\n# jpeg(\r\n#     filename=\"figures/stacked_area_2.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf_colours %>%\r\n  filter(year > 1994) %>%\r\n  group_by(year) %>%\r\n  add_tally(n) %>%\r\n  ungroup() %>%\r\n  mutate(share = n / nn) %>%\r\n  mutate(colour = fct_relevel(colour, \"Silver\")) %>%\r\n  ggplot(aes(year, share, fill = colour)) +\r\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\r\n  scale_fill_manual(values = c(\"#95A5A6\", \"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#EAECEE\")) +\r\n  scale_y_continuous(labels = scales::percent_format()) +\r\n  theme(legend.position = \"bottom\",\r\n        panel.grid = element_blank()) +\r\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\r\n       subtitle = \"Based on Gumtree vehicle adverts\",\r\n       x = \"Year of production\",\r\n       y = \"Share of cars in each colour per year\",\r\n       fill = \"\",\r\n       caption = \"Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\r\n\r\n\r\nShow code\r\n# dev.off()\r\n\r\n\r\n\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-13-south-africa-car-colour-exploration/images/stacked_area.jpeg",
    "last_modified": "2021-03-16T17:06:23+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-18-car-price-explorer/",
    "title": "Car Price Comparison Tool",
    "description": "This web app helps South Africans gauge the median price for a second hand car. You can compare price by odometer reading, make and model, and province. Ad links are included in a table beside the output.",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {}
      }
    ],
    "date": "2020-10-10",
    "categories": [],
    "contents": "\r\nHere’s a link to the web app, along with an image of what it looks like.\r\nThe app has three components. In the scatter plot you can see the price of each car advertised on Gumtree on the y-axis against its odometer reading on the x. Below this is a box and whisker plot of price to show the distribution of prices for the cars in the range. To the right is a table showing the details of each advert along with a link. You can choose the make, model, province, odometer range and year of manufacture in the panel on the left.\r\n\r\n\r\nI hope that you find it useful!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-18-car-price-explorer/images/preview.png",
    "last_modified": "2021-04-09T15:04:20+02:00",
    "input_file": {},
    "preview_width": 1136,
    "preview_height": 679
  },
  {
    "path": "posts/2021-04-21-chips/",
    "title": "Recreating Our World In Data’s Beautiful Transistor Count Graphic",
    "description": "Scraping data from Wikipedia and recreating OWID's figure with ggplot2.",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {}
      }
    ],
    "date": "2020-04-08",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPurpose\r\nScraping\r\nData processing\r\nData viz preparation\r\nPlotting\r\nConclusion\r\n\r\nPurpose\r\nIn this document I walk through the process to recreate a data visualization from Our World in Data. I use the web scraping package rvest to get the data from Wikipedia and some neat tidyverse tools.\r\nThe graphic depicts the metal–oxide–silicon (MOS) transistor count on commercially available microprocessors over time. This progression is known as Moore’s law, which observed that the transistor count doubles approximately every two years.\r\n\r\nGraphic showing Moore’s Law - Our World In Data\r\nFirst, I show the wikipedia page that contains the information that I want to scrape (and contains the Our World in Data graphic).\r\n\r\nWikipedia article\r\nThe Wikipedia article on transistor counts includes a table containing 193 microprocessors released between 1971 and 2020, shown below. Crucially, it specifies for each chip the release year and transistor count.\r\n\r\nTable showing transistor count\r\nScraping\r\nThis chunk of code grabs the HTML of the Wikipedia page, selects all the tables on the page and saves the one we want to analyze as a tibble, a very handy dataframe alternative. Rvest’s html_table function returns all of the elements of a web table, neat!\r\n\r\n\r\nurl <- \"https://en.wikipedia.org/wiki/Transistor_count\"\r\n\r\nhtml <- read_html(url)\r\n\r\nhtml_nodeset <- html %>% \r\n  html_nodes(\"table\")\r\n\r\ndf_transistors <- html_nodeset[[2]] %>% \r\n  html_table() %>% \r\n  as_tibble()\r\n\r\n\r\n\r\nData processing\r\nAs visible in the screen capture of the table above, a few cells also hyper-link to sources for the stats. These square brackets prevent R from reading the column as a set of numbers, instead the cell contents are returned as characters. In the chunk below I use readr’s parse_number function to extract the numbers from each cell and store them as a double.\r\n\r\n\r\ndf_transistors <- df_transistors %>% \r\n  janitor::clean_names() %>% \r\n  mutate(processor = str_squish(str_remove(processor, \"\\\\(.*\"))) %>% \r\n  mutate_at(c(\"mos_transistor_count\", \"date_ofintroduction\", \"mos_process_nm\", \"area_mm2\"), parse_number)\r\n\r\n\r\n\r\nData viz preparation\r\nIn this chunk I lump together the chip designers into the five largest groups and an ‘other’ category with forcats’ fct_lump function. It returns these categories as factors. To make the graphic readable, I sample 50 of the processors from the table of 193 to annotate onto the plot I also store some graph labels.\r\n\r\n\r\ndf_transistors %>% \r\n  count(designer, sort = T)\r\n\r\n\r\n# A tibble: 40 x 2\r\n   designer              n\r\n   <chr>             <int>\r\n 1 Intel                63\r\n 2 AMD                  18\r\n 3 Apple                14\r\n 4 IBM                  13\r\n 5 Fujitsu              10\r\n 6 Huawei                7\r\n 7 Motorola              7\r\n 8 Qualcomm              7\r\n 9 Texas Instruments     5\r\n10 Acorn                 4\r\n# ... with 30 more rows\r\n\r\ndf_transistors <- df_transistors %>% \r\n  mutate(designer = fct_lump(designer, 5))\r\n\r\nprocessor_labels <- sample(df_transistors$processor, 50)\r\n\r\ntitle <- \"Moore's Law\"\r\n\r\nsubtitle <- \"The number of transistors on integrated circuit chips (1971-2020)\"\r\n\r\ncaption <- \"Source: Wikipedia - Transistor Count: https://en.wikipedia.org/wiki/Transistor_count\"\r\n\r\n\r\n\r\nPlotting\r\nHere I draw the graphic using ggplot2.\r\n\r\n\r\n# jpeg(\r\n#     filename=\"C:/Users/User/Documents/Recon/jjayes_distill/_posts/2021-04-21-chips/figures/Transistors_colour.jpeg\",\r\n#     width=6,\r\n#     height=4,\r\n#     units=\"in\",\r\n#     res=1000)\r\n\r\ndf_transistors %>% \r\n  ggplot(aes(date_ofintroduction, mos_transistor_count)) +\r\n  geom_smooth(group = 1, alpha = .1) +\r\n  geom_point(aes(colour = designer, shape = designer), cex = 3) +\r\n  geom_text_repel(data = df_transistors %>% \r\n                    filter(processor %in% processor_labels), \r\n                  aes(date_ofintroduction, mos_transistor_count, label = processor),\r\n                  cex = 3, colour = \"#505050\") +\r\n  scale_color_brewer(palette = \"Dark2\") +\r\n  scale_shape_manual(values = seq(15,20, by = 1)) +\r\n  scale_y_log10(labels = scales::comma_format()) +\r\n  labs(x = \"Year\",\r\n       y = \"Transistor Count (log scale)\",\r\n       shape = \"Chip Designer\",\r\n       colour = \"Chip Designer\",\r\n       caption = caption,\r\n       title = title,\r\n       subtitle = subtitle)\r\n\r\n\r\n\r\n# dev.off()\r\n\r\n\r\n\r\nI include a smoothed line of best fit (with geom_smooth) below the points to show how progression in transistor count has slowed since 2005, despite the number of chips designed each year increasing.\r\nI use geom_text_repel to prevent the labels overlapping each other, and put the y-axis on a log scale to draw attention to the approximate doubling of transistor count each year.\r\nConclusion\r\nThanks for reading along! I hope you learned some neat tricks, and see how easy it is to grab data from the web to visualize.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-21-chips/figures/Transistors_colour.jpeg",
    "last_modified": "2021-05-05T20:59:28+02:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Interlude One",
    "description": "\"Like all good fruit, the balance of life is in the ripe and ruin\"",
    "author": [
      {
        "name": "Jonathan Jayes",
        "url": {}
      }
    ],
    "date": "2020-03-13",
    "categories": [],
    "contents": "\r\nThis is a line from Interlude One off of Alt-J’s first album, An Awesome Wave. I like this lyric a great deal!\r\nI aspire to balance in my own life. This site is a space to share what I find interesting, and encourage me to keep learning and sharing. It includes exploratory data analyses, R tutorials, and answers to some data specific questions. I hope that you enjoy your look around.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/welcome/images/alt-j.jpg",
    "last_modified": "2021-04-08T20:16:49+02:00",
    "input_file": {}
  }
]
